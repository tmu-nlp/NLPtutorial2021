{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "1ba1cc1c-6994-468d-b040-086cdffa411c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "def create_features(x,ids):\n",
    "    phi=np.zeros(len(ids))\n",
    "    words=x.split()\n",
    "    for word in words:\n",
    "        phi[ids[\"UNI:\"+word]]+=1\n",
    "    return phi\n",
    "\n",
    "def create_features_test(x,ids):\n",
    "    phi=np.zeros(len(ids))\n",
    "    words=x.split()\n",
    "    for word in words:\n",
    "        if \"UNI:\"+word in ids:\n",
    "            phi[ids[\"UNI:\"+word]]+=1\n",
    "    return phi\n",
    "\n",
    "def initialize_net_randomly(node,ids,layer):\n",
    "    net=[];\n",
    "    w_in=(np.random.rand(node,len(ids))-0.5)/5\n",
    "    b_in=(np.random.rand(node)-0.5)/5\n",
    "    net.append((w_in,b_in))\n",
    "    for num in range(layer-2):\n",
    "        w_m=(np.random.rand(node,node)-0.5)/5\n",
    "        b_m=(np.random.rand(node)-0.5)/5\n",
    "        net.append((w_m,b_m))\n",
    "    w_out=(np.random.rand(1,node)-0.5)/5\n",
    "    b_out=(np.random.rand(1)-0.5)/5\n",
    "    net.append((w_out,b_out))\n",
    "    return net\n",
    "\n",
    "def forward_nn(net,phi0):\n",
    "    phi=[phi0]\n",
    "    for i in range(len(net)):\n",
    "        w,b=net[i]\n",
    "        phi.append(np.tanh(np.dot(w,phi[i])+b))\n",
    "    return phi\n",
    "\n",
    "def backward_nn(net,phi,y_p):\n",
    "    J=len(net)\n",
    "    delta=[np.ndarray for num in range(J+1)]\n",
    "    delta[J]=float(y_p)-float(phi[J][0])\n",
    "    delta_p=[np.ndarray for num in range(J+1)]\n",
    "    for i in range(J-1,-1,-1):\n",
    "        delta_p[i+1]=delta[i+1]*(1-phi[i+1]**2)\n",
    "        w,b=net[i]\n",
    "        delta[i]=np.dot(delta_p[i+1],w)\n",
    "    return delta_p\n",
    "\n",
    "def update_weights(net,phi,delta_p,lam):\n",
    "    for i in range(len(net)):\n",
    "        w,b=net[i]\n",
    "        w+=lam*np.outer(delta_p[i+1],phi[i])\n",
    "        b+=lam*delta_p[i+1]\n",
    "        \n",
    "def yosoku(score,n):\n",
    "    if float(score[n][0])>0:\n",
    "        return 1\n",
    "    else:\n",
    "        return -1\n",
    "\n",
    "    \n",
    "#train    \n",
    "ids=defaultdict(lambda:len(ids))\n",
    "feat_lab=[]\n",
    "with open(\"titles-en-train.labeled.txt\",\"r\",encoding=\"utf-8\") as train:\n",
    "    for line in train:\n",
    "        line=line.strip()\n",
    "        label,sent=line.split(\"\\t\")\n",
    "        word=sent.split(\" \")\n",
    "        for num in word:\n",
    "            ids[\"UNI:\"+num]\n",
    "            \n",
    "with open(\"titles-en-train.labeled.txt\",\"r\",encoding=\"utf-8\") as train:\n",
    "    for line in train:\n",
    "        label,sent=line.split(\"\\t\")\n",
    "        feat_lab.append((create_features(sent,ids),label))\n",
    "node=2\n",
    "layer=3\n",
    "net=initialize_net_randomly(node,ids,layer)\n",
    "\n",
    "iterations=1\n",
    "lam=0.1\n",
    "for num in range(iterations):\n",
    "    for phi0,y in feat_lab:\n",
    "        phi=forward_nn(net,phi0)\n",
    "        delta_p=backward_nn(net,phi,y)\n",
    "        update_weights(net,phi,delta_p,lam)\n",
    "        \n",
    "\n",
    "with open(\"titles-en-test.txt\",\"r\",encoding=\"utf-8\") as tet:\n",
    "    yosoku_t=[]\n",
    "    for line in tet:\n",
    "        line=line.strip()\n",
    "        phi0=create_features_test(line,ids)\n",
    "        score=forward_nn(net,phi0)\n",
    "        yosoku_t.append(yosoku(score,len(net)))\n",
    "    with open(\"yosoku.test.txt\",\"w\",encoding=\"utf-8\") as yt:\n",
    "        for num in yosoku_t:\n",
    "            yt.write(str(num)+\"\\n\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "175d1a4e-36eb-4874-8723-b455d4a7b83d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9132128940843075\n"
     ]
    }
   ],
   "source": [
    "with open(\"yosoku.test.txt\",\"r\",encoding=\"utf-8\") as ans:\n",
    "    kekka=[]\n",
    "    for line in ans:\n",
    "        tags=line.strip().split(\" \")\n",
    "        for tag in tags:\n",
    "            kekka.append(tag)\n",
    "    \n",
    "with open(\"titles-en-test.labeled.txt\",\"r\",encoding=\"utf-8\") as test:\n",
    "    kekka2=[]\n",
    "    for line in test:\n",
    "        line=line.strip()\n",
    "        tag,sen=line.split(\"\\t\")\n",
    "        kekka2.append(tag)\n",
    "    \n",
    "    count=0\n",
    "    for num in range(len(kekka)):\n",
    "        if kekka[num]==kekka2[num]:\n",
    "            count+=1\n",
    "    seido=count/len(kekka)\n",
    "    print(seido)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
