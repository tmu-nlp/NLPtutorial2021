Natural_JJ language_NN processing_NN -LRB-_-LRB- NLP_NN -RRB-_-RRB- is_VBZ a_DT field_NN of_IN computer_NN science_NN ,_, artificial_JJ intelligence_NN -LRB-_-LRB- also_RB called_VBN machine_NN learning_NN -RRB-_-RRB- ,_, and_CC linguistics_NNS concerned_VBN with_IN the_DT interactions_NNS between_IN computers_NNS and_CC human_JJ -LRB-_-LRB- natural_JJ -RRB-_-RRB- languages_NNS ._.
Specifically_RB ,_, it_PRP is_VBZ the_DT process_NN of_IN a_DT computer_NN extracting_VBG meaningful_JJ information_NN from_IN natural_JJ language_NN input_NN and\/or_CC producing_VBG natural_JJ language_NN output_NN ._.
In_IN theory_NN ,_, natural_JJ language_NN processing_NN is_VBZ a_DT very_RB attractive_JJ method_NN of_IN human_JJ --_: computer_NN interaction_NN ._.
Natural_JJ language_NN understanding_NN is_VBZ sometimes_RB referred_VBN to_TO as_IN an_DT AI-complete_JJ problem_NN because_IN it_PRP seems_VBZ to_TO require_VB extensive_JJ knowledge_NN about_IN the_DT outside_JJ world_NN and_CC the_DT ability_NN to_TO manipulate_VB it_PRP ._.
Whether_NNP NLP_NNP is_VBZ distinct_JJ from_IN ,_, or_CC identical_JJ to_TO ,_, the_DT field_NN of_IN computational_JJ linguistics_NNS is_VBZ a_DT matter_NN of_IN perspective_NN ._.
The_DT Association_NNP for_IN Computational_NNP Linguistics_NNPS defines_VBZ the_DT latter_JJ as_IN focusing_VBG on_IN the_DT theoretical_JJ aspects_NNS of_IN NLP_NNP ._.
On_IN the_DT other_JJ hand_NN ,_, the_DT open-access_JJ journal_NN ``_`` Computational_JJ Linguistics_NNS ''_'' ,_, styles_NNS itself_PRP as_IN ``_`` the_DT longest_JJS running_VBG publication_NN devoted_VBD exclusively_RB to_TO the_DT design_NN and_CC analysis_NN of_IN natural_JJ language_NN processing_NN systems_NNS ''_'' -LRB-_-LRB- Computational_NNP Linguistics_NNP -LRB-_-LRB- Journal_NNP -RRB-_-RRB- -RRB-_-RRB- Modern_NNP NLP_NNP algorithms_NNS are_VBP grounded_VBN in_IN machine_NN learning_NN ,_, especially_RB statistical_JJ machine_NN learning_NN ._.
Research_NN into_IN modern_JJ statistical_JJ NLP_NN algorithms_NNS requires_VBZ an_DT understanding_NN of_IN a_DT number_NN of_IN disparate_JJ fields_NNS ,_, including_VBG linguistics_NNS ,_, computer_NN science_NN ,_, and_CC statistics_NNS ._.
For_IN a_DT discussion_NN of_IN the_DT types_NNS of_IN algorithms_NNS currently_RB used_VBN in_IN NLP_NNP ,_, see_VBP the_DT article_NN on_IN pattern_NN recognition_NN ._.
An_DT automated_JJ online_NN assistant_NN providing_VBG customer_NN service_NN on_IN a_DT web_NN page_NN ,_, an_DT example_NN of_IN an_DT application_NN where_WRB natural_JJ language_NN processing_NN is_VBZ a_DT major_JJ component_NN ._.
In_IN 1950_CD ,_, Alan_NNP Turing_NNP published_VBD his_PRP$ famous_JJ article_NN ``_`` Computing_NNP Machinery_NNP and_CC Intelligence_NNP ''_'' which_WDT proposed_VBD what_WP is_VBZ now_RB called_VBN the_DT Turing_NNP test_NN as_IN a_DT criterion_NN of_IN intelligence_NN ._.
This_DT criterion_NN depends_VBZ on_IN the_DT ability_NN of_IN a_DT computer_NN program_NN to_TO impersonate_VB a_DT human_NN in_IN a_DT real-time_JJ written_JJ conversation_NN with_IN a_DT human_JJ judge_NN ,_, sufficiently_RB well_RB that_IN the_DT judge_NN is_VBZ unable_JJ to_TO distinguish_VB reliably_RB --_: on_IN the_DT basis_NN of_IN the_DT conversational_JJ content_NN alone_RB --_: between_IN the_DT program_NN and_CC a_DT real_JJ human_NN ._.
The_DT Georgetown_NNP experiment_NN in_IN 1954_CD involved_VBD fully_RB automatic_JJ translation_NN of_IN more_JJR than_IN sixty_CD Russian_JJ sentences_NNS into_IN English_NNP ._.
The_DT authors_NNS claimed_VBD that_IN within_IN three_CD or_CC five_CD years_NNS ,_, machine_NN translation_NN would_MD be_VB a_DT solved_VBN problem_NN ._.
However_RB ,_, real_JJ progress_NN was_VBD much_RB slower_JJR ,_, and_CC after_IN the_DT ALPAC_NNP report_NN in_IN 1966_CD ,_, which_WDT found_VBD that_IN ten_CD years_NNS long_JJ research_NN had_VBD failed_VBN to_TO fulfill_VB the_DT expectations_NNS ,_, funding_VBG for_IN machine_NN translation_NN was_VBD dramatically_RB reduced_VBN ._.
Little_JJ further_JJ research_NN in_IN machine_NN translation_NN was_VBD conducted_VBN until_IN the_DT late_JJ 1980s_NNS ,_, when_WRB the_DT first_JJ statistical_JJ machine_NN translation_NN systems_NNS were_VBD developed_VBN ._.
Some_DT notably_RB successful_JJ NLP_NNP systems_NNS developed_VBD in_IN the_DT 1960s_NNS were_VBD SHRDLU_NNP ,_, a_DT natural_JJ language_NN system_NN working_VBG in_IN restricted_JJ ``_`` blocks_NNS worlds_NNS ''_'' with_IN restricted_JJ vocabularies_NNS ,_, and_CC ELIZA_NNP ,_, a_DT simulation_NN of_IN a_DT Rogerian_JJ psychotherapist_NN ,_, written_VBN by_IN Joseph_NNP Weizenbaum_NNP between_IN 1964_CD to_TO 1966_CD ._.
Using_VBG almost_RB no_DT information_NN about_IN human_JJ thought_NN or_CC emotion_NN ,_, ELIZA_NNP sometimes_RB provided_VBD a_DT startlingly_RB human-like_JJ interaction_NN ._.
When_WRB the_DT ``_`` patient_NN ''_'' exceeded_VBD the_DT very_RB small_JJ knowledge_NN base_NN ,_, ELIZA_NNP might_MD provide_VB a_DT generic_JJ response_NN ,_, for_IN example_NN ,_, responding_VBG to_TO ``_`` My_PRP$ head_NN hurts_VBZ ''_'' with_IN ``_`` Why_WRB do_VBP you_PRP say_VB your_PRP$ head_NN hurts_VBZ ?_. ''_'' ._.
During_IN the_DT 70_CD 's_POS many_JJ programmers_NNS began_VBD to_TO write_VB `_`` conceptual_JJ ontologies_NNS '_POS ,_, which_WDT structured_VBD real-world_JJ information_NN into_IN computer-understandable_JJ data_NNS ._.
Examples_NNS are_VBP MARGIE_NNP -LRB-_-LRB- Schank_NNP ,_, 1975_CD -RRB-_-RRB- ,_, SAM_NNP -LRB-_-LRB- Cullingford_NNP ,_, 1978_CD -RRB-_-RRB- ,_, PAM_NNS -LRB-_-LRB- Wilensky_NNP ,_, 1978_CD -RRB-_-RRB- ,_, TaleSpin_NNP -LRB-_-LRB- Meehan_NNP ,_, 1976_CD -RRB-_-RRB- ,_, QUALM_NN -LRB-_-LRB- Lehnert_NNP ,_, 1977_CD -RRB-_-RRB- ,_, Politics_NNS -LRB-_-LRB- Carbonell_NNP ,_, 1979_CD -RRB-_-RRB- ,_, and_CC Plot_NN Units_NNS -LRB-_-LRB- Lehnert_NNP 1981_CD -RRB-_-RRB- ._.
During_IN this_DT time_NN ,_, many_JJ chatterbots_NNS were_VBD written_VBN including_VBG PARRY_NN ,_, Racter_NN ,_, and_CC Jabberwacky_NNP ._.
Up_IN to_TO the_DT 1980s_CD ,_, most_JJS NLP_NNP systems_NNS were_VBD based_VBN on_IN complex_JJ sets_NNS of_IN hand-written_JJ rules_NNS ._.
Starting_VBG in_IN the_DT late_JJ 1980s_NNS ,_, however_RB ,_, there_EX was_VBD a_DT revolution_NN in_IN NLP_NN with_IN the_DT introduction_NN of_IN machine_NN learning_NN algorithms_NNS for_IN language_NN processing_NN ._.
This_DT was_VBD due_JJ both_DT to_TO the_DT steady_JJ increase_NN in_IN computational_JJ power_NN resulting_VBG from_IN Moore_NNP 's_POS Law_NN and_CC the_DT gradual_JJ lessening_VBG of_IN the_DT dominance_NN of_IN Chomskyan_JJ theories_NNS of_IN linguistics_NNS -LRB-_-LRB- e.g._FW transformational_JJ grammar_NN -RRB-_-RRB- ,_, whose_WP$ theoretical_JJ underpinnings_NNS discouraged_VBD the_DT sort_NN of_IN corpus_NN linguistics_NNS that_WDT underlies_VBZ the_DT machine-learning_JJ approach_NN to_TO language_NN processing_NN ._.
Some_DT of_IN the_DT earliest-used_JJ machine_NN learning_NN algorithms_NNS ,_, such_JJ as_IN decision_NN trees_NNS ,_, produced_VBD systems_NNS of_IN hard_JJ if-then_JJ rules_NNS similar_JJ to_TO existing_VBG hand-written_JJ rules_NNS ._.
Increasingly_RB ,_, however_RB ,_, research_NN has_VBZ focused_VBN on_IN statistical_JJ models_NNS ,_, which_WDT make_VBP soft_JJ ,_, probabilistic_JJ decisions_NNS based_VBN on_IN attaching_VBG real-valued_JJ weights_NNS to_TO the_DT features_NNS making_VBG up_RP the_DT input_NN data_NNS ._.
The_DT cache_NN language_NN models_NNS upon_IN which_WDT many_JJ speech_NN recognition_NN systems_NNS now_RB rely_VBP are_VBP examples_NNS of_IN such_JJ statistical_JJ models_NNS ._.
Such_JJ models_NNS are_VBP generally_RB more_RBR robust_JJ when_WRB given_VBN unfamiliar_JJ input_NN ,_, especially_RB input_NN that_WDT contains_VBZ errors_NNS -LRB-_-LRB- as_IN is_VBZ very_RB common_JJ for_IN real-world_JJ data_NNS -RRB-_-RRB- ,_, and_CC produce_VBP more_RBR reliable_JJ results_NNS when_WRB integrated_VBN into_IN a_DT larger_JJR system_NN comprising_VBG multiple_JJ subtasks_NNS ._.
Many_JJ of_IN the_DT notable_JJ early_JJ successes_NNS occurred_VBD in_IN the_DT field_NN of_IN machine_NN translation_NN ,_, due_JJ especially_RB to_TO work_VB at_IN IBM_NNP Research_NNP ,_, where_WRB successively_RB more_RBR complicated_JJ statistical_JJ models_NNS were_VBD developed_VBN ._.
These_DT systems_NNS were_VBD able_JJ to_TO take_VB advantage_NN of_IN existing_VBG multilingual_JJ textual_JJ corpora_NN that_WDT had_VBD been_VBN produced_VBN by_IN the_DT Parliament_NNP of_IN Canada_NNP and_CC the_DT European_NNP Union_NNP as_IN a_DT result_NN of_IN laws_NNS calling_VBG for_IN the_DT translation_NN of_IN all_DT governmental_JJ proceedings_NNS into_IN all_DT official_JJ languages_NNS of_IN the_DT corresponding_JJ systems_NNS of_IN government_NN ._.
However_RB ,_, most_RBS other_JJ systems_NNS depended_VBD on_IN corpora_NN specifically_RB developed_VBD for_IN the_DT tasks_NNS implemented_VBN by_IN these_DT systems_NNS ,_, which_WDT was_VBD -LRB-_-LRB- and_CC often_RB continues_VBZ to_TO be_VB -RRB-_-RRB- a_DT major_JJ limitation_NN in_IN the_DT success_NN of_IN these_DT systems_NNS ._.
As_IN a_DT result_NN ,_, a_DT great_JJ deal_NN of_IN research_NN has_VBZ gone_VBN into_IN methods_NNS of_IN more_RBR effectively_RB learning_VBG from_IN limited_JJ amounts_NNS of_IN data_NNS ._.
Recent_JJ research_NN has_VBZ increasingly_RB focused_VBN on_IN unsupervised_JJ and_CC semi-supervised_JJ learning_NN algorithms_NNS ._.
Such_JJ algorithms_NNS are_VBP able_JJ to_TO learn_VB from_IN data_NNS that_WDT has_VBZ not_RB been_VBN hand-annotated_JJ with_IN the_DT desired_VBN answers_NNS ,_, or_CC using_VBG a_DT combination_NN of_IN annotated_JJ and_CC non-annotated_JJ data_NNS ._.
Generally_RB ,_, this_DT task_NN is_VBZ much_RB more_RBR difficult_JJ than_IN supervised_JJ learning_NN ,_, and_CC typically_RB produces_VBZ less_RBR accurate_JJ results_NNS for_IN a_DT given_VBN amount_NN of_IN input_NN data_NNS ._.
However_RB ,_, there_EX is_VBZ an_DT enormous_JJ amount_NN of_IN non-annotated_JJ data_NNS available_JJ -LRB-_-LRB- including_VBG ,_, among_IN other_JJ things_NNS ,_, the_DT entire_JJ content_NN of_IN the_DT World_NNP Wide_NN Web_NN -RRB-_-RRB- ,_, which_WDT can_MD often_RB make_VB up_RP for_IN the_DT inferior_JJ results_NNS ._.
NLP_NN using_VBG machine_NN learning_NN As_IN described_VBN above_JJ ,_, modern_JJ approaches_NNS to_TO natural_JJ language_NN processing_NN -LRB-_-LRB- NLP_NN -RRB-_-RRB- are_VBP grounded_VBN in_IN machine_NN learning_NN ._.
The_DT paradigm_NN of_IN machine_NN learning_NN is_VBZ different_JJ from_IN that_DT of_IN most_JJS prior_JJ attempts_NNS at_IN language_NN processing_NN ._.
Prior_JJ implementations_NNS of_IN language-processing_JJ tasks_NNS typically_RB involved_VBD the_DT direct_JJ hand_NN coding_NN of_IN large_JJ sets_NNS of_IN rules_NNS ._.
The_DT machine-learning_JJ paradigm_NN calls_NNS instead_RB for_IN using_VBG general_JJ learning_NN algorithms_NNS --_: often_RB ,_, although_IN not_RB always_RB ,_, grounded_VBN in_IN statistical_JJ inference_NN --_: to_TO automatically_RB learn_VB such_JJ rules_NNS through_IN the_DT analysis_NN of_IN large_JJ corpora_NN of_IN typical_JJ real-world_JJ examples_NNS ._.
A_DT corpus_NN -LRB-_-LRB- plural_NN ,_, ``_`` corpora_NN ''_'' -RRB-_-RRB- is_VBZ a_DT set_NN of_IN documents_NNS -LRB-_-LRB- or_CC sometimes_RB ,_, individual_JJ sentences_NNS -RRB-_-RRB- that_WDT have_VBP been_VBN hand-annotated_JJ with_IN the_DT correct_JJ values_NNS to_TO be_VB learned_VBN ._.
Consider_VB the_DT task_NN of_IN part_NN of_IN speech_NN tagging_NN ,_, i.e._FW determining_VBG the_DT correct_JJ part_NN of_IN speech_NN of_IN each_DT word_NN in_IN a_DT given_VBN sentence_NN ,_, typically_RB one_CD that_WDT has_VBZ never_RB been_VBN seen_VBN before_RB ._.
A_DT typical_JJ machine-learning-based_JJ implementation_NN of_IN a_DT part_NN of_IN speech_NN tagger_NN proceeds_VBZ in_IN two_CD steps_NNS ,_, a_DT training_NN step_NN and_CC an_DT evaluation_NN step_NN ._.
The_DT first_JJ step_NN --_: the_DT training_NN step_NN --_: makes_VBZ use_NN of_IN a_DT corpus_NN of_IN training_NN data_NNS ,_, which_WDT consists_VBZ of_IN a_DT large_JJ number_NN of_IN sentences_NNS ,_, each_DT of_IN which_WDT has_VBZ the_DT correct_JJ part_NN of_IN speech_NN attached_VBN to_TO each_DT word_NN ._.
-LRB-_-LRB- An_DT example_NN of_IN such_PDT a_DT corpus_NN in_IN common_JJ use_NN is_VBZ the_DT Penn_NNP Treebank_NNP ._.
This_DT includes_VBZ -LRB-_-LRB- among_IN other_JJ things_NNS -RRB-_-RRB- a_DT set_NN of_IN 500_CD texts_NNS from_IN the_DT Brown_NNP Corpus_NNP ,_, containing_VBG examples_NNS of_IN various_JJ genres_NNS of_IN text_NN ,_, and_CC 2500_CD articles_NNS from_IN the_DT Wall_NNP Street_NNP Journal_NNP ._. -RRB-_-RRB-
This_DT corpus_NN is_VBZ analyzed_VBN and_CC a_DT learning_NN model_NN is_VBZ generated_VBN from_IN it_PRP ,_, consisting_VBG of_IN automatically_RB created_VBN rules_NNS for_IN determining_VBG the_DT part_NN of_IN speech_NN for_IN a_DT word_NN in_IN a_DT sentence_NN ,_, typically_RB based_VBN on_IN the_DT nature_NN of_IN the_DT word_NN in_IN question_NN ,_, the_DT nature_NN of_IN surrounding_VBG words_NNS ,_, and_CC the_DT most_RBS likely_JJ part_NN of_IN speech_NN for_IN those_DT surrounding_VBG words_NNS ._.
The_DT model_NN that_WDT is_VBZ generated_VBN is_VBZ typically_RB the_DT best_JJS model_NN that_WDT can_MD be_VB found_VBN that_IN simultaneously_RB meets_VBZ two_CD conflicting_JJ objectives_NNS :_: To_TO perform_VB as_RB well_RB as_IN possible_JJ on_IN the_DT training_NN data_NNS ,_, and_CC to_TO be_VB as_RB simple_JJ as_IN possible_JJ -LRB-_-LRB- so_IN that_IN the_DT model_NN avoids_VBZ overfitting_VBG the_DT training_NN data_NNS ,_, i.e._FW so_IN that_IN it_PRP generalizes_VBZ as_RB well_RB as_IN possible_JJ to_TO new_JJ data_NNS rather_RB than_IN only_RB succeeding_VBG on_IN sentences_NNS that_WDT have_VBP already_RB been_VBN seen_VBN -RRB-_-RRB- ._.
In_IN the_DT second_JJ step_NN -LRB-_-LRB- the_DT evaluation_NN step_NN -RRB-_-RRB- ,_, the_DT model_NN that_WDT has_VBZ been_VBN learned_VBN is_VBZ used_VBN to_TO process_VB new_JJ sentences_NNS ._.
An_DT important_JJ part_NN of_IN the_DT development_NN of_IN any_DT learning_NN algorithm_NN is_VBZ testing_VBG the_DT model_NN that_WDT has_VBZ been_VBN learned_VBN on_IN new_JJ ,_, previously_RB unseen_JJ data_NNS ._.
It_PRP is_VBZ critical_JJ that_IN the_DT data_NNS used_VBN for_IN testing_NN is_VBZ not_RB the_DT same_JJ as_IN the_DT data_NNS used_VBN for_IN training_NN ;_: otherwise_RB ,_, the_DT testing_NN accuracy_NN will_MD be_VB unrealistically_RB high_JJ ._.
Many_JJ different_JJ classes_NNS of_IN machine_NN learning_NN algorithms_NNS have_VBP been_VBN applied_VBN to_TO NLP_NNP tasks_NNS ._.
In_IN common_JJ to_TO all_DT of_IN these_DT algorithms_NNS is_VBZ that_IN they_PRP take_VBP as_IN input_NN a_DT large_JJ set_NN of_IN ``_`` features_NNS ''_'' that_WDT are_VBP generated_VBN from_IN the_DT input_NN data_NNS ._.
As_IN an_DT example_NN ,_, for_IN a_DT part-of-speech_JJ tagger_NN ,_, typical_JJ features_NNS might_MD be_VB the_DT identity_NN of_IN the_DT word_NN being_VBG processed_VBN ,_, the_DT identity_NN of_IN the_DT words_NNS immediately_RB to_TO the_DT left_NN and_CC right_NN ,_, the_DT part-of-speech_JJ tag_NN of_IN the_DT word_NN to_TO the_DT left_NN ,_, and_CC whether_IN the_DT word_NN being_VBG considered_VBN or_CC its_PRP$ immediate_JJ neighbors_NNS are_VBP content_JJ words_NNS or_CC function_NN words_NNS ._.
The_DT algorithms_NNS differ_VBP ,_, however_RB ,_, in_IN the_DT nature_NN of_IN the_DT rules_NNS generated_VBN ._.
Some_DT of_IN the_DT earliest-used_JJ algorithms_NNS ,_, such_JJ as_IN decision_NN trees_NNS ,_, produced_VBD systems_NNS of_IN hard_JJ if-then_JJ rules_NNS similar_JJ to_TO the_DT systems_NNS of_IN hand-written_JJ rules_NNS that_WDT were_VBD then_RB common_JJ ._.
Increasingly_RB ,_, however_RB ,_, research_NN has_VBZ focused_VBN on_IN statistical_JJ models_NNS ,_, which_WDT make_VBP soft_JJ ,_, probabilistic_JJ decisions_NNS based_VBN on_IN attaching_VBG real-valued_JJ weights_NNS to_TO each_DT input_NN feature_NN ._.
Such_JJ models_NNS have_VBP the_DT advantage_NN that_IN they_PRP can_MD express_VB the_DT relative_JJ certainty_NN of_IN many_JJ different_JJ possible_JJ answers_NNS rather_RB than_IN only_RB one_CD ,_, producing_VBG more_RBR reliable_JJ results_NNS when_WRB such_JJ a_DT model_NN is_VBZ included_VBN as_IN a_DT component_NN of_IN a_DT larger_JJR system_NN ._.
In_IN addition_NN ,_, models_NNS that_WDT make_VBP soft_JJ decisions_NNS are_VBP generally_RB more_RBR robust_JJ when_WRB given_VBN unfamiliar_JJ input_NN ,_, especially_RB input_NN that_WDT contains_VBZ errors_NNS -LRB-_-LRB- as_IN is_VBZ very_RB common_JJ for_IN real-world_JJ data_NNS -RRB-_-RRB- ._.
Systems_NNP based_VBN on_IN machine-learning_JJ algorithms_NNS have_VBP many_JJ advantages_NNS over_IN hand-produced_JJ rules_NNS :_: The_DT learning_VBG procedures_NNS used_VBN during_IN machine_NN learning_VBG automatically_RB focus_VB on_IN the_DT most_RBS common_JJ cases_NNS ,_, whereas_IN when_WRB writing_VBG rules_NNS by_IN hand_NN it_PRP is_VBZ often_RB not_RB obvious_JJ at_IN all_DT where_WRB the_DT effort_NN should_MD be_VB directed_VBN ._.
Automatic_NNP learning_NN procedures_NNS can_MD make_VB use_NN of_IN statistical_JJ inference_NN algorithms_NNS to_TO produce_VB models_NNS that_WDT are_VBP robust_JJ to_TO unfamiliar_JJ input_NN -LRB-_-LRB- e.g._FW containing_VBG words_NNS or_CC structures_NNS that_WDT have_VBP not_RB been_VBN seen_VBN before_IN -RRB-_-RRB- and_CC to_TO erroneous_JJ input_NN -LRB-_-LRB- e.g._FW with_IN misspelled_VBN words_NNS or_CC words_NNS accidentally_RB omitted_VBN -RRB-_-RRB- ._.
Generally_RB ,_, handling_VBG such_JJ input_NN gracefully_RB with_IN hand-written_JJ rules_NNS --_: or_CC more_RBR generally_RB ,_, creating_VBG systems_NNS of_IN hand-written_JJ rules_NNS that_WDT make_VBP soft_JJ decisions_NNS --_: is_VBZ extremely_RB difficult_JJ ,_, error-prone_JJ and_CC time-consuming_JJ ._.
Systems_NNP based_VBN on_IN automatically_RB learning_VBG the_DT rules_NNS can_MD be_VB made_VBN more_RBR accurate_JJ simply_RB by_IN supplying_VBG more_JJR input_NN data_NNS ._.
However_RB ,_, systems_NNS based_VBN on_IN hand-written_JJ rules_NNS can_MD only_RB be_VB made_VBN more_RBR accurate_JJ by_IN increasing_VBG the_DT complexity_NN of_IN the_DT rules_NNS ,_, which_WDT is_VBZ a_DT much_RB more_RBR difficult_JJ task_NN ._.
In_IN particular_JJ ,_, there_EX is_VBZ a_DT limit_NN to_TO the_DT complexity_NN of_IN systems_NNS based_VBN on_IN hand-crafted_JJ rules_NNS ,_, beyond_IN which_WDT the_DT systems_NNS become_VBP more_RBR and_CC more_RBR unmanageable_JJ ._.
However_RB ,_, creating_VBG more_JJR data_NNS to_TO input_NN to_TO machine-learning_JJ systems_NNS simply_RB requires_VBZ a_DT corresponding_JJ increase_NN in_IN the_DT number_NN of_IN man-hours_NN worked_VBD ,_, generally_RB without_IN significant_JJ increases_NNS in_IN the_DT complexity_NN of_IN the_DT annotation_NN process_NN ._.
Major_JJ tasks_NNS in_IN NLP_NN The_DT following_NN is_VBZ a_DT list_NN of_IN some_DT of_IN the_DT most_RBS commonly_RB researched_VBN tasks_NNS in_IN NLP_NNP ._.
Note_VB that_IN some_DT of_IN these_DT tasks_NNS have_VBP direct_JJ real-world_JJ applications_NNS ,_, while_IN others_NNS more_RBR commonly_RB serve_VB as_IN subtasks_NNS that_WDT are_VBP used_VBN to_TO aid_VB in_IN solving_VBG larger_JJR tasks_NNS ._.
What_WP distinguishes_VBZ these_DT tasks_NNS from_IN other_JJ potential_JJ and_CC actual_JJ NLP_NN tasks_NNS is_VBZ not_RB only_RB the_DT volume_NN of_IN research_NN devoted_VBN to_TO them_PRP but_CC the_DT fact_NN that_IN for_IN each_DT one_CD there_EX is_VBZ typically_RB a_DT well-defined_JJ problem_NN setting_NN ,_, a_DT standard_JJ metric_NN for_IN evaluating_VBG the_DT task_NN ,_, standard_JJ corpora_NN on_IN which_WDT the_DT task_NN can_MD be_VB evaluated_VBN ,_, and_CC competitions_NNS devoted_VBN to_TO the_DT specific_JJ task_NN ._.
Automatic_NNP summarization_NN :_: Produce_VB a_DT readable_JJ summary_NN of_IN a_DT chunk_NN of_IN text_NN ._.
Often_RB used_VBN to_TO provide_VB summaries_NNS of_IN text_NN of_IN a_DT known_JJ type_NN ,_, such_JJ as_IN articles_NNS in_IN the_DT financial_JJ section_NN of_IN a_DT newspaper_NN ._.
Coreference_NN resolution_NN :_: Given_VBN a_DT sentence_NN or_CC larger_JJR chunk_NN of_IN text_NN ,_, determine_VB which_WDT words_NNS -LRB-_-LRB- ``_`` mentions_VBZ ''_'' -RRB-_-RRB- refer_VBP to_TO the_DT same_JJ objects_NNS -LRB-_-LRB- ``_`` entities_NNS ''_'' -RRB-_-RRB- ._.
Anaphora_NN resolution_NN is_VBZ a_DT specific_JJ example_NN of_IN this_DT task_NN ,_, and_CC is_VBZ specifically_RB concerned_VBN with_IN matching_VBG up_RP pronouns_NNS with_IN the_DT nouns_NNS or_CC names_NNS that_IN they_PRP refer_VBP to_TO ._.
For_IN example_NN ,_, in_IN a_DT sentence_NN such_JJ as_IN ``_`` He_PRP entered_VBD John_NNP 's_POS house_NN through_IN the_DT front_JJ door_NN ''_'' ,_, ``_`` the_DT front_JJ door_NN ''_'' is_VBZ a_DT referring_VBG expression_NN and_CC the_DT bridging_VBG relationship_NN to_TO be_VB identified_VBN is_VBZ the_DT fact_NN that_IN the_DT door_NN being_VBG referred_VBN to_TO is_VBZ the_DT front_JJ door_NN of_IN John_NNP 's_POS house_NN -LRB-_-LRB- rather_RB than_IN of_IN some_DT other_JJ structure_NN that_WDT might_MD also_RB be_VB referred_VBN to_TO -RRB-_-RRB- ._.
Discourse_NN analysis_NN :_: This_DT rubric_NN includes_VBZ a_DT number_NN of_IN related_JJ tasks_NNS ._.
One_CD task_NN is_VBZ identifying_VBG the_DT discourse_NN structure_NN of_IN connected_JJ text_NN ,_, i.e._FW the_DT nature_NN of_IN the_DT discourse_NN relationships_NNS between_IN sentences_NNS -LRB-_-LRB- e.g._FW elaboration_NN ,_, explanation_NN ,_, contrast_NN -RRB-_-RRB- ._.
Another_DT possible_JJ task_NN is_VBZ recognizing_VBG and_CC classifying_VBG the_DT speech_NN acts_VBZ in_IN a_DT chunk_NN of_IN text_NN -LRB-_-LRB- e.g._FW yes-no_FW question_NN ,_, content_NN question_NN ,_, statement_NN ,_, assertion_NN ,_, etc._NN -RRB-_-RRB- ._.
Machine_NN translation_NN :_: Automatically_RB translate_VB text_NN from_IN one_CD human_JJ language_NN to_TO another_DT ._.
This_DT is_VBZ one_CD of_IN the_DT most_RBS difficult_JJ problems_NNS ,_, and_CC is_VBZ a_DT member_NN of_IN a_DT class_NN of_IN problems_NNS colloquially_RB termed_VBN ``_`` AI-complete_JJ ''_'' ,_, i.e._FW requiring_VBG all_DT of_IN the_DT different_JJ types_NNS of_IN knowledge_NN that_IN humans_NNS possess_VBP -LRB-_-LRB- grammar_NN ,_, semantics_NNS ,_, facts_NNS about_IN the_DT real_JJ world_NN ,_, etc._NN -RRB-_-RRB- in_IN order_NN to_TO solve_VB properly_RB ._.
Morphological_JJ segmentation_NN :_: Separate_JJ words_NNS into_IN individual_JJ morphemes_NNS and_CC identify_VBP the_DT class_NN of_IN the_DT morphemes_NNS ._.
The_DT difficulty_NN of_IN this_DT task_NN depends_VBZ greatly_RB on_IN the_DT complexity_NN of_IN the_DT morphology_NN -LRB-_-LRB- i.e._FW the_DT structure_NN of_IN words_NNS -RRB-_-RRB- of_IN the_DT language_NN being_VBG considered_VBN ._.
English_NNP has_VBZ fairly_RB simple_JJ morphology_NN ,_, especially_RB inflectional_JJ morphology_NN ,_, and_CC thus_RB it_PRP is_VBZ often_RB possible_JJ to_TO ignore_VB this_DT task_NN entirely_RB and_CC simply_RB model_VB all_DT possible_JJ forms_NNS of_IN a_DT word_NN -LRB-_-LRB- e.g._FW ``_`` open_JJ ,_, opens_VBZ ,_, opened_VBD ,_, opening_VBG ''_'' -RRB-_-RRB- as_IN separate_JJ words_NNS ._.
In_IN languages_NNS such_JJ as_IN Turkish_JJ ,_, however_RB ,_, such_JJ an_DT approach_NN is_VBZ not_RB possible_JJ ,_, as_IN each_DT dictionary_NN entry_NN has_VBZ thousands_NNS of_IN possible_JJ word_NN forms_NNS ._.
Named_VBN entity_NN recognition_NN -LRB-_-LRB- NER_NN -RRB-_-RRB- :_: Given_VBN a_DT stream_NN of_IN text_NN ,_, determine_VB which_WDT items_NNS in_IN the_DT text_NN map_NN to_TO proper_JJ names_NNS ,_, such_JJ as_IN people_NNS or_CC places_NNS ,_, and_CC what_WP the_DT type_NN of_IN each_DT such_JJ name_NN is_VBZ -LRB-_-LRB- e.g._FW person_NN ,_, location_NN ,_, organization_NN -RRB-_-RRB- ._.
Note_VB that_IN ,_, although_IN capitalization_NN can_MD aid_VB in_IN recognizing_VBG named_VBN entities_NNS in_IN languages_NNS such_JJ as_IN English_NNP ,_, this_DT information_NN can_MD not_RB aid_VB in_IN determining_VBG the_DT type_NN of_IN named_VBN entity_NN ,_, and_CC in_IN any_DT case_NN is_VBZ often_RB inaccurate_JJ or_CC insufficient_JJ ._.
For_IN example_NN ,_, the_DT first_JJ word_NN of_IN a_DT sentence_NN is_VBZ also_RB capitalized_VBN ,_, and_CC named_VBN entities_NNS often_RB span_VBP several_JJ words_NNS ,_, only_RB some_DT of_IN which_WDT are_VBP capitalized_VBN ._.
Furthermore_RB ,_, many_JJ other_JJ languages_NNS in_IN non-Western_JJ scripts_NNS -LRB-_-LRB- e.g._FW Chinese_JJ or_CC Arabic_JJ -RRB-_-RRB- do_VBP not_RB have_VB any_DT capitalization_NN at_IN all_DT ,_, and_CC even_RB languages_NNS with_IN capitalization_NN may_MD not_RB consistently_RB use_VB it_PRP to_TO distinguish_VB names_NNS ._.
For_IN example_NN ,_, German_NNP capitalizes_VBZ all_DT nouns_NNS ,_, regardless_RB of_IN whether_IN they_PRP refer_VBP to_TO names_NNS ,_, and_CC French_NNP and_CC Spanish_NNP do_VBP not_RB capitalize_VB names_NNS that_WDT serve_VBP as_IN adjectives_NNS ._.
Natural_JJ language_NN generation_NN :_: Convert_VB information_NN from_IN computer_NN databases_NNS into_IN readable_JJ human_JJ language_NN ._.
Natural_JJ language_NN understanding_NN :_: Convert_VB chunks_NNS of_IN text_NN into_IN more_RBR formal_JJ representations_NNS such_JJ as_IN first-order_JJ logic_NN structures_NNS that_WDT are_VBP easier_JJR for_IN computer_NN programs_NNS to_TO manipulate_VB ._.
Natural_JJ language_NN understanding_NN involves_VBZ the_DT identification_NN of_IN the_DT intended_VBN semantic_JJ from_IN the_DT multiple_JJ possible_JJ semantics_NNS which_WDT can_MD be_VB derived_VBN from_IN a_DT natural_JJ language_NN expression_NN which_WDT usually_RB takes_VBZ the_DT form_NN of_IN organized_JJ notations_NNS of_IN natural_JJ languages_NNS concepts_NNS ._.
Introduction_NN and_CC creation_NN of_IN language_NN metamodel_NN and_CC ontology_NN are_VBP efficient_JJ however_RB empirical_JJ solutions_NNS ._.
An_DT explicit_JJ formalization_NN of_IN natural_JJ languages_NNS semantics_NNS without_IN confusions_NNS with_IN implicit_JJ assumptions_NNS such_JJ as_IN closed_JJ world_NN assumption_NN -LRB-_-LRB- CWA_NNP -RRB-_-RRB- vs._FW open_JJ world_NN assumption_NN ,_, or_CC subjective_JJ Yes\/No_FW vs._FW objective_NN True\/False_NN is_VBZ expected_VBN for_IN the_DT construction_NN of_IN a_DT basis_NN of_IN semantics_NNS formalization_NN ._.
Optical_NNP character_NN recognition_NN -LRB-_-LRB- OCR_NN -RRB-_-RRB- :_: Given_VBN an_DT image_NN representing_VBG printed_VBN text_NN ,_, determine_VBD the_DT corresponding_JJ text_NN ._.
Part-of-speech_JJ tagging_NN :_: Given_VBN a_DT sentence_NN ,_, determine_VBD the_DT part_NN of_IN speech_NN for_IN each_DT word_NN ._.
Many_JJ words_NNS ,_, especially_RB common_JJ ones_NNS ,_, can_MD serve_VB as_IN multiple_JJ parts_NNS of_IN speech_NN ._.
For_IN example_NN ,_, ``_`` book_NN ''_'' can_MD be_VB a_DT noun_NN -LRB-_-LRB- ``_`` the_DT book_NN on_IN the_DT table_NN ''_'' -RRB-_-RRB- or_CC verb_NN -LRB-_-LRB- ``_`` to_TO book_VB a_DT flight_NN ''_'' -RRB-_-RRB- ;_: ``_`` set_NN ''_'' can_MD be_VB a_DT noun_NN ,_, verb_NN or_CC adjective_NN ;_: and_CC ``_`` out_RP ''_'' can_MD be_VB any_DT of_IN at_IN least_JJS five_CD different_JJ parts_NNS of_IN speech_NN ._.
Note_VB that_IN some_DT languages_NNS have_VBP more_RBR such_JJ ambiguity_NN than_IN others_NNS ._.
Languages_NNS with_IN little_JJ inflectional_JJ morphology_NN ,_, such_JJ as_IN English_NNP are_VBP particularly_RB prone_JJ to_TO such_JJ ambiguity_NN ._.
Chinese_NNP is_VBZ prone_JJ to_TO such_JJ ambiguity_NN because_IN it_PRP is_VBZ a_DT tonal_JJ language_NN during_IN verbalization_NN ._.
Such_JJ inflection_NN is_VBZ not_RB readily_RB conveyed_VBN via_IN the_DT entities_NNS employed_VBN within_IN the_DT orthography_NN to_TO convey_VB intended_JJ meaning_NN ._.
Parsing_VBG :_: Determine_VB the_DT parse_NN tree_NN -LRB-_-LRB- grammatical_JJ analysis_NN -RRB-_-RRB- of_IN a_DT given_VBN sentence_NN ._.
The_DT grammar_NN for_IN natural_JJ languages_NNS is_VBZ ambiguous_JJ and_CC typical_JJ sentences_NNS have_VBP multiple_JJ possible_JJ analyses_NNS ._.
In_IN fact_NN ,_, perhaps_RB surprisingly_RB ,_, for_IN a_DT typical_JJ sentence_NN there_EX may_MD be_VB thousands_NNS of_IN potential_JJ parses_NNS -LRB-_-LRB- most_JJS of_IN which_WDT will_MD seem_VB completely_RB nonsensical_JJ to_TO a_DT human_JJ -RRB-_-RRB- ._.
Question_NN answering_NN :_: Given_IN a_DT human-language_JJ question_NN ,_, determine_VBD its_PRP$ answer_NN ._.
Typical_JJ questions_NNS have_VBP a_DT specific_JJ right_JJ answer_NN -LRB-_-LRB- such_JJ as_IN ``_`` What_WP is_VBZ the_DT capital_NN of_IN Canada_NNP ?_. ''_'' -RRB-_-RRB-
,_, but_CC sometimes_RB open-ended_JJ questions_NNS are_VBP also_RB considered_VBN -LRB-_-LRB- such_JJ as_IN ``_`` What_WP is_VBZ the_DT meaning_NN of_IN life_NN ?_. ''_'' -RRB-_-RRB- ._.
Relationship_NN extraction_NN :_: Given_VBN a_DT chunk_NN of_IN text_NN ,_, identify_VBP the_DT relationships_NNS among_IN named_VBN entities_NNS -LRB-_-LRB- e.g._FW who_WP is_VBZ the_DT wife_NN of_IN whom_WP -RRB-_-RRB- ._.
Sentence_NN breaking_NN -LRB-_-LRB- also_RB known_VBN as_IN sentence_NN boundary_NN disambiguation_NN -RRB-_-RRB- :_: Given_VBN a_DT chunk_NN of_IN text_NN ,_, find_VBP the_DT sentence_NN boundaries_NNS ._.
Sentence_NN boundaries_NNS are_VBP often_RB marked_VBN by_IN periods_NNS or_CC other_JJ punctuation_NN marks_NNS ,_, but_CC these_DT same_JJ characters_NNS can_MD serve_VB other_JJ purposes_NNS -LRB-_-LRB- e.g._FW marking_VBG abbreviations_NNS -RRB-_-RRB- ._.
Sentiment_NN analysis_NN :_: Extract_NN subjective_JJ information_NN usually_RB from_IN a_DT set_NN of_IN documents_NNS ,_, often_RB using_VBG online_JJ reviews_NNS to_TO determine_VB ``_`` polarity_NN ''_'' about_IN specific_JJ objects_NNS ._.
It_PRP is_VBZ especially_RB useful_JJ for_IN identifying_VBG trends_NNS of_IN public_JJ opinion_NN in_IN the_DT social_JJ media_NNS ,_, for_IN the_DT purpose_NN of_IN marketing_NN ._.
Speech_NN recognition_NN :_: Given_IN a_DT sound_JJ clip_NN of_IN a_DT person_NN or_CC people_NNS speaking_VBG ,_, determine_VB the_DT textual_JJ representation_NN of_IN the_DT speech_NN ._.
This_DT is_VBZ the_DT opposite_NN of_IN text_NN to_TO speech_NN and_CC is_VBZ one_CD of_IN the_DT extremely_RB difficult_JJ problems_NNS colloquially_RB termed_VBN ``_`` AI-complete_JJ ''_'' -LRB-_-LRB- see_VB above_RB -RRB-_-RRB- ._.
In_IN natural_JJ speech_NN there_EX are_VBP hardly_RB any_DT pauses_VBZ between_IN successive_JJ words_NNS ,_, and_CC thus_RB speech_NN segmentation_NN is_VBZ a_DT necessary_JJ subtask_NN of_IN speech_NN recognition_NN -LRB-_-LRB- see_VB below_IN -RRB-_-RRB- ._.
Note_VB also_RB that_IN in_IN most_JJS spoken_VBN languages_NNS ,_, the_DT sounds_NNS representing_VBG successive_JJ letters_NNS blend_VBP into_IN each_DT other_JJ in_IN a_DT process_NN termed_VBN coarticulation_NN ,_, so_IN the_DT conversion_NN of_IN the_DT analog_NN signal_NN to_TO discrete_JJ characters_NNS can_MD be_VB a_DT very_RB difficult_JJ process_NN ._.
Speech_NN segmentation_NN :_: Given_IN a_DT sound_JJ clip_NN of_IN a_DT person_NN or_CC people_NNS speaking_VBG ,_, separate_VB it_PRP into_IN words_NNS ._.
A_DT subtask_NN of_IN speech_NN recognition_NN and_CC typically_RB grouped_VBN with_IN it_PRP ._.
Topic_NN segmentation_NN and_CC recognition_NN :_: Given_VBN a_DT chunk_NN of_IN text_NN ,_, separate_VB it_PRP into_IN segments_NNS each_DT of_IN which_WDT is_VBZ devoted_VBN to_TO a_DT topic_NN ,_, and_CC identify_VBP the_DT topic_NN of_IN the_DT segment_NN ._.
Word_NN segmentation_NN :_: Separate_JJ a_DT chunk_NN of_IN continuous_JJ text_NN into_IN separate_JJ words_NNS ._.
For_IN a_DT language_NN like_IN English_NNP ,_, this_DT is_VBZ fairly_RB trivial_JJ ,_, since_IN words_NNS are_VBP usually_RB separated_VBN by_IN spaces_NNS ._.
However_RB ,_, some_DT written_VBN languages_NNS like_IN Chinese_NNP ,_, Japanese_NNP and_CC Thai_NNP do_VBP not_RB mark_VB word_NN boundaries_NNS in_IN such_PDT a_DT fashion_NN ,_, and_CC in_IN those_DT languages_NNS text_NN segmentation_NN is_VBZ a_DT significant_JJ task_NN requiring_VBG knowledge_NN of_IN the_DT vocabulary_NN and_CC morphology_NN of_IN words_NNS in_IN the_DT language_NN ._.
Word_NN sense_NN disambiguation_NN :_: Many_JJ words_NNS have_VBP more_JJR than_IN one_CD meaning_NN ;_: we_PRP have_VBP to_TO select_VB the_DT meaning_NN which_WDT makes_VBZ the_DT most_JJS sense_NN in_IN context_NN ._.
For_IN this_DT problem_NN ,_, we_PRP are_VBP typically_RB given_VBN a_DT list_NN of_IN words_NNS and_CC associated_VBN word_NN senses_NNS ,_, e.g._FW from_IN a_DT dictionary_NN or_CC from_IN an_DT online_JJ resource_NN such_JJ as_IN WordNet_NNP ._.
In_IN some_DT cases_NNS ,_, sets_NNS of_IN related_JJ tasks_NNS are_VBP grouped_VBN into_IN subfields_NNS of_IN NLP_NNP that_WDT are_VBP often_RB considered_VBN separately_RB from_IN NLP_NNP as_IN a_DT whole_NN ._.
Examples_NNS include_VBP :_: Information_NN retrieval_NN -LRB-_-LRB- IR_NN -RRB-_-RRB- :_: This_DT is_VBZ concerned_VBN with_IN storing_VBG ,_, searching_VBG and_CC retrieving_VBG information_NN ._.
It_PRP is_VBZ a_DT separate_JJ field_NN within_IN computer_NN science_NN -LRB-_-LRB- closer_JJR to_TO databases_NNS -RRB-_-RRB- ,_, but_CC IR_NNP relies_VBZ on_IN some_DT NLP_NN methods_NNS -LRB-_-LRB- for_IN example_NN ,_, stemming_VBG -RRB-_-RRB- ._.
Some_DT current_JJ research_NN and_CC applications_NNS seek_VBP to_TO bridge_VB the_DT gap_NN between_IN IR_NNP and_CC NLP_NNP ._.
Information_NN extraction_NN -LRB-_-LRB- IE_NN -RRB-_-RRB- :_: This_DT is_VBZ concerned_VBN in_IN general_JJ with_IN the_DT extraction_NN of_IN semantic_JJ information_NN from_IN text_NN ._.
This_DT covers_VBZ tasks_NNS such_JJ as_IN named_VBN entity_NN recognition_NN ,_, coreference_NN resolution_NN ,_, relationship_NN extraction_NN ,_, etc._NN ._.
Speech_NN processing_NN :_: This_DT covers_VBZ speech_NN recognition_NN ,_, text-to-speech_JJ and_CC related_JJ tasks_NNS ._.
Other_JJ tasks_NNS include_VBP :_: Stemming_VBG Text_NN simplification_NN Text-to-speech_JJ Text-proofing_JJ Natural_NNP language_NN search_NN Query_NNP expansion_NN Automated_NNP essay_NN scoring_VBG Truecasing_NNP Statistical_NNP NLP_NNP Main_NNP article_NN :_: statistical_JJ natural_JJ language_NN processing_NN Statistical_JJ natural-language_JJ processing_NN uses_VBZ stochastic_JJ ,_, probabilistic_JJ and_CC statistical_JJ methods_NNS to_TO resolve_VB some_DT of_IN the_DT difficulties_NNS discussed_VBN above_IN ,_, especially_RB those_DT which_WDT arise_VBP because_IN longer_JJR sentences_NNS are_VBP highly_RB ambiguous_JJ when_WRB processed_VBN with_IN realistic_JJ grammars_NNS ,_, yielding_VBG thousands_NNS or_CC millions_NNS of_IN possible_JJ analyses_NNS ._.
Methods_NNS for_IN disambiguation_NN often_RB involve_VBP the_DT use_NN of_IN corpora_NN and_CC Markov_NNP models_NNS ._.
Statistical_JJ NLP_NN comprises_VBZ all_DT quantitative_JJ approaches_NNS to_TO automated_JJ language_NN processing_NN ,_, including_VBG probabilistic_JJ modeling_NN ,_, information_NN theory_NN ,_, and_CC linear_JJ algebra_NN ._.
The_DT technology_NN for_IN statistical_JJ NLP_NN comes_VBZ mainly_RB from_IN machine_NN learning_NN and_CC data_NN mining_NN ,_, both_DT of_IN which_WDT are_VBP fields_NNS of_IN artificial_JJ intelligence_NN that_WDT involve_VBP learning_VBG from_IN data_NNS ._.
Evaluation_NN of_IN natural_JJ language_NN processing_NN Objectives_NNS The_DT goal_NN of_IN NLP_NN evaluation_NN is_VBZ to_TO measure_VB one_CD or_CC more_JJR qualities_NNS of_IN an_DT algorithm_NN or_CC a_DT system_NN ,_, in_IN order_NN to_TO determine_VB whether_IN -LRB-_-LRB- or_CC to_TO what_WDT extent_NN -RRB-_-RRB- the_DT system_NN answers_VBZ the_DT goals_NNS of_IN its_PRP$ designers_NNS ,_, or_CC meets_VBZ the_DT needs_NNS of_IN its_PRP$ users_NNS ._.
Research_NN in_IN NLP_NN evaluation_NN has_VBZ received_VBN considerable_JJ attention_NN ,_, because_IN the_DT definition_NN of_IN proper_JJ evaluation_NN criteria_NNS is_VBZ one_CD way_NN to_TO specify_VB precisely_RB an_DT NLP_NN problem_NN ,_, going_VBG thus_RB beyond_IN the_DT vagueness_NN of_IN tasks_NNS defined_VBN only_RB as_IN language_NN understanding_NN or_CC language_NN generation_NN ._.
A_DT precise_JJ set_NN of_IN evaluation_NN criteria_NNS ,_, which_WDT includes_VBZ mainly_RB evaluation_NN data_NNS and_CC evaluation_NN metrics_NNS ,_, enables_VBZ several_JJ teams_NNS to_TO compare_VB their_PRP$ solutions_NNS to_TO a_DT given_VBN NLP_NN problem_NN ._.
Short_JJ history_NN of_IN evaluation_NN in_IN NLP_NN The_DT first_JJ evaluation_NN campaign_NN on_IN written_VBN texts_NNS seems_VBZ to_TO be_VB a_DT campaign_NN dedicated_VBN to_TO message_NN understanding_NN in_IN 1987_CD -LRB-_-LRB- Pallet_NN 1998_CD -RRB-_-RRB- ._.
Then_RB ,_, the_DT Parseval\/GEIG_NN project_NN compared_VBD phrase-structure_JJ grammars_NNS -LRB-_-LRB- Black_NNP 1991_CD -RRB-_-RRB- ._.
A_DT series_NN of_IN campaigns_NNS within_IN Tipster_NN project_NN were_VBD realized_VBN on_IN tasks_NNS like_IN summarization_NN ,_, translation_NN and_CC searching_VBG -LRB-_-LRB- Hirschman_NNP 1998_CD -RRB-_-RRB- ._.
In_IN 1994_CD ,_, in_IN Germany_NNP ,_, the_DT Morpholympics_NNPS compared_VBD German_JJ taggers_NNS ._.
Then_RB ,_, the_DT Senseval_NNP and_CC Romanseval_NNP campaigns_NNS were_VBD conducted_VBN with_IN the_DT objectives_NNS of_IN semantic_JJ disambiguation_NN ._.
In_IN 1996_CD ,_, the_DT Sparkle_NNP campaign_NN compared_VBD syntactic_JJ parsers_NNS in_IN four_CD different_JJ languages_NNS -LRB-_-LRB- English_NNP ,_, French_NNP ,_, German_NNP and_CC Italian_NNP -RRB-_-RRB- ._.
In_IN France_NNP ,_, the_DT Grace_NNP project_NN compared_VBD a_DT set_NN of_IN 21_CD taggers_NNS for_IN French_NNP in_IN 1997_CD -LRB-_-LRB- Adda_NNP 1999_CD -RRB-_-RRB- ._.
In_IN 2004_CD ,_, during_IN the_DT Technolangue\/Easy_NN project_NN ,_, 13_CD parsers_NNS for_IN French_NNS were_VBD compared_VBN ._.
Large-scale_JJ evaluation_NN of_IN dependency_NN parsers_NNS were_VBD performed_VBN in_IN the_DT context_NN of_IN the_DT CoNLL_NN shared_VBD tasks_NNS in_IN 2006_CD and_CC 2007_CD ._.
In_IN Italy_NNP ,_, the_DT EVALITA_NNP campaign_NN was_VBD conducted_VBN in_IN 2007_CD and_CC 2009_CD to_TO compare_VB various_JJ NLP_NN and_CC speech_NN tools_NNS for_IN Italian_NNP ;_: the_DT 2011_CD campaign_NN is_VBZ in_IN full_JJ progress_NN -_: EVALITA_NN web_NN site_NN ._.
In_IN France_NNP ,_, within_IN the_DT ANR-Passage_JJ project_NN -LRB-_-LRB- end_NN of_IN 2007_CD -RRB-_-RRB- ,_, 10_CD parsers_NNS for_IN French_NNS were_VBD compared_VBN -_: passage_NN web_NN site_NN ._.
Adda_NNP G._NNP ,_, Mariani_NNP J._NNP ,_, Paroubek_NNP P._NNP ,_, Rajman_NNP M._NNP 1999_CD L'action_NN GRACE_NN d'évaluation_NN de_FW l'assignation_FW des_FW parties_NNS du_NNP discors_NNS pour_VBP le_FW français_FW ._.
Langues_FW vol-2_FW Black_NNP E._NNP ,_, Abney_NNP S._NNP ,_, Flickinger_NNP D._NNP ,_, Gdaniec_NNP C._NNP ,_, Grishman_NNP R._NNP ,_, Harrison_NNP P._NNP ,_, Hindle_NNP D._NNP ,_, Ingria_NNP R._NNP ,_, Jelinek_NNP F._NNP ,_, Klavans_NNP J._NNP ,_, Liberman_NNP M._NNP ,_, Marcus_NNP M._NNP ,_, Reukos_NNP S._NNP ,_, Santoni_NNP B._NNP ,_, Strzalkowski_NNP T._NNP 1991_CD A_NNP procedure_NN for_IN quantitatively_RB comparing_VBG the_DT syntactic_JJ coverage_NN of_IN English_JJ grammars_NNS ._.
DARPA_NNP Speech_NNP and_CC Natural_NNP Language_NNP Workshop_NNP Hirschman_NNP L._NNP 1998_CD Language_NN understanding_NN evaluation_NN :_: lessons_NNS learned_VBN from_IN MUC_NN and_CC ATIS_NN ._.
LREC_NNP Granada_NNP Pallet_NNP D.S._NNP 1998_CD The_NNP NIST_NNP role_NN in_IN automatic_JJ speech_NN recognition_NN benchmark_NN tests_NNS ._.
LREC_NNP Granada_NNP Different_NNP types_NNS of_IN evaluation_NN Depending_VBG on_IN the_DT evaluation_NN procedures_NNS ,_, a_DT number_NN of_IN distinctions_NNS are_VBP traditionally_RB made_VBN in_IN NLP_NN evaluation_NN ._.
Intrinsic_JJ vs._CC extrinsic_JJ evaluation_NN Intrinsic_JJ evaluation_NN considers_VBZ an_DT isolated_VBN NLP_NN system_NN and_CC characterizes_VBZ its_PRP$ performance_NN mainly_RB with_IN respect_NN to_TO a_DT gold_JJ standard_JJ result_NN ,_, pre-defined_JJ by_IN the_DT evaluators_NNS ._.
Extrinsic_JJ evaluation_NN ,_, also_RB called_VBN evaluation_NN in_IN use_NN considers_VBZ the_DT NLP_NN system_NN in_IN a_DT more_RBR complex_JJ setting_NN ,_, either_CC as_IN an_DT embedded_JJ system_NN or_CC serving_VBG a_DT precise_JJ function_NN for_IN a_DT human_JJ user_NN ._.
The_DT extrinsic_JJ performance_NN of_IN the_DT system_NN is_VBZ then_RB characterized_VBN in_IN terms_NNS of_IN its_PRP$ utility_NN with_IN respect_NN to_TO the_DT overall_JJ task_NN of_IN the_DT complex_JJ system_NN or_CC the_DT human_JJ user_NN ._.
For_IN example_NN ,_, consider_VB a_DT syntactic_JJ parser_NN that_WDT is_VBZ based_VBN on_IN the_DT output_NN of_IN some_DT new_JJ part_NN of_IN speech_NN -LRB-_-LRB- POS_NN -RRB-_-RRB- tagger_NN ._.
An_DT intrinsic_JJ evaluation_NN would_MD run_VB the_DT POS_NN tagger_NN on_IN some_DT labeled_JJ data_NNS ,_, and_CC compare_VB the_DT system_NN output_NN of_IN the_DT POS_NN tagger_NN to_TO the_DT gold_JJ standard_NN -LRB-_-LRB- correct_JJ -RRB-_-RRB- output_NN ._.
An_DT extrinsic_JJ evaluation_NN would_MD run_VB the_DT parser_NN with_IN some_DT other_JJ POS_NN tagger_NN ,_, and_CC then_RB with_IN the_DT new_JJ POS_NN tagger_NN ,_, and_CC compare_VB the_DT parsing_VBG accuracy_NN ._.
Black-box_FW vs._FW glass-box_NN evaluation_NN Black-box_JJ evaluation_NN requires_VBZ one_CD to_TO run_VB an_DT NLP_NN system_NN on_IN a_DT given_VBN data_NN set_NN and_CC to_TO measure_VB a_DT number_NN of_IN parameters_NNS related_VBN to_TO the_DT quality_NN of_IN the_DT process_NN -LRB-_-LRB- speed_NN ,_, reliability_NN ,_, resource_NN consumption_NN -RRB-_-RRB- and_CC ,_, most_RBS importantly_RB ,_, to_TO the_DT quality_NN of_IN the_DT result_NN -LRB-_-LRB- e.g._FW the_DT accuracy_NN of_IN data_NNS annotation_NN or_CC the_DT fidelity_NN of_IN a_DT translation_NN -RRB-_-RRB- ._.
Glass-box_JJ evaluation_NN looks_VBZ at_IN the_DT design_NN of_IN the_DT system_NN ,_, the_DT algorithms_NNS that_WDT are_VBP implemented_VBN ,_, the_DT linguistic_JJ resources_NNS it_PRP uses_VBZ -LRB-_-LRB- e.g._FW vocabulary_NN size_NN -RRB-_-RRB- ,_, etc._NN ._.
Given_VBN the_DT complexity_NN of_IN NLP_NN problems_NNS ,_, it_PRP is_VBZ often_RB difficult_JJ to_TO predict_VB performance_NN only_RB on_IN the_DT basis_NN of_IN glass-box_NN evaluation_NN ,_, but_CC this_DT type_NN of_IN evaluation_NN is_VBZ more_RBR informative_JJ with_IN respect_NN to_TO error_NN analysis_NN or_CC future_JJ developments_NNS of_IN a_DT system_NN ._.
Automatic_NNP vs._FW manual_FW evaluation_NN In_IN many_JJ cases_NNS ,_, automatic_JJ procedures_NNS can_MD be_VB defined_VBN to_TO evaluate_VB an_DT NLP_NN system_NN by_IN comparing_VBG its_PRP$ output_NN with_IN the_DT gold_JJ standard_NN -LRB-_-LRB- or_CC desired_VBN -RRB-_-RRB- one_CD ._.
Although_IN the_DT cost_NN of_IN producing_VBG the_DT gold_JJ standard_NN can_MD be_VB quite_RB high_JJ ,_, automatic_JJ evaluation_NN can_MD be_VB repeated_VBN as_RB often_RB as_IN needed_VBN without_IN much_JJ additional_JJ costs_NNS -LRB-_-LRB- on_IN the_DT same_JJ input_NN data_NNS -RRB-_-RRB- ._.
However_RB ,_, for_IN many_JJ NLP_NNP problems_NNS ,_, the_DT definition_NN of_IN a_DT gold_JJ standard_NN is_VBZ a_DT complex_JJ task_NN ,_, and_CC can_MD prove_VB impossible_JJ when_WRB inter-annotator_JJ agreement_NN is_VBZ insufficient_JJ ._.
Manual_JJ evaluation_NN is_VBZ performed_VBN by_IN human_JJ judges_NNS ,_, which_WDT are_VBP instructed_VBN to_TO estimate_VB the_DT quality_NN of_IN a_DT system_NN ,_, or_CC most_RBS often_RB of_IN a_DT sample_NN of_IN its_PRP$ output_NN ,_, based_VBN on_IN a_DT number_NN of_IN criteria_NNS ._.
Although_IN ,_, thanks_NNS to_TO their_PRP$ linguistic_JJ competence_NN ,_, human_JJ judges_NNS can_MD be_VB considered_VBN as_IN the_DT reference_NN for_IN a_DT number_NN of_IN language_NN processing_NN tasks_NNS ,_, there_EX is_VBZ also_RB considerable_JJ variation_NN across_IN their_PRP$ ratings_NNS ._.
This_DT is_VBZ why_WRB automatic_JJ evaluation_NN is_VBZ sometimes_RB referred_VBN to_TO as_IN objective_JJ evaluation_NN ,_, while_IN the_DT human_JJ kind_NN appears_VBZ to_TO be_VB more_RBR subjective_JJ ._.
Shared_VBN tasks_NNS -LRB-_-LRB- Campaigns_NNS -RRB-_-RRB- BioCreative_JJ Message_NN Understanding_VBG Conference_NNP Technolangue\/Easy_NNP Text_NNP Retrieval_NNP Conference_NNP Evaluation_NN exercises_NNS on_IN Semantic_JJ Evaluation_NN -LRB-_-LRB- SemEval_NN -RRB-_-RRB- MorphoChallenge_NN Semi-supervised_JJ and_CC Unsupervised_JJ Morpheme_NNP Analysis_NNP Standardization_NNP in_IN NLP_NNP An_DT ISO_NN sub-committee_NN is_VBZ working_VBG in_IN order_NN to_TO ease_VB interoperability_NN between_IN lexical_JJ resources_NNS and_CC NLP_NN programs_NNS ._.
The_DT sub-committee_NN is_VBZ part_NN of_IN ISO\/TC37_NN and_CC is_VBZ called_VBN ISO\/TC37\/SC4_NN ._.
Some_DT ISO_NN standards_NNS are_VBP already_RB published_VBN but_CC most_JJS of_IN them_PRP are_VBP under_IN construction_NN ,_, mainly_RB on_IN lexicon_NN representation_NN -LRB-_-LRB- see_VB LMF_NN -RRB-_-RRB- ,_, annotation_NN and_CC data_NN category_NN registry_NN ._.
Automatic_NNP summarization_NN is_VBZ the_DT creation_NN of_IN a_DT shortened_VBN version_NN of_IN a_DT text_NN by_IN a_DT computer_NN program_NN ._.
The_DT product_NN of_IN this_DT procedure_NN still_RB contains_VBZ the_DT most_RBS important_JJ points_NNS of_IN the_DT original_JJ text_NN ._.
Discourse_NN analysis_NN -LRB-_-LRB- DA_NN -RRB-_-RRB- ,_, or_CC discourse_NN studies_NNS ,_, is_VBZ a_DT general_JJ term_NN for_IN a_DT number_NN of_IN approaches_NNS to_TO analyzing_NN written_VBN ,_, spoken_VBN ,_, signed_VBN language_NN use_NN or_CC any_DT significant_JJ semiotic_JJ event_NN ._.
Machine_NN translation_NN ,_, sometimes_RB referred_VBN to_TO by_IN the_DT abbreviation_NN MT_NN -LRB-_-LRB- not_RB to_TO be_VB confused_VBN with_IN computer-aided_JJ translation_NN ,_, machine-aided_JJ human_JJ translation_NN MAHT_NN and_CC interactive_JJ translation_NN -RRB-_-RRB- is_VBZ a_DT sub-field_NN of_IN computational_JJ linguistics_NNS that_WDT investigates_VBZ the_DT use_NN of_IN software_NN to_TO translate_VB text_NN or_CC speech_NN from_IN one_CD natural_JJ language_NN to_TO another_DT ._.
On_IN a_DT basic_JJ level_NN ,_, MT_NN performs_VBZ simple_JJ substitution_NN of_IN words_NNS in_IN one_CD natural_JJ language_NN for_IN words_NNS in_IN another_DT ,_, but_CC that_IN alone_RB usually_RB can_MD not_RB produce_VB a_DT good_JJ translation_NN of_IN a_DT text_NN ,_, because_IN recognition_NN of_IN whole_JJ phrases_NNS and_CC their_PRP$ closest_JJS counterparts_NNS in_IN the_DT target_NN language_NN is_VBZ needed_VBN ._.
Solving_VBG this_DT problem_NN with_IN corpus_NN and_CC statistical_JJ techniques_NNS is_VBZ a_DT rapidly_RB growing_VBG field_NN that_WDT is_VBZ leading_VBG to_TO better_JJR translations_NNS ,_, handling_VBG differences_NNS in_IN linguistic_JJ typology_NN ,_, translation_NN of_IN idioms_NNS ,_, and_CC the_DT isolation_NN of_IN anomalies_NNS ._.
-LRB-_-LRB- citation_NN needed_VBN -RRB-_-RRB- Current_JJ machine_NN translation_NN software_NN often_RB allows_VBZ for_IN customisation_NN by_IN domain_NN or_CC profession_NN -LRB-_-LRB- such_JJ as_IN weather_NN reports_NNS -RRB-_-RRB- ,_, improving_VBG output_NN by_IN limiting_VBG the_DT scope_NN of_IN allowable_JJ substitutions_NNS ._.
This_DT technique_NN is_VBZ particularly_RB effective_JJ in_IN domains_NNS where_WRB formal_JJ or_CC formulaic_JJ language_NN is_VBZ used_VBN ._.
It_PRP follows_VBZ that_IN machine_NN translation_NN of_IN government_NN and_CC legal_JJ documents_NNS more_RBR readily_RB produces_VBZ usable_JJ output_NN than_IN conversation_NN or_CC less_RBR standardised_JJ text_NN ._.
Improved_VBN output_NN quality_NN can_MD also_RB be_VB achieved_VBN by_IN human_JJ intervention_NN :_: for_IN example_NN ,_, some_DT systems_NNS are_VBP able_JJ to_TO translate_VB more_RBR accurately_RB if_IN the_DT user_NN has_VBZ unambiguously_RB identified_VBN which_WDT words_NNS in_IN the_DT text_NN are_VBP names_NNS ._.
With_IN the_DT assistance_NN of_IN these_DT techniques_NNS ,_, MT_NNP has_VBZ proven_VBN useful_JJ as_IN a_DT tool_NN to_TO assist_VB human_JJ translators_NNS and_CC ,_, in_IN a_DT very_RB limited_JJ number_NN of_IN cases_NNS ,_, can_MD even_RB produce_VB output_NN that_WDT can_MD be_VB used_VBN as_IN is_VBZ -LRB-_-LRB- e.g._FW ,_, weather_NN reports_NNS -RRB-_-RRB- ._.
The_DT progress_NN and_CC potential_NN of_IN machine_NN translation_NN has_VBZ been_VBN debated_VBN much_RB through_IN its_PRP$ history_NN ._.
Since_IN the_DT 1950s_CD ,_, a_DT number_NN of_IN scholars_NNS have_VBP questioned_VBN the_DT possibility_NN of_IN achieving_VBG fully_RB automatic_JJ machine_NN translation_NN of_IN high_JJ quality_NN ._.
Some_DT critics_NNS claim_VBP that_IN there_EX are_VBP in-principle_JJ obstacles_NNS to_TO automatizing_VBG the_DT translation_NN process_NN ._.
In_IN 1629_CD ,_, René_NNP Descartes_NNP proposed_VBD a_DT universal_JJ language_NN ,_, with_IN equivalent_JJ ideas_NNS in_IN different_JJ tongues_NNS sharing_VBG one_CD symbol_NN ._.
In_IN the_DT 1950s_CD ,_, The_DT Georgetown_NNP experiment_NN -LRB-_-LRB- 1954_CD -RRB-_-RRB- involved_VBN fully_RB automatic_JJ translation_NN of_IN over_IN sixty_CD Russian_JJ sentences_NNS into_IN English_NNP ._.
The_DT experiment_NN was_VBD a_DT great_JJ success_NN and_CC ushered_VBD in_IN an_DT era_NN of_IN substantial_JJ funding_NN for_IN machine-translation_NN research_NN ._.
The_DT authors_NNS claimed_VBD that_IN within_IN three_CD to_TO five_CD years_NNS ,_, machine_NN translation_NN would_MD be_VB a_DT solved_VBN problem_NN ._.
Real_JJ progress_NN was_VBD much_RB slower_JJR ,_, however_RB ,_, and_CC after_IN the_DT ALPAC_NNP report_NN -LRB-_-LRB- 1966_CD -RRB-_-RRB- ,_, which_WDT found_VBD that_IN the_DT ten-year-long_JJ research_NN had_VBD failed_VBN to_TO fulfill_VB expectations_NNS ,_, funding_NN was_VBD greatly_RB reduced_VBN ._.
Beginning_VBG in_IN the_DT late_JJ 1980s_NNS ,_, as_IN computational_JJ power_NN increased_VBD and_CC became_VBD less_RBR expensive_JJ ,_, more_JJR interest_NN was_VBD shown_VBN in_IN statistical_JJ models_NNS for_IN machine_NN translation_NN ._.
The_DT idea_NN of_IN using_VBG digital_JJ computers_NNS for_IN translation_NN of_IN natural_JJ languages_NNS was_VBD proposed_VBN as_RB early_RB as_IN 1946_CD by_IN A._NNP D._NNP Booth_NNP and_CC possibly_RB others_NNS ._.
Warren_NNP Weaver_NNP wrote_VBD an_DT important_JJ memorandum_NN ``_`` Translation_NN ''_'' in_IN 1949_CD ._.
The_DT Georgetown_NNP experiment_NN was_VBD by_IN no_DT means_VBZ the_DT first_JJ such_JJ application_NN ,_, and_CC a_DT demonstration_NN was_VBD made_VBN in_IN 1954_CD on_IN the_DT APEXC_NNP machine_NN at_IN Birkbeck_NNP College_NNP -LRB-_-LRB- University_NNP of_IN London_NNP -RRB-_-RRB- of_IN a_DT rudimentary_JJ translation_NN of_IN English_NNP into_IN French_NNP ._.
Several_JJ papers_NNS on_IN the_DT topic_NN were_VBD published_VBN at_IN the_DT time_NN ,_, and_CC even_RB articles_NNS in_IN popular_JJ journals_NNS -LRB-_-LRB- see_VB for_IN example_NN Wireless_NNP World_NNP ,_, Sept._NNP 1955_CD ,_, Cleave_NNP and_CC Zacharov_NNP -RRB-_-RRB- ._.
A_DT similar_JJ application_NN ,_, also_RB pioneered_VBD at_IN Birkbeck_NNP College_NNP at_IN the_DT time_NN ,_, was_VBD reading_VBG and_CC composing_VBG Braille_NNP texts_NNS by_IN computer_NN ._.
Translation_NN process_NN Main_NNP article_NN :_: Translation_NN process_NN The_DT human_JJ translation_NN process_NN may_MD be_VB described_VBN as_IN :_: Decoding_VBG the_DT meaning_NN of_IN the_DT source_NN text_NN ;_: and_CC Re-encoding_JJ this_DT meaning_NN in_IN the_DT target_NN language_NN ._.
Behind_IN this_DT ostensibly_RB simple_JJ procedure_NN lies_VBZ a_DT complex_JJ cognitive_JJ operation_NN ._.
To_TO decode_VB the_DT meaning_NN of_IN the_DT source_NN text_NN in_IN its_PRP$ entirety_NN ,_, the_DT translator_NN must_MD interpret_VB and_CC analyze_VB all_PDT the_DT features_NNS of_IN the_DT text_NN ,_, a_DT process_NN that_WDT requires_VBZ in-depth_JJ knowledge_NN of_IN the_DT grammar_NN ,_, semantics_NNS ,_, syntax_NN ,_, idioms_NNS ,_, etc._FW ,_, of_IN the_DT source_NN language_NN ,_, as_RB well_RB as_IN the_DT culture_NN of_IN its_PRP$ speakers_NNS ._.
The_DT translator_NN needs_VBZ the_DT same_JJ in-depth_JJ knowledge_NN to_TO re-encode_VB the_DT meaning_NN in_IN the_DT target_NN language_NN ._.
Therein_NNP lies_VBZ the_DT challenge_NN in_IN machine_NN translation_NN :_: how_WRB to_TO program_VB a_DT computer_NN that_WDT will_MD ``_`` understand_VB ''_'' a_DT text_NN as_IN a_DT person_NN does_VBZ ,_, and_CC that_DT will_MD ``_`` create_VB ''_'' a_DT new_JJ text_NN in_IN the_DT target_NN language_NN that_IN ``_`` sounds_NNS ''_'' as_IN if_IN it_PRP has_VBZ been_VBN written_VBN by_IN a_DT person_NN ._.
This_DT problem_NN may_MD be_VB approached_VBN in_IN a_DT number_NN of_IN ways_NNS ._.
Approaches_VBZ Bernard_NNP Vauquois_NNP '_POS pyramid_NN showing_VBG comparative_JJ depths_NNS of_IN intermediary_JJ representation_NN ,_, interlingual_JJ machine_NN translation_NN at_IN the_DT peak_NN ,_, followed_VBN by_IN transfer-based_JJ ,_, then_RB direct_JJ translation_NN ._.
Machine_NN translation_NN can_MD use_VB a_DT method_NN based_VBN on_IN linguistic_JJ rules_NNS ,_, which_WDT means_VBZ that_IN words_NNS will_MD be_VB translated_VBN in_IN a_DT linguistic_JJ way_NN --_: the_DT most_RBS suitable_JJ -LRB-_-LRB- orally_RB speaking_VBG -RRB-_-RRB- words_NNS of_IN the_DT target_NN language_NN will_MD replace_VB the_DT ones_NNS in_IN the_DT source_NN language_NN ._.
It_PRP is_VBZ often_RB argued_VBN that_IN the_DT success_NN of_IN machine_NN translation_NN requires_VBZ the_DT problem_NN of_IN natural_JJ language_NN understanding_NN to_TO be_VB solved_VBN first_RB ._.
Generally_RB ,_, rule-based_JJ methods_NNS parse_VBP a_DT text_NN ,_, usually_RB creating_VBG an_DT intermediary_JJ ,_, symbolic_JJ representation_NN ,_, from_IN which_WDT the_DT text_NN in_IN the_DT target_NN language_NN is_VBZ generated_VBN ._.
According_VBG to_TO the_DT nature_NN of_IN the_DT intermediary_JJ representation_NN ,_, an_DT approach_NN is_VBZ described_VBN as_IN interlingual_JJ machine_NN translation_NN or_CC transfer-based_JJ machine_NN translation_NN ._.
These_DT methods_NNS require_VBP extensive_JJ lexicons_NNS with_IN morphological_JJ ,_, syntactic_JJ ,_, and_CC semantic_JJ information_NN ,_, and_CC large_JJ sets_NNS of_IN rules_NNS ._.
Given_VBN enough_JJ data_NNS ,_, machine_NN translation_NN programs_NNS often_RB work_VBP well_RB enough_RB for_IN a_DT native_JJ speaker_NN of_IN one_CD language_NN to_TO get_VB the_DT approximate_JJ meaning_NN of_IN what_WP is_VBZ written_VBN by_IN the_DT other_JJ native_JJ speaker_NN ._.
The_DT difficulty_NN is_VBZ getting_VBG enough_JJ data_NNS of_IN the_DT right_JJ kind_NN to_TO support_VB the_DT particular_JJ method_NN ._.
For_IN example_NN ,_, the_DT large_JJ multilingual_JJ corpus_NN of_IN data_NNS needed_VBN for_IN statistical_JJ methods_NNS to_TO work_NN is_VBZ not_RB necessary_JJ for_IN the_DT grammar-based_JJ methods_NNS ._.
But_CC then_RB ,_, the_DT grammar_NN methods_NNS need_VBP a_DT skilled_JJ linguist_NN to_TO carefully_RB design_VB the_DT grammar_NN that_IN they_PRP use_VBP ._.
To_TO translate_VB between_IN closely_RB related_JJ languages_NNS ,_, a_DT technique_NN referred_VBN to_TO as_IN shallow-transfer_JJ machine_NN translation_NN may_MD be_VB used_VBN ._.
Rule-based_JJ The_DT rule-based_JJ machine_NN translation_NN paradigm_NN includes_VBZ transfer-based_JJ machine_NN translation_NN ,_, interlingual_JJ machine_NN translation_NN and_CC dictionary-based_JJ machine_NN translation_NN paradigms_NNS ._.
Main_NNP article_NN :_: Rule-based_JJ machine_NN translation_NN Transfer-based_JJ machine_NN translation_NN Main_NNP article_NN :_: Transfer-based_JJ machine_NN translation_NN Interlingual_NNP Main_NNP article_NN :_: Interlingual_JJ machine_NN translation_NN Interlingual_JJ machine_NN translation_NN is_VBZ one_CD instance_NN of_IN rule-based_JJ machine-translation_NN approaches_NNS ._.
In_IN this_DT approach_NN ,_, the_DT source_NN language_NN ,_, i.e._FW the_DT text_NN to_TO be_VB translated_VBN ,_, is_VBZ transformed_VBN into_IN an_DT interlingual_JJ ,_, i.e._FW source_NN -_: \/_: target-language-independent_JJ representation_NN ._.
The_DT target_NN language_NN is_VBZ then_RB generated_VBN out_IN of_IN the_DT interlingua_NN ._.
Dictionary-based_JJ Main_NNP article_NN :_: Dictionary-based_JJ machine_NN translation_NN Machine_NN translation_NN can_MD use_VB a_DT method_NN based_VBN on_IN dictionary_NN entries_NNS ,_, which_WDT means_VBZ that_IN the_DT words_NNS will_MD be_VB translated_VBN as_IN they_PRP are_VBP by_IN a_DT dictionary_NN ._.
Statistical_JJ Main_NNP article_NN :_: Statistical_JJ machine_NN translation_NN Statistical_JJ machine_NN translation_NN tries_VBZ to_TO generate_VB translations_NNS using_VBG statistical_JJ methods_NNS based_VBN on_IN bilingual_JJ text_NN corpora_NN ,_, such_JJ as_IN the_DT Canadian_NNP Hansard_NNP corpus_NN ,_, the_DT English-French_JJ record_NN of_IN the_DT Canadian_JJ parliament_NN and_CC EUROPARL_NN ,_, the_DT record_NN of_IN the_DT European_NNP Parliament_NNP ._.
Where_WRB such_JJ corpora_NNS are_VBP available_JJ ,_, impressive_JJ results_NNS can_MD be_VB achieved_VBN translating_VBG texts_NNS of_IN a_DT similar_JJ kind_NN ,_, but_CC such_JJ corpora_NNS are_VBP still_RB very_RB rare_JJ ._.
The_DT first_JJ statistical_JJ machine_NN translation_NN software_NN was_VBD CANDIDE_NNP from_IN IBM_NNP ._.
Google_NNP used_VBD SYSTRAN_NNP for_IN several_JJ years_NNS ,_, but_CC switched_VBD to_TO a_DT statistical_JJ translation_NN method_NN in_IN October_NNP 2007_CD ._.
Recently_RB ,_, they_PRP improved_VBD their_PRP$ translation_NN capabilities_NNS by_IN inputting_VBG approximately_RB 200_CD billion_CD words_NNS from_IN United_NNP Nations_NNP materials_NNS to_TO train_VB their_PRP$ system_NN ._.
Accuracy_NN of_IN the_DT translation_NN has_VBZ improved_VBN ._.
Example-based_JJ Main_NNP article_NN :_: Example-based_JJ machine_NN translation_NN Example-based_JJ machine_NN translation_NN -LRB-_-LRB- EBMT_NN -RRB-_-RRB- approach_NN was_VBD proposed_VBN by_IN Makoto_NNP Nagao_NNP in_IN 1984_CD ._.
It_PRP is_VBZ often_RB characterised_VBN by_IN its_PRP$ use_NN of_IN a_DT bilingual_JJ corpus_NN as_IN its_PRP$ main_JJ knowledge_NN base_NN ,_, at_IN run-time_NN ._.
It_PRP is_VBZ essentially_RB a_DT translation_NN by_IN analogy_NN and_CC can_MD be_VB viewed_VBN as_IN an_DT implementation_NN of_IN case-based_JJ reasoning_NN approach_NN of_IN machine_NN learning_NN ._.
Hybrid_NNP MT_NNP Hybrid_NNP machine_NN translation_NN -LRB-_-LRB- HMT_NN -RRB-_-RRB- leverages_VBZ the_DT strengths_NNS of_IN statistical_JJ and_CC rule-based_JJ translation_NN methodologies_NNS ._.
Several_JJ MT_NN companies_NNS -LRB-_-LRB- Asia_NNP Online_NNP ,_, LinguaSys_NNP ,_, Systran_NNP ,_, PangeaMT_NN ,_, UPV_NN -RRB-_-RRB- are_VBP claiming_VBG to_TO have_VB a_DT hybrid_NN approach_NN using_VBG both_DT rules_NNS and_CC statistics_NNS ._.
The_DT approaches_NNS differ_VBP in_IN a_DT number_NN of_IN ways_NNS :_: Rules_NNS post-processed_JJ by_IN statistics_NNS :_: Translations_NNS are_VBP performed_VBN using_VBG a_DT rules_NNS based_VBN engine_NN ._.
Statistics_NNS are_VBP then_RB used_VBN in_IN an_DT attempt_NN to_TO adjust\/correct_VB the_DT output_NN from_IN the_DT rules_NNS engine_NN ._.
Statistics_NNS guided_VBN by_IN rules_NNS :_: Rules_NNS are_VBP used_VBN to_TO pre-process_JJ data_NNS in_IN an_DT attempt_NN to_TO better_RB guide_VB the_DT statistical_JJ engine_NN ._.
Rules_NNS are_VBP also_RB used_VBN to_TO post-process_VB the_DT statistical_JJ output_NN to_TO perform_VB functions_NNS such_JJ as_IN normalization_NN ._.
This_DT approach_NN has_VBZ a_DT lot_NN more_JJR power_NN ,_, flexibility_NN and_CC control_NN when_WRB translating_VBG ._.
Major_JJ issues_NNS Disambiguation_NNP Main_NNP article_NN :_: Word_NN sense_NN disambiguation_NN Word-sense_JJ disambiguation_NN concerns_NNS finding_VBG a_DT suitable_JJ translation_NN when_WRB a_DT word_NN can_MD have_VB more_JJR than_IN one_CD meaning_NN ._.
The_DT problem_NN was_VBD first_RB raised_VBN in_IN the_DT 1950s_CD by_IN Yehoshua_NNP Bar-Hillel_NNP ._.
He_PRP pointed_VBD out_RP that_IN without_IN a_DT ``_`` universal_JJ encyclopedia_NN ''_'' ,_, a_DT machine_NN would_MD never_RB be_VB able_JJ to_TO distinguish_VB between_IN the_DT two_CD meanings_NNS of_IN a_DT word_NN ._.
Today_NN there_EX are_VBP numerous_JJ approaches_NNS designed_VBN to_TO overcome_VB this_DT problem_NN ._.
They_PRP can_MD be_VB approximately_RB divided_VBN into_IN ``_`` shallow_JJ ''_'' approaches_NNS and_CC ``_`` deep_JJ ''_'' approaches_NNS ._.
Shallow_JJ approaches_NNS assume_VBP no_DT knowledge_NN of_IN the_DT text_NN ._.
They_PRP simply_RB apply_VB statistical_JJ methods_NNS to_TO the_DT words_NNS surrounding_VBG the_DT ambiguous_JJ word_NN ._.
Deep_JJ approaches_NNS presume_VBP a_DT comprehensive_JJ knowledge_NN of_IN the_DT word_NN ._.
So_RB far_RB ,_, shallow_JJ approaches_NNS have_VBP been_VBN more_RBR successful_JJ ._.
-LRB-_-LRB- citation_NN needed_VBN -RRB-_-RRB- The_DT late_JJ Claude_NNP Piron_NNP ,_, a_DT long-time_JJ translator_NN for_IN the_DT United_NNP Nations_NNPS and_CC the_DT World_NNP Health_NNP Organization_NNP ,_, wrote_VBD that_IN machine_NN translation_NN ,_, at_IN its_PRP$ best_JJS ,_, automates_VBZ the_DT easier_JJR part_NN of_IN a_DT translator_NN 's_POS job_NN ;_: the_DT harder_JJR and_CC more_RBR time-consuming_JJ part_NN usually_RB involves_VBZ doing_VBG extensive_JJ research_NN to_TO resolve_VB ambiguities_NNS in_IN the_DT source_NN text_NN ,_, which_WDT the_DT grammatical_JJ and_CC lexical_JJ exigencies_NNS of_IN the_DT target_NN language_NN require_VBP to_TO be_VB resolved_VBN :_: Why_WRB does_VBZ a_DT translator_NN need_VB a_DT whole_JJ workday_NN to_TO translate_VB five_CD pages_NNS ,_, and_CC not_RB an_DT hour_NN or_CC two_CD ?_.
..._: About_IN 90_CD %_NN of_IN an_DT average_JJ text_NN corresponds_VBZ to_TO these_DT simple_JJ conditions_NNS ._.
But_CC unfortunately_RB ,_, there_EX 's_VBZ the_DT other_JJ 10_CD %_NN ._.
It_PRP 's_VBZ that_DT part_NN that_WDT requires_VBZ six_CD -LRB-_-LRB- more_JJR -RRB-_-RRB- hours_NNS of_IN work_NN ._.
There_EX are_VBP ambiguities_NNS one_CD has_VBZ to_TO resolve_VB ._.
For_IN instance_NN ,_, the_DT author_NN of_IN the_DT source_NN text_NN ,_, an_DT Australian_JJ physician_NN ,_, cited_VBD the_DT example_NN of_IN an_DT epidemic_JJ which_WDT was_VBD declared_VBN during_IN World_NNP War_NNP II_NNP in_IN a_DT ``_`` Japanese_JJ prisoner_NN of_IN war_NN camp_NN ''_'' ._.
Was_VBD he_PRP talking_VBG about_IN an_DT American_JJ camp_NN with_IN Japanese_JJ prisoners_NNS or_CC a_DT Japanese_JJ camp_NN with_IN American_JJ prisoners_NNS ?_.
The_DT English_NNP has_VBZ two_CD senses_NNS ._.
It_PRP 's_VBZ necessary_JJ therefore_RB to_TO do_VB research_NN ,_, maybe_RB to_TO the_DT extent_NN of_IN a_DT phone_NN call_NN to_TO Australia_NNP ._.
The_DT ideal_JJ deep_JJ approach_NN would_MD require_VB the_DT translation_NN software_NN to_TO do_VB all_PDT the_DT research_NN necessary_JJ for_IN this_DT kind_NN of_IN disambiguation_NN on_IN its_PRP$ own_JJ ;_: but_CC this_DT would_MD require_VB a_DT higher_JJR degree_NN of_IN AI_NN than_IN has_VBZ yet_RB been_VBN attained_VBN ._.
A_DT shallow_JJ approach_NN which_WDT simply_RB guessed_VBD at_IN the_DT sense_NN of_IN the_DT ambiguous_JJ English_NNP phrase_NN that_IN Piron_NNP mentions_VBZ -LRB-_-LRB- based_VBN ,_, perhaps_RB ,_, on_IN which_WDT kind_NN of_IN prisoner-of-war_JJ camp_NN is_VBZ more_RBR often_RB mentioned_VBN in_IN a_DT given_VBN corpus_NN -RRB-_-RRB- would_MD have_VB a_DT reasonable_JJ chance_NN of_IN guessing_VBG wrong_JJ fairly_RB often_RB ._.
A_DT shallow_JJ approach_NN that_WDT involves_VBZ ``_`` ask_VB the_DT user_NN about_IN each_DT ambiguity_NN ''_'' would_MD ,_, by_IN Piron_NNP 's_POS estimate_NN ,_, only_RB automate_VBP about_IN 25_CD %_NN of_IN a_DT professional_JJ translator_NN 's_POS job_NN ,_, leaving_VBG the_DT harder_JJR 75_CD %_NN still_RB to_TO be_VB done_VBN by_IN a_DT human_JJ ._.
The_DT objects_NNS of_IN discourse_NN analysis_NN --_: discourse_NN ,_, writing_NN ,_, conversation_NN ,_, communicative_JJ event_NN ,_, etc._NN --_: are_VBP variously_RB defined_VBN in_IN terms_NNS of_IN coherent_JJ sequences_NNS of_IN sentences_NNS ,_, propositions_NNS ,_, speech_NN acts_NNS or_CC turns-at-talk_NN ._.
Contrary_JJ to_TO much_JJ of_IN traditional_JJ linguistics_NNS ,_, discourse_NN analysts_NNS not_RB only_RB study_VB language_NN use_NN `_`` beyond_IN the_DT sentence_NN boundary_NN '_'' ,_, but_CC also_RB prefer_VBP to_TO analyze_VB `_`` naturally_RB occurring_VBG '_POS language_NN use_NN ,_, and_CC not_RB invented_VBN examples_NNS ._.
Text_NN linguistics_NNS is_VBZ related_VBN ._.
The_DT essential_JJ difference_NN between_IN discourse_NN analysis_NN and_CC text_NN linguistics_NNS is_VBZ that_IN it_PRP aims_VBZ at_IN revealing_VBG socio-psychological_JJ characteristics_NNS of_IN a_DT person\/persons_NNS rather_RB than_IN text_NN structure_NN ._.
Discourse_NN analysis_NN has_VBZ been_VBN taken_VBN up_RP in_IN a_DT variety_NN of_IN social_JJ science_NN disciplines_NNS ,_, including_VBG linguistics_NNS ,_, sociology_NN ,_, anthropology_NN ,_, social_JJ work_NN ,_, cognitive_JJ psychology_NN ,_, social_JJ psychology_NN ,_, international_JJ relations_NNS ,_, human_JJ geography_NN ,_, communication_NN studies_NNS and_CC translation_NN studies_NNS ,_, each_DT of_IN which_WDT is_VBZ subject_JJ to_TO its_PRP$ own_JJ assumptions_NNS ,_, dimensions_NNS of_IN analysis_NN ,_, and_CC methodologies_NNS ._.
The_DT examples_NNS and_CC perspective_NN in_IN this_DT article_NN deal_NN primarily_RB with_IN the_DT United_NNP States_NNPS and_CC do_VBP not_RB represent_VB a_DT worldwide_JJ view_NN of_IN the_DT subject_NN ._.
Please_VB improve_VB this_DT article_NN and_CC discuss_VBP the_DT issue_NN on_IN the_DT talk_NN page_NN ._.
-LRB-_-LRB- December_NNP 2010_CD -RRB-_-RRB- Some_DT scholars_NNS -LRB-_-LRB- which_WDT ?_. -RRB-_-RRB-
consider_VB the_DT Austrian_JJ emigre_NN Leo_NNP Spitzer_NNP 's_POS Stilstudien_NNP -LRB-_-LRB- Style_NNP Studies_NNP -RRB-_-RRB- of_IN 1928_CD the_DT earliest_JJS example_NN of_IN discourse_NN analysis_NN -LRB-_-LRB- DA_NN -RRB-_-RRB- ;_: Michel_NNP Foucault_NNP himself_PRP translated_VBD it_PRP into_IN French_NNP ._.
But_CC the_DT term_NN first_RB came_VBD into_IN general_JJ use_NN following_VBG the_DT publication_NN of_IN a_DT series_NN of_IN papers_NNS by_IN Zellig_NNP Harris_NNP beginning_VBG in_IN 1952_CD and_CC reporting_VBG on_IN work_NN from_IN which_WDT he_PRP developed_VBD transformational_JJ grammar_NN in_IN the_DT late_JJ 1930s_NNS ._.
Formal_JJ equivalence_JJ relations_NNS among_IN the_DT sentences_NNS of_IN a_DT coherent_JJ discourse_NN are_VBP made_VBN explicit_JJ by_IN using_VBG sentence_NN transformations_NNS to_TO put_VB the_DT text_NN in_IN a_DT canonical_JJ form_NN ._.
Words_NNS and_CC sentences_NNS with_IN equivalent_JJ information_NN then_RB appear_VBP in_IN the_DT same_JJ column_NN of_IN an_DT array_NN ._.
This_DT work_NN progressed_VBD over_IN the_DT next_JJ four_CD decades_NNS -LRB-_-LRB- see_VB references_NNS -RRB-_-RRB- into_IN a_DT science_NN of_IN sublanguage_NN analysis_NN -LRB-_-LRB- Kittredge_NNP &_CC Lehrberger_NNP 1982_CD -RRB-_-RRB- ,_, culminating_VBG in_IN a_DT demonstration_NN of_IN the_DT informational_JJ structures_NNS in_IN texts_NNS of_IN a_DT sublanguage_NN of_IN science_NN ,_, that_DT of_IN immunology_NN ,_, -LRB-_-LRB- Harris_NNP et_FW al._FW 1989_CD -RRB-_-RRB- and_CC a_DT fully_RB articulated_VBN theory_NN of_IN linguistic_JJ informational_JJ content_NN -LRB-_-LRB- Harris_NNP 1991_CD -RRB-_-RRB- ._.
During_IN this_DT time_NN ,_, however_RB ,_, most_JJS linguists_NNS decided_VBD a_DT succession_NN of_IN elaborate_JJ theories_NNS of_IN sentence-level_JJ syntax_NN and_CC semantics_NNS ._.
Although_IN Harris_NNP had_VBD mentioned_VBN the_DT analysis_NN of_IN whole_JJ discourses_NNS ,_, he_PRP had_VBD not_RB worked_VBN out_RP a_DT comprehensive_JJ model_NN ,_, as_IN of_IN January_NNP ,_, 1952_CD ._.
A_DT linguist_NN working_VBG for_IN the_DT American_NNP Bible_NNP Society_NNP ,_, James_NNP A._NNP Lauriault\/Loriot_NNP ,_, needed_VBN to_TO find_VB answers_NNS to_TO some_DT fundamental_JJ errors_NNS in_IN translating_VBG Quechua_NNP ,_, in_IN the_DT Cuzco_NNP area_NN of_IN Peru_NNP ._.
He_PRP took_VBD Harris_NNP 's_POS idea_NN ,_, recorded_VBD all_DT of_IN the_DT legends_NNS and_CC ,_, after_IN going_VBG over_IN the_DT meaning_NN and_CC placement_NN of_IN each_DT word_NN with_IN a_DT native_JJ speaker_NN of_IN Quechua_NNP ,_, was_VBD able_JJ to_TO form_VB logical_JJ ,_, mathematical_JJ rules_NNS that_WDT transcended_VBD the_DT simple_JJ sentence_NN structure_NN ._.
He_PRP then_RB applied_VBD the_DT process_NN to_TO another_DT language_NN of_IN Eastern_NNP Peru_NNP ,_, Shipibo_NNP ._.
He_PRP taught_VBD the_DT theory_NN in_IN Norman_NNP ,_, Oklahoma_NNP ,_, in_IN the_DT summers_NNS of_IN 1956_CD and_CC 1957_CD and_CC entered_VBD the_DT University_NNP of_IN Pennsylvania_NNP in_IN the_DT interim_JJ year_NN ._.
He_PRP tried_VBD to_TO publish_VB a_DT paper_NN Shipibo_NNP Paragraph_NNP Structure_NN ,_, but_CC it_PRP was_VBD delayed_VBN until_IN 1970_CD -LRB-_-LRB- Loriot_NNP &_CC Hollenbach_NNP 1970_CD -RRB-_-RRB- ._.
In_IN the_DT meantime_NN ,_, Dr._NNP Kenneth_NNP Lee_NNP Pike_NNP ,_, a_DT professor_NN at_IN University_NNP of_IN Michigan_NNP ,_, Ann_NNP Arbor_NNP ,_, taught_VBD the_DT theory_NN ,_, and_CC one_CD of_IN his_PRP$ students_NNS ,_, Robert_NNP E._NNP Longacre_NNP ,_, was_VBD able_JJ to_TO disseminate_VB it_PRP in_IN a_DT dissertation_NN ._.
Harris_NNP 's_POS methodology_NN was_VBD developed_VBN into_IN a_DT system_NN for_IN the_DT computer-aided_JJ analysis_NN of_IN natural_JJ language_NN by_IN a_DT team_NN led_VBN by_IN Naomi_NNP Sager_NNP at_IN NYU_NNP ,_, which_WDT has_VBZ been_VBN applied_VBN to_TO a_DT number_NN of_IN sublanguage_JJ domains_NNS ,_, most_RBS notably_RB to_TO medical_JJ informatics_NNS ._.
The_DT software_NN for_IN the_DT Medical_NNP Language_NNP Processor_NNP is_VBZ publicly_RB available_JJ on_IN SourceForge_NNP ._.
In_IN the_DT late_JJ 1960s_NNS and_CC 1970s_NNS ,_, and_CC without_IN reference_NN to_TO this_DT prior_JJ work_NN ,_, a_DT variety_NN of_IN other_JJ approaches_NNS to_TO a_DT new_JJ cross-discipline_NN of_IN DA_NNP began_VBD to_TO develop_VB in_IN most_JJS of_IN the_DT humanities_NNS and_CC social_JJ sciences_NNS concurrently_RB with_IN ,_, and_CC related_JJ to_TO ,_, other_JJ disciplines_NNS ,_, such_JJ as_IN semiotics_NNS ,_, psycholinguistics_NNS ,_, sociolinguistics_NNS ,_, and_CC pragmatics_NNS ._.
Many_JJ of_IN these_DT approaches_NNS ,_, especially_RB those_DT influenced_VBN by_IN the_DT social_JJ sciences_NNS ,_, favor_VBP a_DT more_RBR dynamic_JJ study_NN of_IN oral_JJ talk-in-interaction_NN ._.
Mention_NN must_MD also_RB be_VB made_VBN of_IN the_DT term_NN ``_`` Conversational_JJ analysis_NN ''_'' ,_, which_WDT was_VBD influenced_VBN by_IN the_DT Sociologist_NNP Harold_NNP Garfinkel_NNP who_WP is_VBZ the_DT founder_NN of_IN Ethnomethodology_NN ._.
In_IN Europe_NNP ,_, Michel_NNP Foucault_NNP became_VBD one_CD of_IN the_DT key_JJ theorists_NNS of_IN the_DT subject_NN ,_, especially_RB of_IN discourse_NN ,_, and_CC wrote_VBD The_DT Archaeology_NN of_IN Knowledge_NN on_IN the_DT subject_NN ._.
Topics_NNS of_IN interest_NN Topics_NNS of_IN discourse_NN analysis_NN include_VBP :_: The_DT various_JJ levels_NNS or_CC dimensions_NNS of_IN discourse_NN ,_, such_JJ as_IN sounds_NNS -LRB-_-LRB- intonation_NN ,_, etc._NN -RRB-_-RRB- ,_, gestures_NNS ,_, syntax_NN ,_, the_DT lexicon_NN ,_, style_NN ,_, rhetoric_NN ,_, meanings_NNS ,_, speech_NN acts_NNS ,_, moves_NNS ,_, strategies_NNS ,_, turns_NNS and_CC other_JJ aspects_NNS of_IN interaction_NN Genres_NNS of_IN discourse_NN -LRB-_-LRB- various_JJ types_NNS of_IN discourse_NN in_IN politics_NNS ,_, the_DT media_NNS ,_, education_NN ,_, science_NN ,_, business_NN ,_, etc._NN -RRB-_-RRB- The_DT relations_NNS between_IN discourse_NN and_CC the_DT emergence_NN of_IN syntactic_JJ structure_NN The_DT relations_NNS between_IN text_NN -LRB-_-LRB- discourse_NN -RRB-_-RRB- and_CC context_NN The_DT relations_NNS between_IN discourse_NN and_CC power_NN The_DT relations_NNS between_IN discourse_NN and_CC interaction_NN The_DT relations_NNS between_IN discourse_NN and_CC cognition_NN and_CC memory_NN Political_JJ discourse_NN Political_JJ discourse_NN analysis_NN is_VBZ a_DT field_NN of_IN discourse_NN analysis_NN which_WDT focuses_VBZ on_IN discourse_NN in_IN political_JJ forums_NNS -LRB-_-LRB- such_JJ as_IN debates_NNS ,_, speeches_NNS ,_, and_CC hearings_NNS -RRB-_-RRB- as_IN the_DT phenomenon_NN of_IN interest_NN ._.
Political_JJ discourse_NN is_VBZ the_DT informal_JJ exchange_NN of_IN reasoned_VBN views_NNS as_IN to_TO which_WDT of_IN several_JJ alternative_JJ courses_NNS of_IN action_NN should_MD be_VB taken_VBN to_TO solve_VB a_DT societal_JJ problem_NN ._.
It_PRP is_VBZ a_DT science_NN that_WDT has_VBZ been_VBN used_VBN through_IN the_DT history_NN of_IN the_DT United_NNP States_NNPS ._.
It_PRP is_VBZ the_DT essence_NN of_IN democracy_NN ._.
Full_JJ of_IN problems_NNS and_CC persuasion_NN ,_, political_JJ discourse_NN is_VBZ used_VBN in_IN many_JJ debates_NNS ,_, candidacies_NNS and_CC in_IN our_PRP$ everyday_JJ life_NN ._.
Perspectives_NNS The_DT following_NN are_VBP some_DT of_IN the_DT specific_JJ theoretical_JJ perspectives_NNS and_CC analytical_JJ approaches_NNS used_VBN in_IN linguistic_JJ discourse_NN analysis_NN :_: Emergent_JJ grammar_NN Text_NN grammar_NN -LRB-_-LRB- or_CC `_`` discourse_NN grammar_NN '_'' -RRB-_-RRB- Cohesion_NN and_CC relevance_NN theory_NN Functional_JJ grammar_NN Rhetoric_NN Stylistics_NNS -LRB-_-LRB- linguistics_NNS -RRB-_-RRB- Interactional_JJ sociolinguistics_NNS Ethnography_NN of_IN communication_NN Pragmatics_NNS ,_, particularly_RB speech_NN act_NN theory_NN Conversation_NN analysis_NN Variation_NN analysis_NN Applied_NNP linguistics_VBZ Cognitive_JJ psychology_NN ,_, often_RB under_IN the_DT label_NN discourse_NN processing_NN ,_, studying_VBG the_DT production_NN and_CC comprehension_NN of_IN discourse_NN ._.
Discursive_JJ psychology_NN Response_NNP based_VBN therapy_NN -LRB-_-LRB- counselling_NN -RRB-_-RRB- Critical_JJ discourse_NN analysis_NN Sublanguage_NN analysis_NN Genre_NNP Analysis_NNP &_CC Critical_JJ Genre_NN Analysis_NN Although_IN these_DT approaches_NNS emphasize_VBP different_JJ aspects_NNS of_IN language_NN use_NN ,_, they_PRP all_DT view_VBP language_NN as_IN social_JJ interaction_NN ,_, and_CC are_VBP concerned_VBN with_IN the_DT social_JJ contexts_NNS in_IN which_WDT discourse_NN is_VBZ embedded_JJ ._.
Often_RB a_DT distinction_NN is_VBZ made_VBN between_IN `_`` local_JJ '_'' structures_NNS of_IN discourse_NN -LRB-_-LRB- such_JJ as_IN relations_NNS among_IN sentences_NNS ,_, propositions_NNS ,_, and_CC turns_NNS -RRB-_-RRB- and_CC `_`` global_JJ '_'' structures_NNS ,_, such_JJ as_IN overall_JJ topics_NNS and_CC the_DT schematic_JJ organization_NN of_IN discourses_NNS and_CC conversations_NNS ._.
For_IN instance_NN ,_, many_JJ types_NNS of_IN discourse_NN begin_VB with_IN some_DT kind_NN of_IN global_JJ `_`` summary_NN '_'' ,_, in_IN titles_NNS ,_, headlines_NNS ,_, leads_NNS ,_, abstracts_NNS ,_, and_CC so_RB on_RB ._.
A_DT problem_NN for_IN the_DT discourse_NN analyst_NN is_VBZ to_TO decide_VB when_WRB a_DT particular_JJ feature_NN is_VBZ relevant_JJ to_TO the_DT specification_NN is_VBZ required_VBN ._.
Are_VBP there_RB general_JJ principles_NNS which_WDT will_MD determine_VB the_DT relevance_NN or_CC nature_NN of_IN the_DT specification_NN ._.
Prominent_JJ discourse_NN analysts_NNS This_DT article_NN contains_VBZ embedded_JJ lists_NNS that_WDT may_MD be_VB poorly_RB defined_VBN ,_, unverified_JJ or_CC indiscriminate_JJ ._.
Please_VB help_NN to_TO clean_VB it_PRP up_RP to_TO meet_VB Wikipedia_NNP 's_POS quality_NN standards_NNS ._.
-LRB-_-LRB- May_NNP 2012_CD -RRB-_-RRB- Marc_NNP Angenot_NNP ,_, Robert_NNP de_IN Beaugrande_NNP ,_, Jan_NNP Blommaert_NNP ,_, Adriana_NNP Bolivar_NNP ,_, Carmen_NNP Rosa_NNP Caldas-Coulthard_NNP ,_, Robyn_NNP Carston_NNP ,_, Wallace_NNP Chafe_NNP ,_, Paul_NNP Chilton_NNP ,_, Guy_NNP Cook_NNP ,_, Malcolm_NNP Coulthard_NNP ,_, James_NNP Deese_NNP ,_, Paul_NNP Drew_NNP ,_, John_NNP Du_NNP Bois_NNP ,_, Alessandro_NNP Duranti_NNP ,_, Brenton_NNP D._NNP Faber_NNP ,_, Norman_NNP Fairclough_NNP ,_, Michel_NNP Foucault_NNP ,_, Roger_NNP Fowler_NNP ,_, James_NNP Paul_NNP Gee_NNP ,_, Talmy_NNP Givón_NNP ,_, Charles_NNP Goodwin_NNP ,_, Art_NNP Graesser_NNP ,_, Michael_NNP Halliday_NNP ,_, Zellig_NNP Harris_NNP ,_, John_NNP Heritage_NNP ,_, Janet_NNP Holmes_NNP ,_, David_NNP R._NNP Howarth_NNP ,_, Paul_NNP Hopper_NNP ,_, Gail_NN Jefferson_NNP ,_, Barbara_NNP Johnstone_NNP ,_, Walter_NNP Kintsch_NNP ,_, Richard_NNP Kittredge_NNP ,_, Adam_NNP Jaworski_NNP ,_, William_NNP Labov_NNP ,_, George_NNP Lakoff_NNP ,_, Jay_NNP Lemke_NNP ,_, Stephen_NNP H._NNP Levinsohn_NNP ,_, James_NNP A._NNP Lauriault\/Loriot_NNP ,_, Robert_NNP E._NNP Longacre_NNP ,_, Jim_NNP Martin_NNP ,_, Aletta_NNP Norval_NNP ,_, David_NNP Nunan_NNP ,_, Elinor_NNP Ochs_NNP ,_, Gina_NNP Poncini_NNP ,_, Jonathan_NNP Potter_NNP ,_, Edward_NNP Robinson_NNP ,_, Nikolas_NNP Rose_NNP ,_, Harvey_NNP Sacks_NNP ,_, Svenka_NNP Savic_NNP Naomi_NNP Sager_NNP ,_, Emanuel_NNP Schegloff_NNP ,_, Deborah_NNP Schiffrin_NNP ,_, Michael_NNP Schober_NNP ,_, Stef_NNP Slembrouck_NNP ,_, Michael_NNP Stubbs_NNP ,_, John_NNP Swales_NNP ,_, Deborah_NNP Tannen_NNP ,_, Sandra_NNP Thompson_NNP ,_, Teun_NNP A._NNP van_NNP Dijk_NNP ,_, Theo_NNP van_NNP Leeuwen_NNP ,_, Jef_NNP Verschueren_NNP ,_, Henry_NNP Widdowson_NNP ,_, Carla_NNP Willig_NNP ,_, Deirdre_NNP Wilson_NNP ,_, Ruth_NNP Wodak_NNP ,_, Margaret_NNP Wetherell_NNP ,_, Ernesto_NNP Laclau_NNP ,_, Chantal_NNP Mouffe_NNP ,_, Judith_NNP M._NNP De_NNP Guzman_NNP ,_, Cynthia_NNP Hardy_NNP ,_, Louise_NNP J._NNP Phillips_NNP ._.
-LRB-_-LRB- citation_NN needed_VBN -RRB-_-RRB- Bhatia_NNP ,_, V.J._NNP ,_, John_NNP Swales_NNP ,_, Zellig_NNP Harris_NNP The_DT phenomenon_NN of_IN information_NN overload_NN has_VBZ meant_VBN that_DT access_NN to_TO coherent_JJ and_CC correctly-developed_JJ summaries_NNS is_VBZ vital_JJ ._.
As_IN access_NN to_TO data_NNS has_VBZ increased_VBN so_RB has_VBZ interest_NN in_IN automatic_JJ summarization_NN ._.
An_DT example_NN of_IN the_DT use_NN of_IN summarization_NN technology_NN is_VBZ search_NN engines_NNS such_JJ as_IN Google_NNP ._.
Technologies_NNP that_WDT can_MD make_VB a_DT coherent_JJ summary_NN ,_, of_IN any_DT kind_NN of_IN text_NN ,_, need_VBP to_TO take_VB into_IN account_NN several_JJ variables_NNS such_JJ as_IN length_NN ,_, writing_VBG style_NN and_CC syntax_NN to_TO make_VB a_DT useful_JJ summary_NN ._.
Extractive_JJ methods_NNS work_VBP by_IN selecting_VBG a_DT subset_NN of_IN existing_VBG words_NNS ,_, phrases_NNS ,_, or_CC sentences_NNS in_IN the_DT original_JJ text_NN to_TO form_VB the_DT summary_NN ._.
In_IN contrast_NN ,_, abstractive_JJ methods_NNS build_VBP an_DT internal_JJ semantic_JJ representation_NN and_CC then_RB use_VB natural_JJ language_NN generation_NN techniques_NNS to_TO create_VB a_DT summary_NN that_WDT is_VBZ closer_JJR to_TO what_WP a_DT human_JJ might_NN generate_VB ._.
Such_JJ a_DT summary_NN might_MD contain_VB words_NNS not_RB explicitly_RB present_JJ in_IN the_DT original_NN ._.
The_DT state-of-the-art_JJ abstractive_JJ methods_NNS are_VBP still_RB quite_RB weak_JJ ,_, so_RB most_JJS research_NN has_VBZ focused_VBN on_IN extractive_JJ methods_NNS ,_, and_CC this_DT is_VBZ what_WP we_PRP will_MD cover_VB ._.
Two_CD particular_JJ types_NNS of_IN summarization_NN often_RB addressed_VBN in_IN the_DT literature_NN are_VBP keyphrase_NN extraction_NN ,_, where_WRB the_DT goal_NN is_VBZ to_TO select_VB individual_JJ words_NNS or_CC phrases_NNS to_TO ``_`` tag_VB ''_'' a_DT document_NN ,_, and_CC document_NN summarization_NN ,_, where_WRB the_DT goal_NN is_VBZ to_TO select_VB whole_JJ sentences_NNS to_TO create_VB a_DT short_JJ paragraph_NN summary_NN ._.
Extraction_NN and_CC abstraction_NN Broadly_RB ,_, one_CD distinguishes_VBZ two_CD approaches_NNS :_: extraction_NN and_CC abstraction_NN ._.
Extraction_NN techniques_NNS merely_RB copy_VBP the_DT information_NN deemed_VBD most_RBS important_JJ by_IN the_DT system_NN to_TO the_DT summary_NN -LRB-_-LRB- for_IN example_NN ,_, key_JJ clauses_NNS ,_, sentences_NNS or_CC paragraphs_NNS -RRB-_-RRB- ,_, while_IN abstraction_NN involves_VBZ paraphrasing_VBG sections_NNS of_IN the_DT source_NN document_NN ._.
In_IN general_JJ ,_, abstraction_NN can_MD condense_VB a_DT text_NN more_RBR strongly_RB than_IN extraction_NN ,_, but_CC the_DT programs_NNS that_WDT can_MD do_VB this_DT are_VBP harder_JJR to_TO develop_VB as_IN they_PRP require_VBP the_DT use_NN of_IN natural_JJ language_NN generation_NN technology_NN ,_, which_WDT itself_PRP is_VBZ a_DT growing_VBG field_NN ._.
Types_NNS of_IN summaries_NNS There_EX are_VBP different_JJ types_NNS of_IN summaries_NNS depending_VBG what_WP the_DT summarization_NN program_NN focuses_VBZ on_RP to_TO make_VB the_DT summary_NN of_IN the_DT text_NN ,_, for_IN example_NN generic_JJ summaries_NNS or_CC query_JJ relevant_JJ summaries_NNS -LRB-_-LRB- sometimes_RB called_VBN query-biased_JJ summaries_NNS -RRB-_-RRB- ._.
Summarization_NN systems_NNS are_VBP able_JJ to_TO create_VB both_DT query_JJ relevant_JJ text_NN summaries_NNS and_CC generic_JJ machine-generated_JJ summaries_NNS depending_VBG on_IN what_WP the_DT user_NN needs_NNS ._.
Summarization_NN of_IN multimedia_NNS documents_NNS ,_, e.g._FW pictures_NNS or_CC movies_NNS ,_, is_VBZ also_RB possible_JJ ._.
Some_DT systems_NNS will_MD generate_VB a_DT summary_NN based_VBN on_IN a_DT single_JJ source_NN document_NN ,_, while_IN others_NNS can_MD use_VB multiple_JJ source_NN documents_NNS -LRB-_-LRB- for_IN example_NN ,_, a_DT cluster_NN of_IN news_NN stories_NNS on_IN the_DT same_JJ topic_NN -RRB-_-RRB- ._.
These_DT systems_NNS are_VBP known_VBN as_IN multi-document_JJ summarization_NN systems_NNS ._.
Keyphrase_NN extraction_NN Task_NNP description_NN and_CC example_NN The_DT task_NN is_VBZ the_DT following_NN ._.
You_PRP are_VBP given_VBN a_DT piece_NN of_IN text_NN ,_, such_JJ as_IN a_DT journal_NN article_NN ,_, and_CC you_PRP must_MD produce_VB a_DT list_NN of_IN keywords_NNS or_CC keyphrases_NNS that_WDT capture_VBP the_DT primary_JJ topics_NNS discussed_VBN in_IN the_DT text_NN ._.
In_IN the_DT case_NN of_IN research_NN articles_NNS ,_, many_JJ authors_NNS provide_VBP manually_RB assigned_VBN keywords_NNS ,_, but_CC most_JJS text_NN lacks_VBZ pre-existing_JJ keyphrases_NNS ._.
For_IN example_NN ,_, news_NN articles_NNS rarely_RB have_VBP keyphrases_NNS attached_VBN ,_, but_CC it_PRP would_MD be_VB useful_JJ to_TO be_VB able_JJ to_TO automatically_RB do_VB so_RB for_IN a_DT number_NN of_IN applications_NNS discussed_VBN below_IN ._.
Consider_VB the_DT example_NN text_NN from_IN a_DT recent_JJ news_NN article_NN :_: ``_`` The_DT Army_NNP Corps_NNP of_IN Engineers_NNP ,_, rushing_VBG to_TO meet_VB President_NNP Bush_NNP 's_POS promise_NN to_TO protect_VB New_NNP Orleans_NNP by_IN the_DT start_NN of_IN the_DT 2006_CD hurricane_NN season_NN ,_, installed_VBN defective_JJ flood-control_JJ pumps_NNS last_JJ year_NN despite_IN warnings_NNS from_IN its_PRP$ own_JJ expert_NN that_IN the_DT equipment_NN would_MD fail_VB during_IN a_DT storm_NN ,_, according_VBG to_TO documents_NNS obtained_VBN by_IN The_DT Associated_NNP Press_NNP ''_'' ._.
An_DT extractive_JJ keyphrase_NN extractor_NN might_MD select_VB ``_`` Army_NNP Corps_NNP of_IN Engineers_NNP ''_'' ,_, ``_`` President_NNP Bush_NNP ''_'' ,_, ``_`` New_NNP Orleans_NNP ''_'' ,_, and_CC ``_`` defective_JJ flood-control_JJ pumps_NNS ''_'' as_IN keyphrases_NNS ._.
These_DT are_VBP pulled_VBN directly_RB from_IN the_DT text_NN ._.
In_IN contrast_NN ,_, an_DT abstractive_JJ keyphrase_NN system_NN would_MD somehow_RB internalize_VB the_DT content_NN and_CC generate_VB keyphrases_NNS that_WDT might_MD be_VB more_RBR descriptive_JJ and_CC more_JJR like_IN what_WP a_DT human_JJ would_MD produce_VB ,_, such_JJ as_IN ``_`` political_JJ negligence_NN ''_'' or_CC ``_`` inadequate_JJ protection_NN from_IN floods_NNS ''_'' ._.
Note_VB that_IN these_DT terms_NNS do_VBP not_RB appear_VB in_IN the_DT text_NN and_CC require_VB a_DT deep_JJ understanding_NN ,_, which_WDT makes_VBZ it_PRP difficult_JJ for_IN a_DT computer_NN to_TO produce_VB such_JJ keyphrases_NNS ._.
Keyphrases_NNS have_VBP many_JJ applications_NNS ,_, such_JJ as_IN to_TO improve_VB document_NN browsing_NN by_IN providing_VBG a_DT short_JJ summary_NN ._.
Also_RB ,_, keyphrases_NNS can_MD improve_VB information_NN retrieval_NN --_: if_IN documents_NNS have_VBP keyphrases_NNS assigned_VBN ,_, a_DT user_NN could_MD search_VB by_IN keyphrase_NN to_TO produce_VB more_RBR reliable_JJ hits_NNS than_IN a_DT full-text_JJ search_NN ._.
Also_RB ,_, automatic_JJ keyphrase_NN extraction_NN can_MD be_VB useful_JJ in_IN generating_VBG index_NN entries_NNS for_IN a_DT large_JJ text_NN corpus_NN ._.
Keyphrase_NN extraction_NN as_IN supervised_JJ learning_NN Beginning_NN with_IN the_DT Turney_NNP paper_NN ,_, many_JJ researchers_NNS have_VBP approached_VBN keyphrase_NN extraction_NN as_IN a_DT supervised_JJ machine_NN learning_NN problem_NN ._.
Given_VBN a_DT document_NN ,_, we_PRP construct_VBP an_DT example_NN for_IN each_DT unigram_NN ,_, bigram_NN ,_, and_CC trigram_NN found_VBN in_IN the_DT text_NN -LRB-_-LRB- though_IN other_JJ text_NN units_NNS are_VBP also_RB possible_JJ ,_, as_IN discussed_VBN below_IN -RRB-_-RRB- ._.
We_PRP then_RB compute_VBP various_JJ features_NNS describing_VBG each_DT example_NN -LRB-_-LRB- e.g._FW ,_, does_VBZ the_DT phrase_NN begin_VB with_IN an_DT upper-case_JJ letter_NN ?_. -RRB-_-RRB- ._.
We_PRP assume_VBP there_EX are_VBP known_JJ keyphrases_NNS available_JJ for_IN a_DT set_NN of_IN training_NN documents_NNS ._.
Using_VBG the_DT known_JJ keyphrases_NNS ,_, we_PRP can_MD assign_VB positive_JJ or_CC negative_JJ labels_NNS to_TO the_DT examples_NNS ._.
Then_RB we_PRP learn_VBP a_DT classifier_NN that_WDT can_MD discriminate_VB between_IN positive_JJ and_CC negative_JJ examples_NNS as_IN a_DT function_NN of_IN the_DT features_NNS ._.
Some_DT classifiers_NNS make_VBP a_DT binary_JJ classification_NN for_IN a_DT test_NN example_NN ,_, while_IN others_NNS assign_VBP a_DT probability_NN of_IN being_VBG a_DT keyphrase_NN ._.
For_IN instance_NN ,_, in_IN the_DT above_JJ text_NN ,_, we_PRP might_MD learn_VB a_DT rule_NN that_WDT says_VBZ phrases_NNS with_IN initial_JJ capital_NN letters_NNS are_VBP likely_JJ to_TO be_VB keyphrases_NNS ._.
After_IN training_VBG a_DT learner_NN ,_, we_PRP can_MD select_VB keyphrases_NNS for_IN test_NN documents_NNS in_IN the_DT following_JJ manner_NN ._.
We_PRP apply_VBP the_DT same_JJ example-generation_NN strategy_NN to_TO the_DT test_NN documents_NNS ,_, then_RB run_VB each_DT example_NN through_IN the_DT learner_NN ._.
We_PRP can_MD determine_VB the_DT keyphrases_NNS by_IN looking_VBG at_IN binary_JJ classification_NN decisions_NNS or_CC probabilities_NNS returned_VBN from_IN our_PRP$ learned_VBN model_NN ._.
If_IN probabilities_NNS are_VBP given_VBN ,_, a_DT threshold_NN is_VBZ used_VBN to_TO select_VB the_DT keyphrases_NNS ._.
Keyphrase_NN extractors_NNS are_VBP generally_RB evaluated_VBN using_VBG precision_NN and_CC recall_NN ._.
Precision_NN measures_VBZ how_WRB many_JJ of_IN the_DT proposed_VBN keyphrases_NNS are_VBP actually_RB correct_JJ ._.
Recall_VB measures_NNS how_WRB many_JJ of_IN the_DT true_JJ keyphrases_NNS your_PRP$ system_NN proposed_VBN ._.
The_DT two_CD measures_NNS can_MD be_VB combined_VBN in_IN an_DT F-score_NN ,_, which_WDT is_VBZ the_DT harmonic_JJ mean_NN of_IN the_DT two_CD -LRB-_-LRB- F_NN =_JJ 2PR_NN \/_: -LRB-_-LRB- P_NN +_CC R_NN -RRB-_-RRB- -RRB-_-RRB- ._.
Matches_NNS between_IN the_DT proposed_VBN keyphrases_NNS and_CC the_DT known_JJ keyphrases_NNS can_MD be_VB checked_VBN after_IN stemming_VBG or_CC applying_VBG some_DT other_JJ text_NN normalization_NN ._.
Design_NN choices_NNS Designing_NNP a_DT supervised_JJ keyphrase_NN extraction_NN system_NN involves_VBZ deciding_VBG on_IN several_JJ choices_NNS -LRB-_-LRB- some_DT of_IN these_DT apply_VBP to_TO unsupervised_JJ ,_, too_RB -RRB-_-RRB- :_: What_WP are_VBP the_DT examples_NNS ?_.
The_DT first_JJ choice_NN is_VBZ exactly_RB how_WRB to_TO generate_VB examples_NNS ._.
Turney_NN and_CC others_NNS have_VBP used_VBN all_DT possible_JJ unigrams_NNS ,_, bigrams_NNS ,_, and_CC trigrams_NNS without_IN intervening_VBG punctuation_NN and_CC after_IN removing_VBG stopwords_NNS ._.
Hulth_NNP showed_VBD that_IN you_PRP can_MD get_VB some_DT improvement_NN by_IN selecting_VBG examples_NNS to_TO be_VB sequences_NNS of_IN tokens_VBN that_WDT match_VBP certain_JJ patterns_NNS of_IN part-of-speech_JJ tags_NNS ._.
Ideally_RB ,_, the_DT mechanism_NN for_IN generating_VBG examples_NNS produces_VBZ all_PDT the_DT known_JJ labeled_JJ keyphrases_NNS as_IN candidates_NNS ,_, though_IN this_DT is_VBZ often_RB not_RB the_DT case_NN ._.
For_IN example_NN ,_, if_IN we_PRP use_VBP only_JJ unigrams_NNS ,_, bigrams_NNS ,_, and_CC trigrams_NNS ,_, then_RB we_PRP will_MD never_RB be_VB able_JJ to_TO extract_VB a_DT known_JJ keyphrase_NN containing_VBG four_CD words_NNS ._.
Thus_RB ,_, recall_NN may_MD suffer_VB ._.
However_RB ,_, generating_VBG too_RB many_JJ examples_NNS can_MD also_RB lead_VB to_TO low_JJ precision_NN ._.
What_WP are_VBP the_DT features_NNS ?_.
We_PRP also_RB need_VBP to_TO create_VB features_NNS that_WDT describe_VBP the_DT examples_NNS and_CC are_VBP informative_JJ enough_RB to_TO allow_VB a_DT learning_NN algorithm_NN to_TO discriminate_VB keyphrases_NNS from_IN non_JJ -_: keyphrases_NNS ._.
Typically_RB features_NNS involve_VBP various_JJ term_NN frequencies_NNS -LRB-_-LRB- how_WRB many_JJ times_NNS a_DT phrase_NN appears_VBZ in_IN the_DT current_JJ text_NN or_CC in_IN a_DT larger_JJR corpus_NN -RRB-_-RRB- ,_, the_DT length_NN of_IN the_DT example_NN ,_, relative_JJ position_NN of_IN the_DT first_JJ occurrence_NN ,_, various_JJ boolean_JJ syntactic_JJ features_NNS -LRB-_-LRB- e.g._FW ,_, contains_VBZ all_DT caps_NNS -RRB-_-RRB- ,_, etc._NN ._.
The_DT Turney_NNP paper_NN used_VBN about_IN 12_CD such_JJ features_NNS ._.
Hulth_NNP uses_VBZ a_DT reduced_VBN set_NN of_IN features_NNS ,_, which_WDT were_VBD found_VBN most_RBS successful_JJ in_IN the_DT KEA_NN -LRB-_-LRB- Keyphrase_NN Extraction_NN Algorithm_NN -RRB-_-RRB- work_NN derived_VBN from_IN Turney_NNP 's_POS seminal_JJ paper_NN ._.
How_WRB many_JJ keyphrases_NNS to_TO return_VB ?_.
In_IN the_DT end_NN ,_, the_DT system_NN will_MD need_VB to_TO return_VB a_DT list_NN of_IN keyphrases_NNS for_IN a_DT test_NN document_NN ,_, so_IN we_PRP need_VBP to_TO have_VB a_DT way_NN to_TO limit_VB the_DT number_NN ._.
Ensemble_NN methods_NNS -LRB-_-LRB- i.e._FW ,_, using_VBG votes_NNS from_IN several_JJ classifiers_NNS -RRB-_-RRB- have_VBP been_VBN used_VBN to_TO produce_VB numeric_JJ scores_NNS that_WDT can_MD be_VB thresholded_VBN to_TO provide_VB a_DT user-provided_JJ number_NN of_IN keyphrases_NNS ._.
This_DT is_VBZ the_DT technique_NN used_VBN by_IN Turney_NN with_IN C4_NN .5_CD decision_NN trees_NNS ._.
Hulth_NNP used_VBD a_DT single_JJ binary_JJ classifier_NN so_IN the_DT learning_NN algorithm_NN implicitly_RB determines_VBZ the_DT appropriate_JJ number_NN ._.
What_WDT learning_NN algorithm_NN ?_.
Once_RB examples_NNS and_CC features_NNS are_VBP created_VBN ,_, we_PRP need_VBP a_DT way_NN to_TO learn_VB to_TO predict_VB keyphrases_NNS ._.
Virtually_RB any_DT supervised_JJ learning_NN algorithm_NN could_MD be_VB used_VBN ,_, such_JJ as_IN decision_NN trees_NNS ,_, Naive_JJ Bayes_NNS ,_, and_CC rule_NN induction_NN ._.
In_IN the_DT case_NN of_IN Turney_NNP 's_POS GenEx_NN algorithm_NN ,_, a_DT genetic_JJ algorithm_NN is_VBZ used_VBN to_TO learn_VB parameters_NNS for_IN a_DT domain-specific_JJ keyphrase_NN extraction_NN algorithm_NN ._.
The_DT extractor_NN follows_VBZ a_DT series_NN of_IN heuristics_NNS to_TO identify_VB keyphrases_NNS ._.
The_DT genetic_JJ algorithm_NN optimizes_VBZ parameters_NNS for_IN these_DT heuristics_NNS with_IN respect_NN to_TO performance_NN on_IN training_NN documents_NNS with_IN known_JJ key_JJ phrases_NNS ._.
Unsupervised_JJ keyphrase_NN extraction_NN :_: TextRank_NN While_IN supervised_JJ methods_NNS have_VBP some_DT nice_JJ properties_NNS ,_, like_IN being_VBG able_JJ to_TO produce_VB interpretable_JJ rules_NNS for_IN what_WP features_NNS characterize_VBP a_DT keyphrase_NN ,_, they_PRP also_RB require_VBP a_DT large_JJ amount_NN of_IN training_NN data_NNS ._.
Many_JJ documents_NNS with_IN known_JJ keyphrases_NNS are_VBP needed_VBN ._.
Furthermore_RB ,_, training_NN on_IN a_DT specific_JJ domain_NN tends_VBZ to_TO customize_VB the_DT extraction_NN process_NN to_TO that_DT domain_NN ,_, so_IN the_DT resulting_VBG classifier_NN is_VBZ not_RB necessarily_RB portable_JJ ,_, as_IN some_DT of_IN Turney_NNP 's_POS results_NNS demonstrate_VBP ._.
Unsupervised_JJ keyphrase_NN extraction_NN removes_VBZ the_DT need_NN for_IN training_NN data_NNS ._.
It_PRP approaches_VBZ the_DT problem_NN from_IN a_DT different_JJ angle_NN ._.
Instead_RB of_IN trying_VBG to_TO learn_VB explicit_JJ features_NNS that_WDT characterize_VBP keyphrases_NNS ,_, the_DT TextRank_NNP algorithm_NN exploits_VBZ the_DT structure_NN of_IN the_DT text_NN itself_PRP to_TO determine_VB keyphrases_NNS that_WDT appear_VBP ``_`` central_JJ ''_'' to_TO the_DT text_NN in_IN the_DT same_JJ way_NN that_IN PageRank_NN selects_VBZ important_JJ Web_NN pages_NNS ._.
Recall_VB this_DT is_VBZ based_VBN on_IN the_DT notion_NN of_IN ``_`` prestige_NN ''_'' or_CC ``_`` recommendation_NN ''_'' from_IN social_JJ networks_NNS ._.
In_IN this_DT way_NN ,_, TextRank_NN does_VBZ not_RB rely_VB on_IN any_DT previous_JJ training_NN data_NNS at_IN all_DT ,_, but_CC rather_RB can_MD be_VB run_VBN on_IN any_DT arbitrary_JJ piece_NN of_IN text_NN ,_, and_CC it_PRP can_MD produce_VB output_NN simply_RB based_VBN on_IN the_DT text_NN 's_POS intrinsic_JJ properties_NNS ._.
Thus_RB the_DT algorithm_NN is_VBZ easily_RB portable_JJ to_TO new_JJ domains_NNS and_CC languages_NNS ._.
TextRank_NNP is_VBZ a_DT general_JJ purpose_NN graph-based_JJ ranking_NN algorithm_NN for_IN NLP_NNP ._.
Essentially_RB ,_, it_PRP runs_VBZ PageRank_NNP on_IN a_DT graph_NN specially_RB designed_VBN for_IN a_DT particular_JJ NLP_NN task_NN ._.
For_IN keyphrase_NN extraction_NN ,_, it_PRP builds_VBZ a_DT graph_NN using_VBG some_DT set_NN of_IN text_NN units_NNS as_IN vertices_NNS ._.
Edges_NNS are_VBP based_VBN on_IN some_DT measure_NN of_IN semantic_JJ or_CC lexical_JJ similarity_NN between_IN the_DT text_NN unit_NN vertices_NNS ._.
Unlike_IN PageRank_NNP ,_, the_DT edges_NNS are_VBP typically_RB undirected_JJ and_CC can_MD be_VB weighted_VBN to_TO reflect_VB a_DT degree_NN of_IN similarity_NN ._.
Once_RB the_DT graph_NN is_VBZ constructed_VBN ,_, it_PRP is_VBZ used_VBN to_TO form_VB a_DT stochastic_JJ matrix_NN ,_, combined_VBN with_IN a_DT damping_VBG factor_NN -LRB-_-LRB- as_IN in_IN the_DT ``_`` random_JJ surfer_NN model_NN ''_'' -RRB-_-RRB- ,_, and_CC the_DT ranking_NN over_IN vertices_NNS is_VBZ obtained_VBN by_IN finding_VBG the_DT eigenvector_NN corresponding_VBG to_TO eigenvalue_NN 1_CD -LRB-_-LRB- i.e._FW ,_, the_DT stationary_JJ distribution_NN of_IN the_DT random_JJ walk_NN on_IN the_DT graph_NN -RRB-_-RRB- ._.
Design_NN choices_NNS What_WP should_MD vertices_NNS be_VB ?_.
The_DT vertices_NNS should_MD correspond_VB to_TO what_WP we_PRP want_VBP to_TO rank_VB ._.
Potentially_RB ,_, we_PRP could_MD do_VB something_NN similar_JJ to_TO the_DT supervised_JJ methods_NNS and_CC create_VB a_DT vertex_NN for_IN each_DT unigram_NN ,_, bigram_NN ,_, trigram_NN ,_, etc._NN ._.
However_RB ,_, to_TO keep_VB the_DT graph_NN small_JJ ,_, the_DT authors_NNS decide_VBP to_TO rank_VB individual_JJ unigrams_NNS in_IN a_DT first_JJ step_NN ,_, and_CC then_RB include_VBP a_DT second_JJ step_NN that_WDT merges_VBZ highly_RB ranked_VBN adjacent_JJ unigrams_NNS to_TO form_VB multi-word_JJ phrases_NNS ._.
This_DT has_VBZ a_DT nice_JJ side_NN effect_NN of_IN allowing_VBG us_PRP to_TO produce_VB keyphrases_NNS of_IN arbitrary_JJ length_NN ._.
For_IN example_NN ,_, if_IN we_PRP rank_VBP unigrams_NNS and_CC find_VBP that_IN ``_`` advanced_JJ ''_'' ,_, ``_`` natural_JJ ''_'' ,_, ``_`` language_NN ''_'' ,_, and_CC ``_`` processing_NN ''_'' all_DT get_VBP high_JJ ranks_NNS ,_, then_RB we_PRP would_MD look_VB at_IN the_DT original_JJ text_NN and_CC see_VB that_IN these_DT words_NNS appear_VBP consecutively_RB and_CC create_VB a_DT final_JJ keyphrase_NN using_VBG all_DT four_CD together_RB ._.
Note_VB that_IN the_DT unigrams_NNS placed_VBN in_IN the_DT graph_NN can_MD be_VB filtered_VBN by_IN part_NN of_IN speech_NN ._.
The_DT authors_NNS found_VBD that_IN adjectives_NNS and_CC nouns_NNS were_VBD the_DT best_JJS to_TO include_VB ._.
Thus_RB ,_, some_DT linguistic_JJ knowledge_NN comes_VBZ into_IN play_NN in_IN this_DT step_NN ._.
How_WRB should_MD we_PRP create_VB edges_NNS ?_.
Edges_NNS are_VBP created_VBN based_VBN on_IN word_NN co-occurrence_NN in_IN this_DT application_NN of_IN TextRank_NNP ._.
Two_CD vertices_NNS are_VBP connected_VBN by_IN an_DT edge_NN if_IN the_DT unigrams_NNS appear_VBP within_IN a_DT window_NN of_IN size_NN N_NN in_IN the_DT original_JJ text_NN ._.
N_NN is_VBZ typically_RB around_IN 2_CD --_: 10_CD ._.
Thus_RB ,_, ``_`` natural_JJ ''_'' and_CC ``_`` language_NN ''_'' might_MD be_VB linked_VBN in_IN a_DT text_NN about_IN NLP_NNP ._.
``_`` Natural_NNP ''_'' and_CC ``_`` processing_NN ''_'' would_MD also_RB be_VB linked_VBN because_IN they_PRP would_MD both_DT appear_VB in_IN the_DT same_JJ string_NN of_IN N_NN words_NNS ._.
These_DT edges_NNS build_VBP on_IN the_DT notion_NN of_IN ``_`` text_NN cohesion_NN ''_'' and_CC the_DT idea_NN that_IN words_NNS that_WDT appear_VBP near_IN each_DT other_JJ are_VBP likely_RB related_VBN in_IN a_DT meaningful_JJ way_NN and_CC ``_`` recommend_VB ''_'' each_DT other_JJ to_TO the_DT reader_NN ._.
How_WRB are_VBP the_DT final_JJ keyphrases_NNS formed_VBN ?_.
Since_IN this_DT method_NN simply_RB ranks_VBZ the_DT individual_JJ vertices_NNS ,_, we_PRP need_VBP a_DT way_NN to_TO threshold_NN or_CC produce_VB a_DT limited_JJ number_NN of_IN keyphrases_NNS ._.
The_DT technique_NN chosen_VBN is_VBZ to_TO set_VB a_DT count_NN T_NN to_TO be_VB a_DT user-specified_JJ fraction_NN of_IN the_DT total_JJ number_NN of_IN vertices_NNS in_IN the_DT graph_NN ._.
Then_RB the_DT top_JJ T_NN vertices\/unigrams_NNS are_VBP selected_VBN based_VBN on_IN their_PRP$ stationary_JJ probabilities_NNS ._.
A_DT post_NN -_: processing_NN step_NN is_VBZ then_RB applied_VBN to_TO merge_VB adjacent_JJ instances_NNS of_IN these_DT T_NN unigrams_NNS ._.
As_IN a_DT result_NN ,_, potentially_RB more_JJR or_CC less_JJR than_IN T_NN final_JJ keyphrases_NNS will_MD be_VB produced_VBN ,_, but_CC the_DT number_NN should_MD be_VB roughly_RB proportional_JJ to_TO the_DT length_NN of_IN the_DT original_JJ text_NN ._.
Why_WRB it_PRP works_VBZ It_PRP is_VBZ not_RB initially_RB clear_JJ why_WRB applying_VBG PageRank_NN to_TO a_DT co-occurrence_NN graph_NN would_MD produce_VB useful_JJ keyphrases_NNS ._.
One_CD way_NN to_TO think_VB about_IN it_PRP is_VBZ the_DT following_NN ._.
A_DT word_NN that_WDT appears_VBZ multiple_JJ times_NNS throughout_IN a_DT text_NN may_MD have_VB many_JJ different_JJ co-occurring_JJ neighbors_NNS ._.
For_IN example_NN ,_, in_IN a_DT text_NN about_IN machine_NN learning_NN ,_, the_DT unigram_JJ ``_`` learning_NN ''_'' might_MD co-occur_VB with_IN ``_`` machine_NN ''_'' ,_, supervised_VBN ''_'' ,_, ``_`` un-supervised_JJ ''_'' ,_, and_CC ``_`` semi-supervised_JJ ''_'' in_IN four_CD different_JJ sentences_NNS ._.
Thus_RB ,_, the_DT ``_`` learning_NN ''_'' vertex_NN would_MD be_VB a_DT central_JJ ``_`` hub_NN ''_'' that_WDT connects_VBZ to_TO these_DT other_JJ modifying_VBG words_NNS ._.
Running_VBG PageRank\/TextRank_NN on_IN the_DT graph_NN is_VBZ likely_JJ to_TO rank_VB ``_`` learning_VBG ''_'' highly_RB ._.
Similarly_RB ,_, if_IN the_DT text_NN contains_VBZ the_DT phrase_NN ``_`` supervised_JJ classification_NN ''_'' ,_, then_RB there_EX would_MD be_VB an_DT edge_NN between_IN ``_`` supervised_JJ ''_'' and_CC ``_`` classification_NN ''_'' ._.
If_IN ``_`` classification_NN ''_'' appears_VBZ several_JJ other_JJ places_NNS and_CC thus_RB has_VBZ many_JJ neighbors_NNS ,_, it_PRP is_VBZ importance_NN would_MD contribute_VB to_TO the_DT importance_NN of_IN ``_`` supervised_VBN ''_'' ._.
If_IN it_PRP ends_VBZ up_RP with_IN a_DT high_JJ rank_NN ,_, it_PRP will_MD be_VB selected_VBN as_IN one_CD of_IN the_DT top_JJ T_NN unigrams_NNS ,_, along_IN with_IN ``_`` learning_VBG ''_'' and_CC probably_RB ``_`` classification_NN ''_'' ._.
In_IN the_DT final_JJ post-processing_JJ step_NN ,_, we_PRP would_MD then_RB end_VB up_RP with_IN keyphrases_NNS ``_`` supervised_JJ learning_NN ''_'' and_CC ``_`` supervised_JJ classification_NN ''_'' ._.
In_IN short_JJ ,_, the_DT co-occurrence_NN graph_NN will_MD contain_VB densely_RB connected_VBN regions_NNS for_IN terms_NNS that_WDT appear_VBP often_RB and_CC in_IN different_JJ contexts_NNS ._.
A_DT random_JJ walk_NN on_IN this_DT graph_NN will_MD have_VB a_DT stationary_JJ distribution_NN that_WDT assigns_VBZ large_JJ probabilities_NNS to_TO the_DT terms_NNS in_IN the_DT centers_NNS of_IN the_DT clusters_NNS ._.
This_DT is_VBZ similar_JJ to_TO densely_RB connected_VBN Web_NN pages_NNS getting_VBG ranked_VBN highly_RB by_IN PageRank_NN ._.
Document_NNP summarization_NN Like_IN keyphrase_NN extraction_NN ,_, document_NN summarization_NN hopes_VBZ to_TO identify_VB the_DT essence_NN of_IN a_DT text_NN ._.
The_DT only_JJ real_JJ difference_NN is_VBZ that_IN now_RB we_PRP are_VBP dealing_VBG with_IN larger_JJR text_NN units_NNS --_: whole_JJ sentences_NNS instead_RB of_IN words_NNS and_CC phrases_NNS ._.
While_IN some_DT work_NN has_VBZ been_VBN done_VBN in_IN abstractive_JJ summarization_NN -LRB-_-LRB- creating_VBG an_DT abstract_JJ synopsis_NN like_IN that_DT of_IN a_DT human_JJ -RRB-_-RRB- ,_, the_DT majority_NN of_IN summarization_NN systems_NNS are_VBP extractive_JJ -LRB-_-LRB- selecting_VBG a_DT subset_NN of_IN sentences_NNS to_TO place_VB in_IN a_DT summary_NN -RRB-_-RRB- ._.
Before_IN getting_VBG into_IN the_DT details_NNS of_IN some_DT summarization_NN methods_NNS ,_, we_PRP will_MD mention_VB how_WRB summarization_NN systems_NNS are_VBP typically_RB evaluated_VBN ._.
The_DT most_RBS common_JJ way_NN is_VBZ using_VBG the_DT so-called_JJ ROUGE_NN -LRB-_-LRB- Recall-Oriented_JJ Understudy_NN for_IN Gisting_JJ Evaluation_NN -RRB-_-RRB- measure_NN -LRB-_-LRB- http:\/\/haydn.isi.edu\/ROUGE\/_NN -RRB-_-RRB- ._.
This_DT is_VBZ a_DT recall-based_JJ measure_NN that_WDT determines_VBZ how_WRB well_RB a_DT system-generated_JJ summary_NN covers_VBZ the_DT content_NN present_JJ in_IN one_CD or_CC more_JJR human-generated_JJ model_NN summaries_NNS known_VBN as_IN references_NNS ._.
It_PRP is_VBZ recall-based_JJ to_TO encourage_VB systems_NNS to_TO include_VB all_PDT the_DT important_JJ topics_NNS in_IN the_DT text_NN ._.
Recall_VB can_MD be_VB computed_VBN with_IN respect_NN to_TO unigram_NN ,_, bigram_NN ,_, trigram_NN ,_, or_CC 4-gram_JJ matching_NN ,_, though_IN ROUGE-1_NN -LRB-_-LRB- unigram_JJ matching_NN -RRB-_-RRB- has_VBZ been_VBN shown_VBN to_TO correlate_VB best_RB with_IN human_JJ assessments_NNS of_IN system-generated_JJ summaries_NNS -LRB-_-LRB- i.e._FW ,_, the_DT summaries_NNS with_IN highest_JJS ROUGE-1_NN values_NNS correlate_VBP with_IN the_DT summaries_NNS humans_NNS deemed_VBD the_DT best_JJS -RRB-_-RRB- ._.
ROUGE-1_NN is_VBZ computed_VBN as_IN division_NN of_IN count_NN of_IN unigrams_NNS in_IN reference_NN that_WDT appear_VBP in_IN system_NN and_CC count_NN of_IN unigrams_NNS in_IN reference_NN summary_NN ._.
If_IN there_EX are_VBP multiple_JJ references_NNS ,_, the_DT ROUGE-1_NN scores_NNS are_VBP averaged_VBN ._.
Because_IN ROUGE_NNP is_VBZ based_VBN only_RB on_IN content_NN overlap_VBP ,_, it_PRP can_MD determine_VB if_IN the_DT same_JJ general_JJ concepts_NNS are_VBP discussed_VBN between_IN an_DT automatic_JJ summary_NN and_CC a_DT reference_NN summary_NN ,_, but_CC it_PRP can_MD not_RB determine_VB if_IN the_DT result_NN is_VBZ coherent_JJ or_CC the_DT sentences_NNS flow_VBP together_RB in_IN a_DT sensible_JJ manner_NN ._.
High-order_FW n-gram_FW ROUGE_NN measures_NNS try_VBP to_TO judge_VB fluency_NN to_TO some_DT degree_NN ._.
Note_VB that_IN ROUGE_NNP is_VBZ similar_JJ to_TO the_DT BLEU_NNP measure_NN for_IN machine_NN translation_NN ,_, but_CC BLEU_NNP is_VBZ precision_NN -_: based_VBN ,_, because_IN translation_NN systems_NNS favor_VBP accuracy_NN ._.
A_DT promising_JJ line_NN in_IN document_NN summarization_NN is_VBZ adaptive_JJ document\/text_NN summarization_NN ._.
The_DT idea_NN of_IN adaptive_JJ summarization_NN involves_VBZ preliminary_JJ recognition_NN of_IN document\/text_FW genre_FW and_CC subsequent_JJ application_NN of_IN summarization_NN algorithms_NNS optimized_VBN for_IN this_DT genre_NN ._.
First_NNP summarizes_VBZ that_WDT perform_VBP adaptive_JJ summarization_NN have_VBP been_VBN created_VBN ._.
Overview_NN of_IN supervised_JJ learning_NN approaches_NNS Supervised_VBD text_NN summarization_NN is_VBZ very_RB much_RB like_IN supervised_JJ keyphrase_NN extraction_NN ,_, and_CC we_PRP will_MD not_RB spend_VB much_JJ time_NN on_IN it_PRP ._.
Basically_RB ,_, if_IN you_PRP have_VBP a_DT collection_NN of_IN documents_NNS and_CC human-generated_JJ summaries_NNS for_IN them_PRP ,_, you_PRP can_MD learn_VB features_NNS of_IN sentences_NNS that_WDT make_VBP them_PRP good_JJ candidates_NNS for_IN inclusion_NN in_IN the_DT summary_NN ._.
Features_NNS might_MD include_VB the_DT position_NN in_IN the_DT document_NN -LRB-_-LRB- i.e._FW ,_, the_DT first_JJ few_JJ sentences_NNS are_VBP probably_RB important_JJ -RRB-_-RRB- ,_, the_DT number_NN of_IN words_NNS in_IN the_DT sentence_NN ,_, etc._NN ._.
The_DT main_JJ difficulty_NN in_IN supervised_JJ extractive_JJ summarization_NN is_VBZ that_IN the_DT known_JJ summaries_NNS must_MD be_VB manually_RB created_VBN by_IN extracting_VBG sentences_NNS so_IN the_DT sentences_NNS in_IN an_DT original_JJ training_NN document_NN can_MD be_VB labeled_VBN as_IN ``_`` in_IN summary_NN ''_'' or_CC ``_`` not_RB in_IN summary_NN ''_'' ._.
This_DT is_VBZ not_RB typically_RB how_WRB people_NNS create_VBP summaries_NNS ,_, so_RB simply_RB using_VBG journal_NN abstracts_NNS or_CC existing_VBG summaries_NNS is_VBZ usually_RB not_RB sufficient_JJ ._.
The_DT sentences_NNS in_IN these_DT summaries_NNS do_VBP not_RB necessarily_RB match_VB up_RP with_IN sentences_NNS in_IN the_DT original_JJ text_NN ,_, so_IN it_PRP would_MD difficult_JJ to_TO assign_VB labels_NNS to_TO examples_NNS for_IN training_NN ._.
Note_VB ,_, however_RB ,_, that_IN these_DT natural_JJ summaries_NNS can_MD still_RB be_VB used_VBN for_IN evaluation_NN purposes_NNS ,_, since_IN ROUGE-1_NN only_RB cares_VBZ about_IN unigrams_NNS ._.
Unsupervised_JJ approaches_NNS :_: TextRank_NNP and_CC LexRank_NNP The_DT unsupervised_JJ approach_NN to_TO summarization_NN is_VBZ also_RB quite_RB similar_JJ in_IN spirit_NN to_TO unsupervised_JJ keyphrase_NN extraction_NN and_CC gets_VBZ around_IN the_DT issue_NN of_IN costly_JJ training_NN data_NNS ._.
Some_DT unsupervised_JJ summarization_NN approaches_NNS are_VBP based_VBN on_IN finding_VBG a_DT ``_`` centroid_JJ ''_'' sentence_NN ,_, which_WDT is_VBZ the_DT mean_JJ word_NN vector_NN of_IN all_PDT the_DT sentences_NNS in_IN the_DT document_NN ._.
Then_RB the_DT sentences_NNS can_MD be_VB ranked_VBN with_IN regard_NN to_TO their_PRP$ similarity_NN to_TO this_DT centroid_JJ sentence_NN ._.
A_DT more_RBR principled_JJ way_NN to_TO estimate_VB sentence_NN importance_NN is_VBZ using_VBG random_JJ walks_VBZ and_CC eigenvector_NN centrality_NN ._.
LexRank_NNP is_VBZ an_DT algorithm_NN essentially_RB identical_JJ to_TO TextRank_NNP ,_, and_CC both_DT use_VBP this_DT approach_NN for_IN document_NN summarization_NN ._.
The_DT two_CD methods_NNS were_VBD developed_VBN by_IN different_JJ groups_NNS at_IN the_DT same_JJ time_NN ,_, and_CC LexRank_NNP simply_RB focused_VBD on_IN summarization_NN ,_, but_CC could_MD just_RB as_RB easily_RB be_VB used_VBN for_IN keyphrase_NN extraction_NN or_CC any_DT other_JJ NLP_NN ranking_NN task_NN ._.
Design_NN choices_NNS What_WP are_VBP the_DT vertices_NNS ?_.
In_IN both_DT LexRank_NNP and_CC TextRank_NNP ,_, a_DT graph_NN is_VBZ constructed_VBN by_IN creating_VBG a_DT vertex_NN for_IN each_DT sentence_NN in_IN the_DT document_NN ._.
What_WP are_VBP the_DT edges_NNS ?_.
The_DT edges_NNS between_IN sentences_NNS are_VBP based_VBN on_IN some_DT form_NN of_IN semantic_JJ similarity_NN or_CC content_NN overlap_VBP ._.
While_IN LexRank_NN uses_VBZ cosine_NN similarity_NN of_IN TF-IDF_NN vectors_NNS ,_, TextRank_NNP uses_VBZ a_DT very_RB similar_JJ measure_NN based_VBN on_IN the_DT number_NN of_IN words_NNS two_CD sentences_NNS have_VBP in_IN common_JJ -LRB-_-LRB- normalized_VBN by_IN the_DT sentences_NNS '_POS lengths_NNS -RRB-_-RRB- ._.
The_DT LexRank_NNP paper_NN explored_VBD using_VBG unweighted_JJ edges_NNS after_IN applying_VBG a_DT threshold_NN to_TO the_DT cosine_NN values_NNS ,_, but_CC also_RB experimented_VBD with_IN using_VBG edges_NNS with_IN weights_NNS equal_JJ to_TO the_DT similarity_NN score_NN ._.
TextRank_NNP uses_VBZ continuous_JJ similarity_NN scores_NNS as_IN weights_NNS ._.
How_WRB are_VBP summaries_NNS formed_VBN ?_.
In_IN both_DT algorithms_NNS ,_, the_DT sentences_NNS are_VBP ranked_VBN by_IN applying_VBG PageRank_NN to_TO the_DT resulting_VBG graph_NN ._.
A_DT summary_NN is_VBZ formed_VBN by_IN combining_VBG the_DT top_JJ ranking_NN sentences_NNS ,_, using_VBG a_DT threshold_NN or_CC length_NN cutoff_NN to_TO limit_VB the_DT size_NN of_IN the_DT summary_NN ._.
TextRank_NNP and_CC LexRank_NNP differences_NNS It_PRP is_VBZ worth_JJ noting_VBG that_IN TextRank_NNP was_VBD applied_VBN to_TO summarization_NN exactly_RB as_IN described_VBN here_RB ,_, while_IN LexRank_NNP was_VBD used_VBN as_IN part_NN of_IN a_DT larger_JJR summarization_NN system_NN -LRB-_-LRB- MEAD_NN -RRB-_-RRB- that_WDT combines_VBZ the_DT LexRank_NNP score_NN -LRB-_-LRB- stationary_JJ probability_NN -RRB-_-RRB- with_IN other_JJ features_NNS like_IN sentence_NN position_NN and_CC length_NN using_VBG a_DT linear_JJ combination_NN with_IN either_CC user-specified_JJ or_CC automatically_RB tuned_VBN weights_NNS ._.
In_IN this_DT case_NN ,_, some_DT training_NN documents_NNS might_MD be_VB needed_VBN ,_, though_IN the_DT TextRank_NNP results_NNS show_VBP the_DT additional_JJ features_NNS are_VBP not_RB absolutely_RB necessary_JJ ._.
Another_DT important_JJ distinction_NN is_VBZ that_IN TextRank_NNP was_VBD used_VBN for_IN single_JJ document_NN summarization_NN ,_, while_IN LexRank_NNP has_VBZ been_VBN applied_VBN to_TO multi-document_JJ summarization_NN ._.
The_DT task_NN remains_VBZ the_DT same_JJ in_IN both_DT cases_NNS --_: only_RB the_DT number_NN of_IN sentences_NNS to_TO choose_VB from_IN has_VBZ grown_VBN ._.
However_RB ,_, when_WRB summarizing_VBG multiple_JJ documents_NNS ,_, there_EX is_VBZ a_DT greater_JJR risk_NN of_IN selecting_NN duplicate_VBP or_CC highly_RB redundant_JJ sentences_NNS to_TO place_VB in_IN the_DT same_JJ summary_NN ._.
Imagine_VB you_PRP have_VBP a_DT cluster_NN of_IN news_NN articles_NNS on_IN a_DT particular_JJ event_NN ,_, and_CC you_PRP want_VBP to_TO produce_VB one_CD summary_NN ._.
Each_DT article_NN is_VBZ likely_JJ to_TO have_VB many_JJ similar_JJ sentences_NNS ,_, and_CC you_PRP would_MD only_RB want_VB to_TO include_VB distinct_JJ ideas_NNS in_IN the_DT summary_NN ._.
To_TO address_VB this_DT issue_NN ,_, LexRank_NNP applies_VBZ a_DT heuristic_NN post-processing_JJ step_NN that_WDT builds_VBZ up_RP a_DT summary_NN by_IN adding_VBG sentences_NNS in_IN rank_NN order_NN ,_, but_CC discards_VBZ any_DT sentences_NNS that_WDT are_VBP too_RB similar_JJ to_TO ones_NNS already_RB placed_VBN in_IN the_DT summary_NN ._.
The_DT method_NN used_VBN is_VBZ called_VBN Cross-Sentence_NNP Information_NNP Subsumption_NNP -LRB-_-LRB- CSIS_NNP -RRB-_-RRB- ._.
Why_WRB unsupervised_JJ summarization_NN works_VBZ These_DT methods_NNS work_VBP based_VBN on_IN the_DT idea_NN that_IN sentences_NNS ``_`` recommend_VB ''_'' other_JJ similar_JJ sentences_NNS to_TO the_DT reader_NN ._.
Thus_RB ,_, if_IN one_CD sentence_NN is_VBZ very_RB similar_JJ to_TO many_JJ others_NNS ,_, it_PRP will_MD likely_RB be_VB a_DT sentence_NN of_IN great_JJ importance_NN ._.
The_DT importance_NN of_IN this_DT sentence_NN also_RB stems_VBZ from_IN the_DT importance_NN of_IN the_DT sentences_NNS ``_`` recommending_VBG ''_'' it_PRP ._.
Thus_RB ,_, to_TO get_VB ranked_VBN highly_RB and_CC placed_VBN in_IN a_DT summary_NN ,_, a_DT sentence_NN must_MD be_VB similar_JJ to_TO many_JJ sentences_NNS that_WDT are_VBP in_IN turn_NN also_RB similar_JJ to_TO many_JJ other_JJ sentences_NNS ._.
This_DT makes_VBZ intuitive_JJ sense_NN and_CC allows_VBZ the_DT algorithms_NNS to_TO be_VB applied_VBN to_TO any_DT arbitrary_JJ new_JJ text_NN ._.
The_DT methods_NNS are_VBP domain-independent_JJ and_CC easily_RB portable_JJ ._.
One_CD could_MD imagine_VB the_DT features_NNS indicating_VBG important_JJ sentences_NNS in_IN the_DT news_NN domain_NN might_MD vary_VB considerably_RB from_IN the_DT biomedical_JJ domain_NN ._.
However_RB ,_, the_DT unsupervised_JJ ``_`` recommendation_NN ''_'' -_: based_VBN approach_NN applies_VBZ to_TO any_DT domain_NN ._.
Incorporating_VBG diversity_NN :_: GRASSHOPPER_NN algorithm_NN As_IN mentioned_VBN above_JJ ,_, multi-document_JJ extractive_JJ summarization_NN faces_VBZ a_DT problem_NN of_IN potential_JJ redundancy_NN ._.
Ideally_RB ,_, we_PRP would_MD like_VB to_TO extract_VB sentences_NNS that_WDT are_VBP both_DT ``_`` central_JJ ''_'' -LRB-_-LRB- i.e._FW ,_, contain_VBP the_DT main_JJ ideas_NNS -RRB-_-RRB- and_CC ``_`` diverse_JJ ''_'' -LRB-_-LRB- i.e._FW ,_, they_PRP differ_VBP from_IN one_CD another_DT -RRB-_-RRB- ._.
LexRank_NN deals_NNS with_IN diversity_NN as_IN a_DT heuristic_NN final_JJ stage_NN using_VBG CSIS_NN ,_, and_CC other_JJ systems_NNS have_VBP used_VBN similar_JJ methods_NNS ,_, such_JJ as_IN Maximal_JJ Marginal_JJ Relevance_NN -LRB-_-LRB- MMR_NN -RRB-_-RRB- ,_, in_IN trying_VBG to_TO eliminate_VB redundancy_NN in_IN information_NN retrieval_NN results_NNS ._.
We_PRP have_VBP developed_VBN a_DT general_JJ purpose_NN graph-based_JJ ranking_NN algorithm_NN like_IN Page\/Lex\/TextRank_NN that_WDT handles_VBZ both_DT ``_`` centrality_NN ''_'' and_CC ``_`` diversity_NN ''_'' in_IN a_DT unified_JJ mathematical_JJ framework_NN based_VBN on_IN absorbing_VBG Markov_NNP chain_NN random_JJ walks_VBZ ._.
-LRB-_-LRB- An_DT absorbing_VBG random_JJ walk_NN is_VBZ like_IN a_DT standard_JJ random_JJ walk_NN ,_, except_IN some_DT states_NNS are_VBP now_RB absorbing_VBG states_NNS that_WDT act_VBP as_IN ``_`` black_JJ holes_NNS ''_'' that_WDT cause_VBP the_DT walk_NN to_TO end_VB abruptly_RB at_IN that_DT state_NN ._. -RRB-_-RRB-
The_DT algorithm_NN is_VBZ called_VBN GRASSHOPPER_NNP for_IN reasons_NNS that_WDT should_MD soon_RB become_VB clear_JJ ._.
In_IN addition_NN to_TO explicitly_RB promoting_VBG diversity_NN during_IN the_DT ranking_JJ process_NN ,_, GRASSHOPPER_NN incorporates_VBZ a_DT prior_JJ ranking_NN -LRB-_-LRB- based_VBN on_IN sentence_NN position_NN in_IN the_DT case_NN of_IN summarization_NN -RRB-_-RRB- ._.
Maximum_NNP entropy-based_JJ summarization_NN It_PRP is_VBZ an_DT abstractive_JJ method_NN ._.
Even_RB though_IN automating_VBG abstractive_JJ summarization_NN is_VBZ the_DT goal_NN of_IN summarization_NN research_NN ,_, most_RBS practical_JJ systems_NNS are_VBP based_VBN on_IN some_DT form_NN of_IN extractive_JJ summarization_NN ._.
Extracted_VBN sentences_NNS can_MD form_VB a_DT valid_JJ summary_NN in_IN itself_PRP or_CC form_VB a_DT basis_NN for_IN further_JJ condensation_NN operations_NNS ._.
Furthermore_RB ,_, evaluation_NN of_IN extracted_VBN summaries_NNS can_MD be_VB automated_VBN ,_, since_IN it_PRP is_VBZ essentially_RB a_DT classification_NN task_NN ._.
During_IN the_DT DUC_NN 2001_CD and_CC 2002_CD evaluation_NN workshops_NNS ,_, TNO_NN developed_VBD a_DT sentence_NN extraction_NN system_NN for_IN multi-document_JJ summarization_NN in_IN the_DT news_NN domain_NN ._.
The_DT system_NN was_VBD based_VBN on_IN a_DT hybrid_NN system_NN using_VBG a_DT naive_JJ Bayes_NNP classifier_NN and_CC statistical_JJ language_NN models_NNS for_IN modeling_NN salience_NN ._.
Although_IN the_DT system_NN exhibited_VBD good_JJ results_NNS ,_, we_PRP wanted_VBD to_TO explore_VB the_DT effectiveness_NN of_IN a_DT maximum_NN entropy_NN -LRB-_-LRB- ME_NN -RRB-_-RRB- classifier_NN for_IN the_DT meeting_NN summarization_NN task_NN ,_, as_IN ME_NN is_VBZ known_VBN to_TO be_VB robust_JJ against_IN feature_NN dependencies_NNS ._.
Maximum_NNP entropy_NN has_VBZ also_RB been_VBN applied_VBN successfully_RB for_IN summarization_NN in_IN the_DT broadcast_NN news_NN domain_NN ._.
Aided_VBN summarization_NN Machine_NNP learning_VBG techniques_NNS from_IN closely_RB related_JJ fields_NNS such_JJ as_IN information_NN retrieval_NN or_CC text_NN mining_NN have_VBP been_VBN successfully_RB adapted_VBN to_TO help_VB automatic_JJ summarization_NN ._.
Apart_RB from_IN Fully_RB Automated_NNP Summarizers_NNP -LRB-_-LRB- FAS_NNP -RRB-_-RRB- ,_, there_EX are_VBP systems_NNS that_IN aid_NN users_NNS with_IN the_DT task_NN of_IN summarization_NN -LRB-_-LRB- MAHS_NN =_JJ Machine_NN Aided_VBN Human_NNP Summarization_NNP -RRB-_-RRB- ,_, for_IN example_NN by_IN highlighting_VBG candidate_NN passages_NNS to_TO be_VB included_VBN in_IN the_DT summary_NN ,_, and_CC there_EX are_VBP systems_NNS that_WDT depend_VBP on_IN post-processing_JJ by_IN a_DT human_JJ -LRB-_-LRB- HAMS_NN =_JJ Human_JJ Aided_VBN Machine_NN Summarization_NN -RRB-_-RRB- ._.
Evaluation_NN An_DT ongoing_JJ issue_NN in_IN this_DT field_NN is_VBZ that_DT of_IN evaluation_NN ._.
Evaluation_NN techniques_NNS fall_VBP into_IN intrinsic_JJ and_CC extrinsic_JJ ,_, inter-texual_JJ and_CC intra-texual_JJ ._.
An_DT intrinsic_JJ evaluation_NN tests_VBZ the_DT summarization_NN system_NN in_IN of_IN itself_PRP while_IN an_DT extrinsic_JJ evaluation_NN tests_VBZ the_DT summarization_NN based_VBN on_IN how_WRB it_PRP affects_VBZ the_DT completion_NN of_IN some_DT other_JJ task_NN ._.
Intrinsic_JJ evaluations_NNS have_VBP assessed_VBN mainly_RB the_DT coherence_NN and_CC informativeness_NN of_IN summaries_NNS ._.
Extrinsic_JJ evaluations_NNS ,_, on_IN the_DT other_JJ hand_NN ,_, have_VBP tested_VBN the_DT impact_NN of_IN summarization_NN on_IN tasks_NNS like_IN relevance_NN assessment_NN ,_, reading_NN comprehension_NN ,_, etc._NN ._.
Intra-texual_JJ methods_NNS assess_VBP the_DT output_NN of_IN a_DT specific_JJ summarization_NN system_NN ,_, and_CC the_DT inter-texual_JJ ones_NNS focus_VBP on_IN contrastive_JJ analysis_NN of_IN outputs_NNS of_IN several_JJ summarization_NN systems_NNS ._.
Human_JJ judgement_NN often_RB has_VBZ wide_JJ variance_NN on_IN what_WP is_VBZ considered_VBN a_DT ``_`` good_JJ ''_'' summary_NN ,_, which_WDT means_VBZ that_IN making_VBG the_DT evaluation_NN process_NN automatic_JJ is_VBZ particularly_RB difficult_JJ ._.
Manual_JJ evaluation_NN can_MD be_VB used_VBN ,_, but_CC this_DT is_VBZ both_CC time_NN and_CC labor_NN intensive_JJ as_IN it_PRP requires_VBZ humans_NNS to_TO read_VB not_RB only_RB the_DT summaries_NNS but_CC also_RB the_DT source_NN documents_NNS ._.
Other_JJ issues_NNS are_VBP those_DT concerning_VBG coherence_NN and_CC coverage_NN ._.
One_CD of_IN the_DT metrics_NNS used_VBN in_IN NIST_NNP 's_POS annual_JJ Document_NNP Understanding_VBG Conferences_NNS ,_, in_IN which_WDT research_NN groups_NNS submit_VBP their_PRP$ systems_NNS for_IN both_CC summarization_NN and_CC translation_NN tasks_NNS ,_, is_VBZ the_DT ROUGE_NN metric_JJ -LRB-_-LRB- Recall-Oriented_JJ Understudy_NN for_IN Gisting_JJ Evaluation_NN -RRB-_-RRB- ._.
It_PRP essentially_RB calculates_VBZ n-gram_NN overlaps_VBZ between_IN automatically_RB generated_VBN summaries_NNS and_CC previously-written_JJ human_JJ summaries_NNS ._.
A_DT high_JJ level_NN of_IN overlap_VB should_MD indicate_VB a_DT high_JJ level_NN of_IN shared_JJ concepts_NNS between_IN the_DT two_CD summaries_NNS ._.
Note_NN that_WDT overlap_VBP metrics_NNS like_IN this_DT are_VBP unable_JJ to_TO provide_VB any_DT feedback_NN on_IN a_DT summary_NN 's_POS coherence_NN ._.
Anaphor_NNP resolution_NN remains_VBZ another_DT problem_NN yet_RB to_TO be_VB fully_RB solved_VBN ._.
Evaluating_VBG summaries_NNS ,_, either_CC manually_RB or_CC automatically_RB ,_, is_VBZ a_DT hard_JJ task_NN ._.
The_DT main_JJ difficulty_NN in_IN evaluation_NN comes_VBZ from_IN the_DT impossibility_NN of_IN building_VBG a_DT fair_JJ gold-standard_NN against_IN which_WDT the_DT results_NNS of_IN the_DT systems_NNS can_MD be_VB compared_VBN ._.
Furthermore_RB ,_, it_PRP is_VBZ also_RB very_RB hard_JJ to_TO determine_VB what_WP a_DT correct_JJ summary_NN is_VBZ ,_, because_IN there_EX is_VBZ always_RB the_DT possibility_NN of_IN a_DT system_NN to_TO generate_VB a_DT good_JJ summary_NN that_WDT is_VBZ quite_RB different_JJ from_IN any_DT human_JJ summary_NN used_VBN as_IN an_DT approximation_NN to_TO the_DT correct_JJ output_NN ._.
Current_JJ difficulties_NNS in_IN evaluating_VBG summaries_NNS automatically_RB The_DT most_RBS common_JJ way_NN to_TO evaluate_VB the_DT informativeness_NN of_IN automatic_JJ summaries_NNS is_VBZ to_TO compare_VB them_PRP with_IN human-made_JJ model_NN summaries_NNS ._.
However_RB ,_, as_IN content_JJ selection_NN is_VBZ not_RB a_DT deterministic_JJ problem_NN ,_, different_JJ people_NNS would_MD choose_VB different_JJ sentences_NNS ,_, and_CC even_RB ,_, the_DT same_JJ person_NN may_MD chose_VBD different_JJ sentences_NNS at_IN different_JJ times_NNS ,_, showing_VBG evidence_NN of_IN low_JJ agreement_NN among_IN humans_NNS as_IN to_TO which_WDT sentences_NNS are_VBP good_JJ summary_NN sentences_NNS ._.
Besides_IN the_DT human_JJ variability_NN ,_, the_DT semantic_JJ equivalence_JJ is_VBZ another_DT problem_NN ,_, because_IN two_CD distinct_JJ sentences_NNS can_MD express_VB the_DT same_JJ meaning_NN but_CC not_RB using_VBG the_DT same_JJ words_NNS ._.
This_DT phenomenon_NN is_VBZ known_VBN as_IN paraphrase_NN ._.
We_PRP can_MD find_VB an_DT approach_NN to_TO automatically_RB evaluating_VBG summaries_NNS using_VBG paraphrases_NNS -LRB-_-LRB- ParaEval_NN -RRB-_-RRB- ._.
Moreover_RB ,_, most_JJS summarization_NN systems_NNS perform_VBP an_DT extractive_JJ approach_NN ,_, selecting_VBG and_CC copying_VBG important_JJ sentences_NNS from_IN the_DT source_NN documents_NNS ._.
Although_IN humans_NNS can_MD also_RB cut_VB and_CC paste_VB relevant_JJ information_NN of_IN a_DT text_NN ,_, most_JJS of_IN the_DT times_NNS they_PRP rephrase_VBP sentences_NNS when_WRB necessary_JJ ,_, or_CC they_PRP join_VBP different_JJ related_JJ information_NN into_IN one_CD sentence_NN ._.
Evaluating_VBG summaries_NNS qualitatively_RB The_DT main_JJ drawback_NN of_IN the_DT evaluation_NN systems_NNS existing_VBG so_RB far_RB is_VBZ that_IN we_PRP need_VBP at_IN least_JJS one_CD reference_NN summary_NN ,_, and_CC for_IN some_DT methods_NNS more_JJR than_IN one_CD ,_, to_TO be_VB able_JJ to_TO compare_VB automatic_JJ summaries_NNS with_IN models_NNS ._.
This_DT is_VBZ a_DT hard_JJ and_CC expensive_JJ task_NN ._.
Much_JJ effort_NN has_VBZ to_TO be_VB done_VBN in_IN order_NN to_TO have_VB corpus_NN of_IN texts_NNS and_CC their_PRP$ corresponding_JJ summaries_NNS ._.
Furthermore_RB ,_, for_IN some_DT methods_NNS presented_VBN in_IN the_DT previous_JJ Section_NN ,_, not_RB only_RB do_VBP we_PRP need_VB to_TO have_VB human-made_JJ summaries_NNS available_JJ for_IN comparison_NN ,_, but_CC also_RB manual_JJ annotation_NN has_VBZ to_TO be_VB performed_VBN in_IN some_DT of_IN them_PRP -LRB-_-LRB- e.g._FW SCU_NNP in_IN the_DT Pyramid_NN Method_NN -RRB-_-RRB- ._.
In_IN any_DT case_NN ,_, what_WP the_DT evaluation_NN methods_NNS need_VBP as_IN an_DT input_NN ,_, is_VBZ a_DT set_NN of_IN summaries_NNS to_TO serve_VB as_IN gold_JJ standards_NNS and_CC a_DT set_NN of_IN automatic_JJ summaries_NNS ._.
Moreover_RB ,_, they_PRP all_DT perform_VBP a_DT quantitative_JJ evaluation_NN with_IN regard_NN to_TO different_JJ similarity_NN metrics_NNS ._.
To_TO overcome_VB these_DT problems_NNS ,_, we_PRP think_VBP that_IN the_DT quantitative_JJ evaluation_NN might_MD not_RB be_VB the_DT only_JJ way_NN to_TO evaluate_VB summaries_NNS ,_, and_CC a_DT qualitative_JJ automatic_JJ evaluation_NN would_MD be_VB also_RB important_JJ ._.
Therefore_RB ,_, the_DT second_JJ aim_NN of_IN this_DT paper_NN is_VBZ to_TO suggest_VB a_DT novel_JJ proposal_NN for_IN evaluating_VBG automatically_RB the_DT quality_NN of_IN a_DT summary_NN in_IN a_DT qualitative_JJ manner_NN rather_RB than_IN in_IN a_DT quantitative_JJ one_CD ._.
Our_PRP$ evaluation_NN approach_NN is_VBZ a_DT preliminary_JJ approach_NN which_WDT has_VBZ to_TO be_VB studied_VBN more_RBR deeply_RB ,_, and_CC developed_VBD in_IN the_DT future_NN ._.
Its_PRP$ main_JJ underlying_JJ idea_NN is_VBZ to_TO define_VB several_JJ quality_NN criteria_NNS and_CC check_VB how_WRB a_DT generated_VBN summary_NN tackles_VBZ each_DT of_IN these_DT ,_, in_IN such_PDT a_DT way_NN that_IN a_DT reference_NN model_NN would_MD not_RB be_VB necessary_JJ anymore_RB ,_, taking_VBG only_RB into_IN consideration_NN the_DT automatic_JJ summary_NN and_CC the_DT original_JJ source_NN ._.
Once_RB performed_VBN ,_, it_PRP could_MD be_VB used_VBN together_RB with_IN any_DT other_JJ automatic_JJ methodology_NN to_TO measure_VB summary_NN 's_POS informativeness_NN ._.
Natural_JJ Language_NN Generation_NN -LRB-_-LRB- NLG_NN -RRB-_-RRB- is_VBZ the_DT natural_JJ language_NN processing_NN task_NN of_IN generating_VBG natural_JJ language_NN from_IN a_DT machine_NN representation_NN system_NN such_JJ as_IN a_DT knowledge_NN base_NN or_CC a_DT logical_JJ form_NN ._.
Psycholinguists_NNS prefer_VBP the_DT term_NN language_NN production_NN when_WRB such_JJ formal_JJ representations_NNS are_VBP interpreted_VBN as_IN models_NNS for_IN mental_JJ representations_NNS ._.
In_IN a_DT sense_NN ,_, one_PRP can_MD say_VB that_IN an_DT NLG_NN system_NN is_VBZ like_IN a_DT translator_NN that_WDT converts_VBZ a_DT computer_NN based_VBN representation_NN into_IN a_DT natural_JJ language_NN representation_NN ._.
However_RB ,_, the_DT methods_NNS to_TO produce_VB the_DT final_JJ language_NN are_VBP very_RB different_JJ from_IN those_DT of_IN a_DT compiler_NN due_JJ to_TO the_DT inherent_JJ expressivity_NN of_IN natural_JJ languages_NNS ._.
NLG_NN may_MD be_VB viewed_VBN as_IN the_DT opposite_NN of_IN natural_JJ language_NN understanding_NN ._.
The_DT difference_NN can_MD be_VB put_VBN this_DT way_NN :_: whereas_IN in_IN natural_JJ language_NN understanding_VBG the_DT system_NN needs_VBZ to_TO disambiguate_VB the_DT input_NN sentence_NN to_TO produce_VB the_DT machine_NN representation_NN language_NN ,_, in_IN NLG_NNP the_DT system_NN needs_VBZ to_TO make_VB decisions_NNS about_IN how_WRB to_TO put_VB a_DT concept_NN into_IN words_NNS ._.
The_DT simplest_JJS -LRB-_-LRB- and_CC perhaps_RB trivial_JJ -RRB-_-RRB- examples_NNS are_VBP systems_NNS that_WDT generate_VBP form_NN letters_NNS ._.
Such_JJ systems_NNS do_VBP not_RB typically_RB involve_VB grammar_NN rules_NNS ,_, but_CC may_MD generate_VB a_DT letter_NN to_TO a_DT consumer_NN ,_, e.g._FW stating_VBG that_IN a_DT credit_NN card_NN spending_NN limit_NN is_VBZ about_IN to_TO be_VB reached_VBN ._.
More_RBR complex_JJ NLG_NNP systems_NNS dynamically_RB create_VBP texts_NNS to_TO meet_VB a_DT communicative_JJ goal_NN ._.
As_IN in_IN other_JJ areas_NNS of_IN natural_JJ language_NN processing_NN ,_, this_DT can_MD be_VB done_VBN using_VBG either_CC explicit_JJ models_NNS of_IN language_NN -LRB-_-LRB- e.g._FW ,_, grammars_NNS -RRB-_-RRB- and_CC the_DT domain_NN ,_, or_CC using_VBG statistical_JJ models_NNS derived_VBN by_IN analyzing_VBG human-written_JJ texts_NNS ._.
NLG_NNP is_VBZ a_DT fast-evolving_JJ field_NN ._.
The_DT best_JJS single_JJ source_NN for_IN up-to-date_JJ research_NN in_IN the_DT area_NN is_VBZ the_DT SIGGEN_NNP portion_NN of_IN the_DT ACL_NN Anthology_NN ._.
Perhaps_RB the_DT closest_JJS the_DT field_NN comes_VBZ to_TO a_DT specialist_NN textbook_NN is_VBZ Reiter_NNP and_CC Dale_NNP -LRB-_-LRB- 2000_CD -RRB-_-RRB- ,_, but_CC this_DT book_NN does_VBZ not_RB describe_VB developments_NNS in_IN the_DT field_NN since_IN 2000_CD ._.
This_DT system_NN takes_VBZ as_IN input_NN six_CD numbers_NNS ,_, which_WDT give_VBP predicted_VBN pollen_NN levels_NNS in_IN different_JJ parts_NNS of_IN Scotland_NNP ._.
From_IN these_DT numbers_NNS ,_, the_DT system_NN generates_VBZ a_DT short_JJ textual_JJ summary_NN of_IN pollen_NN levels_NNS as_IN its_PRP$ output_NN ._.
For_IN example_NN ,_, using_VBG the_DT historical_JJ data_NNS for_IN 1-July-2005_NN ,_, the_DT software_NN produces_VBZ Grass_NN pollen_NN levels_NNS for_IN Friday_NNP have_VBP increased_VBN from_IN the_DT moderate_JJ to_TO high_JJ levels_NNS of_IN yesterday_NN with_IN values_NNS of_IN around_IN 6_CD to_TO 7_CD across_IN most_JJS parts_NNS of_IN the_DT country_NN ._.
However_RB ,_, in_IN Northern_JJ areas_NNS ,_, pollen_NN levels_NNS will_MD be_VB moderate_JJ with_IN values_NNS of_IN 4_CD ._.
In_IN contrast_NN ,_, the_DT actual_JJ forecast_NN -LRB-_-LRB- written_VBN by_IN a_DT human_JJ meteorologist_NN -RRB-_-RRB- from_IN this_DT data_NN was_VBD Pollen_NNP counts_NNS are_VBP expected_VBN to_TO remain_VB high_JJ at_IN level_NN 6_CD over_IN most_JJS of_IN Scotland_NNP ,_, and_CC even_RB level_NN 7_CD in_IN the_DT south_RB east_JJ ._.
The_DT only_JJ relief_NN is_VBZ in_IN the_DT Northern_NNP Isles_NNPS and_CC far_RB northeast_JJ of_IN mainland_NN Scotland_NNP with_IN medium_NN levels_NNS of_IN pollen_NN count_NN ._.
Comparing_VBG these_DT two_CD illustrates_VBZ some_DT of_IN the_DT choices_NNS that_IN NLG_NNP systems_NNS must_MD make_VB ;_: these_DT are_VBP further_RB discussed_VBN below_IN ._.
Stages_NNS The_DT process_NN to_TO generate_VB text_NN can_MD be_VB as_RB simple_JJ as_IN keeping_VBG a_DT list_NN of_IN canned_JJ text_NN that_WDT is_VBZ copied_VBN and_CC pasted_VBN ,_, possibly_RB linked_VBN with_IN some_DT glue_NN text_NN ._.
The_DT results_NNS may_MD be_VB satisfactory_JJ in_IN simple_JJ domains_NNS such_JJ as_IN horoscope_NN machines_NNS or_CC generators_NNS of_IN personalised_JJ business_NN letters_NNS ._.
However_RB ,_, a_DT sophisticated_JJ NLG_NN system_NN needs_VBZ to_TO include_VB stages_NNS of_IN planning_VBG and_CC merging_VBG of_IN information_NN to_TO enable_VB the_DT generation_NN of_IN text_NN that_WDT looks_VBZ natural_JJ and_CC does_VBZ not_RB become_VB repetitive_JJ ._.
Typical_JJ stages_NNS are_VBP :_: Content_JJ determination_NN :_: Deciding_VBG what_WP information_NN to_TO mention_VB in_IN the_DT text_NN ._.
For_IN instance_NN ,_, in_IN the_DT pollen_NN example_NN above_IN ,_, deciding_VBG whether_IN to_TO explicitly_RB mention_VB that_DT pollen_NN level_NN is_VBZ 7_CD in_IN the_DT south_RB east_JJ ._.
Document_NNP structuring_VBG :_: Overall_JJ organization_NN of_IN the_DT information_NN to_TO convey_VB ._.
For_IN example_NN ,_, deciding_VBG to_TO describe_VB the_DT areas_NNS with_IN high_JJ pollen_NN levels_NNS first_RB ,_, instead_RB of_IN the_DT areas_NNS with_IN low_JJ pollen_NN levels_NNS ._.
Aggregation_NN :_: Merging_VBG of_IN similar_JJ sentences_NNS to_TO improve_VB readability_NN and_CC naturalness_NN ._.
For_IN instance_NN ,_, merging_VBG the_DT two_CD sentences_NNS Grass_NN pollen_NN levels_NNS for_IN Friday_NNP have_VBP increased_VBN from_IN the_DT moderate_JJ to_TO high_JJ levels_NNS of_IN yesterday_NN and_CC Grass_NN pollen_NN levels_NNS will_MD be_VB around_IN 6_CD to_TO 7_CD across_IN most_JJS parts_NNS of_IN the_DT country_NN into_IN the_DT single_JJ sentence_NN Grass_NN pollen_NN levels_NNS for_IN Friday_NNP have_VBP increased_VBN from_IN the_DT moderate_JJ to_TO high_JJ levels_NNS of_IN yesterday_NN with_IN values_NNS of_IN around_IN 6_CD to_TO 7_CD across_IN most_JJS parts_NNS of_IN the_DT country_NN ._.
Lexical_JJ choice_NN :_: Putting_VBG words_NNS to_TO the_DT concepts_NNS ._.
For_IN example_NN ,_, deciding_VBG whether_IN medium_NN or_CC moderate_JJ should_MD be_VB used_VBN when_WRB describing_VBG a_DT pollen_NN level_NN of_IN 4_CD ._.
Referring_VBG expression_NN generation_NN :_: Creating_VBG referring_VBG expressions_NNS that_WDT identify_VBP objects_NNS and_CC regions_NNS ._.
For_IN example_NN ,_, deciding_VBG to_TO use_VB in_IN the_DT Northern_NNP Isles_NNPS and_CC far_RB northeast_JJ of_IN mainland_JJ Scotland_NNP to_TO refer_VB to_TO a_DT certain_JJ region_NN in_IN Scotland_NNP ._.
This_DT task_NN also_RB includes_VBZ making_VBG decisions_NNS about_IN pronouns_NNS and_CC other_JJ types_NNS of_IN anaphora_NN ._.
Realisation_NN :_: Creating_VBG the_DT actual_JJ text_NN ,_, which_WDT should_MD be_VB correct_JJ according_VBG to_TO the_DT rules_NNS of_IN syntax_NN ,_, morphology_NN ,_, and_CC orthography_NN ._.
For_IN example_NN ,_, using_VBG will_MD be_VB for_IN the_DT future_NN tense_JJ of_IN to_TO be_VB ._.
Applications_NNS The_DT popular_JJ media_NNS has_VBZ been_VBN especially_RB interested_JJ in_IN NLG_NNP systems_NNS which_WDT generate_VBP jokes_NNS -LRB-_-LRB- see_VB computational_JJ humor_NN -RRB-_-RRB- ._.
But_CC from_IN a_DT commercial_JJ perspective_NN ,_, the_DT most_RBS successful_JJ NLG_NNP applications_NNS have_VBP been_VBN data-to-text_JJ systems_NNS which_WDT generate_VBP textual_JJ summaries_NNS of_IN databases_NNS and_CC data_NNS sets_NNS ;_: these_DT systems_NNS usually_RB perform_VBP data_NN analysis_NN as_RB well_RB as_IN text_NN generation_NN ._.
In_IN particular_JJ ,_, several_JJ systems_NNS have_VBP been_VBN built_VBN that_WDT produce_VBP textual_JJ weather_NN forecasts_NNS from_IN weather_NN data_NNS ._.
The_DT earliest_JJS such_JJ system_NN to_TO be_VB deployed_VBN was_VBD FoG_NN ,_, which_WDT was_VBD used_VBN by_IN Environment_NNP Canada_NNP to_TO generate_VB weather_NN forecasts_NNS in_IN French_NNP and_CC English_NNP in_IN the_DT early_JJ 1990s_NNS ._.
The_DT success_NN of_IN FoG_NN triggered_VBD other_JJ work_NN ,_, both_CC research_NN and_CC commercial_NN ._.
Recent_JJ research_NN in_IN this_DT area_NN include_VBP an_DT experiment_NN which_WDT showed_VBD that_IN users_NNS sometimes_RB preferred_VBN computer-generated_JJ weather_NN forecasts_NNS to_TO human-written_JJ ones_NNS ,_, in_IN part_NN because_IN the_DT computer_NN forecasts_NNS used_VBD more_RBR consistent_JJ terminology_NN ,_, and_CC a_DT demonstration_NN that_IN statistical_JJ techniques_NNS could_MD be_VB used_VBN to_TO generate_VB high-quality_JJ weather_NN forecasts_NNS ._.
Recent_JJ applications_NNS include_VBP the_DT ARNS_NN system_NN used_VBN to_TO summarise_VB conditions_NNS in_IN US_NNP ports_NNS ._.
In_IN the_DT 1990s_CD there_EX was_VBD considerable_JJ interest_NN in_IN using_VBG NLG_NN to_TO summarise_VB financial_JJ and_CC business_NN data_NNS ._.
For_IN example_NN the_DT SPOTLIGHT_NN system_NN developed_VBD at_IN A.C._NNP Nielsen_NNP automatically_RB generated_VBD readable_JJ English_NNP text_NN based_VBN on_IN the_DT analysis_NN of_IN large_JJ amounts_NNS of_IN retail_JJ sales_NNS data_NNS ._.
More_RBR recently_RB there_EX is_VBZ growing_VBG interest_NN in_IN using_VBG NLG_NN to_TO summarise_VB electronic_JJ medical_JJ records_NNS ._.
Commercial_JJ applications_NNS in_IN this_DT area_NN are_VBP starting_VBG to_TO appear_VB ,_, and_CC researchers_NNS have_VBP shown_VBN that_IN NLG_NN summaries_NNS of_IN medical_JJ data_NNS can_MD be_VB effective_JJ decision-support_JJ aids_NNS for_IN medical_JJ professionals_NNS ._.
There_EX is_VBZ also_RB growing_VBG interest_NN in_IN using_VBG NLG_NN to_TO enhance_VB accessibility_NN ,_, for_IN example_NN by_IN describing_VBG graphs_NNS and_CC data_NNS sets_NNS to_TO blind_JJ people_NNS ._.
An_DT example_NN for_IN a_DT highly_RB interactive_JJ use_NN of_IN NLG_NN is_VBZ the_DT WYSIWYM_NNP framework_NN ._.
It_PRP stands_VBZ for_IN What_WP you_PRP see_VBP is_VBZ what_WP you_PRP meant_VBD and_CC allows_VBZ users_NNS to_TO see_VB and_CC manipulate_VB the_DT continuously_RB rendered_VBN view_NN -LRB-_-LRB- NLG_NN output_NN -RRB-_-RRB- of_IN an_DT underlying_VBG formal_JJ language_NN document_NN -LRB-_-LRB- NLG_NN input_NN -RRB-_-RRB- ,_, thereby_RB editing_NN the_DT formal_JJ language_NN without_IN having_VBG to_TO learn_VB it_PRP ._.
Evaluation_NN As_IN in_IN other_JJ scientific_JJ fields_NNS ,_, NLG_NN researchers_NNS need_VBP to_TO be_VB able_JJ to_TO test_VB how_WRB well_RB their_PRP$ systems_NNS ,_, modules_NNS ,_, and_CC algorithms_NNS work_VBP ._.
This_DT is_VBZ called_VBN evaluation_NN ._.
There_EX are_VBP three_CD basic_JJ techniques_NNS for_IN evaluating_VBG NLG_NNP systems_NNS :_: task-based_JJ -LRB-_-LRB- extrinsic_JJ -RRB-_-RRB- evaluation_NN :_: give_VB the_DT generated_VBN text_NN to_TO a_DT person_NN ,_, and_CC assess_VB how_WRB well_RB it_PRP helps_VBZ him_PRP perform_VB a_DT task_NN -LRB-_-LRB- or_CC otherwise_RB achieves_VBZ its_PRP$ communicative_JJ goal_NN -RRB-_-RRB- ._.
For_IN example_NN ,_, a_DT system_NN which_WDT generates_VBZ summaries_NNS of_IN medical_JJ data_NNS can_MD be_VB evaluated_VBN by_IN giving_VBG these_DT summaries_NNS to_TO doctors_NNS ,_, and_CC assessing_VBG whether_IN the_DT summaries_NNS helps_VBZ doctors_NNS make_VB better_JJR decisions_NNS ._.
human_JJ ratings_NNS :_: give_VB the_DT generated_VBN text_NN to_TO a_DT person_NN ,_, and_CC ask_VB him_PRP or_CC her_PRP to_TO rate_VB the_DT quality_NN and_CC usefulness_NN of_IN the_DT text_NN ._.
metrics_NNS :_: compare_VB generated_VBN texts_NNS to_TO texts_NNS written_VBN by_IN people_NNS from_IN the_DT same_JJ input_NN data_NNS ,_, using_VBG an_DT automatic_JJ metric_NN such_JJ as_IN BLEU_NNP ._.
Generally_RB speaking_VBG ,_, what_WP we_PRP ultimately_RB want_VBP to_TO know_VB is_VBZ how_WRB useful_JJ NLG_NNP systems_NNS are_VBP at_IN helping_VBG people_NNS ,_, which_WDT is_VBZ the_DT first_JJ of_IN the_DT above_JJ techniques_NNS ._.
However_RB ,_, task-based_JJ evaluations_NNS are_VBP time-consuming_JJ and_CC expensive_JJ ,_, and_CC can_MD be_VB difficult_JJ to_TO carry_VB out_RP -LRB-_-LRB- especially_RB if_IN they_PRP require_VBP subjects_NNS with_IN specialised_JJ expertise_NN ,_, such_JJ as_IN doctors_NNS -RRB-_-RRB- ._.
Hence_RB -LRB-_-LRB- as_IN in_IN other_JJ areas_NNS of_IN NLP_NN -RRB-_-RRB- task-based_JJ evaluations_NNS are_VBP the_DT exception_NN ,_, not_RB the_DT norm_NN ._.
In_IN recent_JJ years_NNS researchers_NNS have_VBP started_VBN trying_VBG to_TO assess_VB how_WRB well_RB human-ratings_NNS and_CC metrics_NNS correlate_VBP with_IN -LRB-_-LRB- predict_VBP -RRB-_-RRB- task-based_JJ evaluations_NNS ._.
Much_JJ of_IN this_DT work_NN is_VBZ being_VBG conducted_VBN in_IN the_DT context_NN of_IN Generation_NN Challenges_VBZ shared-task_JJ events_NNS ._.
Initial_JJ results_NNS suggest_VBP that_IN human_JJ ratings_NNS are_VBP much_RB better_JJR than_IN metrics_NNS in_IN this_DT regard_NN ._.
In_IN other_JJ words_NNS ,_, human_JJ ratings_NNS usually_RB do_VBP predict_VB task-effectiveness_NN at_IN least_JJS to_TO some_DT degree_NN -LRB-_-LRB- although_IN there_EX are_VBP exceptions_NNS -RRB-_-RRB- ,_, while_IN ratings_NNS produced_VBN by_IN metrics_NNS often_RB do_VBP not_RB predict_VB task-effectiveness_NN well_RB ._.
These_DT results_NNS are_VBP very_RB preliminary_JJ ,_, hopefully_RB better_JJR data_NNS will_MD be_VB available_JJ soon_RB ._.
In_IN any_DT case_NN ,_, human_JJ ratings_NNS are_VBP currently_RB the_DT most_RBS popular_JJ evaluation_NN technique_NN in_IN NLG_NN ;_: this_DT is_VBZ contrast_NN to_TO machine_NN translation_NN ,_, where_WRB metrics_NNS are_VBP very_RB widely_RB used_VBN ._.
Natural_JJ language_NN understanding_NN is_VBZ a_DT subtopic_NN of_IN natural_JJ language_NN processing_NN in_IN artificial_JJ intelligence_NN that_IN deals_NNS with_IN machine_NN reading_NN comprehension_NN ._.
The_DT process_NN of_IN disassembling_VBG and_CC parsing_VBG input_NN is_VBZ more_RBR complex_JJ than_IN the_DT reverse_JJ process_NN of_IN assembling_VBG output_NN in_IN natural_JJ language_NN generation_NN because_IN of_IN the_DT occurrence_NN of_IN unknown_JJ and_CC unexpected_JJ features_NNS in_IN the_DT input_NN and_CC the_DT need_NN to_TO determine_VB the_DT appropriate_JJ syntactic_JJ and_CC semantic_JJ schemes_NNS to_TO apply_VB to_TO it_PRP ,_, factors_NNS which_WDT are_VBP pre-determined_JJ when_WRB outputting_VBG language_NN ._.
There_EX is_VBZ considerable_JJ commercial_JJ interest_NN in_IN the_DT field_NN because_IN of_IN its_PRP$ application_NN to_TO news-gathering_NN ,_, text_NN categorization_NN ,_, voice-activation_NN ,_, archiving_NN and_CC large-scale_JJ content-analysis_NN ._.
Eight_CD years_NNS after_IN John_NNP McCarthy_NNP coined_VBD the_DT term_NN artificial_JJ intelligence_NN ,_, Bobrow_NNP 's_POS dissertation_NN -LRB-_-LRB- titled_VBN Natural_NNP Language_NNP Input_NNP for_IN a_DT Computer_NNP Problem_NNP Solving_VBG System_NN -RRB-_-RRB- showed_VBD how_WRB a_DT computer_NN can_MD understand_VB simple_JJ natural_JJ language_NN input_NN to_TO solve_VB algebra_NN word_NN problems_NNS ._.
A_DT year_NN later_RB ,_, in_IN 1965_CD ,_, Joseph_NNP Weizenbaum_NNP at_IN MIT_NNP wrote_VBD ELIZA_NNP ,_, an_DT interactive_JJ program_NN that_WDT carried_VBD on_IN a_DT dialogue_NN in_IN English_NNP on_IN any_DT topic_NN ,_, the_DT most_RBS popular_JJ being_VBG psychotherapy_NN ._.
ELIZA_NNP worked_VBD by_IN simple_JJ parsing_NN and_CC substitution_NN of_IN key_JJ words_NNS into_IN canned_JJ phrases_NNS and_CC Weizenbaum_NNP sidestepped_VBD the_DT problem_NN of_IN giving_VBG the_DT program_NN a_DT database_NN of_IN real-world_JJ knowledge_NN or_CC a_DT rich_JJ lexicon_NN ._.
Yet_RB ELIZA_NNP gained_VBD surprising_JJ popularity_NN as_IN a_DT toy_NN project_NN and_CC can_MD be_VB seen_VBN as_IN a_DT very_RB early_JJ precursor_NN to_TO current_JJ commercial_JJ systems_NNS such_JJ as_IN those_DT used_VBN by_IN Ask.com_NN ._.
In_IN 1969_CD Roger_NNP Schank_NNP at_IN Stanford_NNP University_NNP introduced_VBD the_DT conceptual_JJ dependency_NN theory_NN for_IN natural_JJ language_NN understanding_NN ._.
This_DT model_NN ,_, partially_RB influenced_VBN by_IN the_DT work_NN of_IN Sydney_NNP Lamb_NNP ,_, was_VBD extensively_RB used_VBN by_IN Schank_NNP 's_POS students_NNS at_IN Yale_NNP University_NNP ,_, such_JJ as_IN Robert_NNP Wilensky_NNP ,_, Wendy_NNP Lehnert_NNP ,_, and_CC Janet_NNP Kolodner_NNP ._.
In_IN 1970_CD ,_, William_NNP A._NNP Woods_NNP introduced_VBD the_DT augmented_JJ transition_NN network_NN -LRB-_-LRB- ATN_NN -RRB-_-RRB- to_TO represent_VB natural_JJ language_NN input_NN ._.
Instead_RB of_IN phrase_NN structure_NN rules_NNS ATNs_NNS used_VBD an_DT equivalent_JJ set_NN of_IN finite_JJ state_NN automata_NN that_WDT were_VBD called_VBN recursively_RB ._.
ATNs_NNS and_CC their_PRP$ more_RBR general_JJ format_NN called_VBN ``_`` generalized_JJ ATNs_NNS ''_'' continued_VBD to_TO be_VB used_VBN for_IN a_DT number_NN of_IN years_NNS ._.
In_IN 1971_CD Terry_NNP Winograd_NNP finished_VBD writing_VBG SHRDLU_NNP for_IN his_PRP$ PhD_NN thesis_NN at_IN MIT_NNP ._.
SHRDLU_NNP could_MD understand_VB simple_JJ English_JJ sentences_NNS in_IN a_DT restricted_JJ world_NN of_IN children_NNS 's_POS blocks_NNS to_TO direct_VB a_DT robotic_JJ arm_NN to_TO move_VB items_NNS ._.
The_DT successful_JJ demonstration_NN of_IN SHRDLU_NNP provided_VBD significant_JJ momentum_NN for_IN continued_JJ research_NN in_IN the_DT field_NN ._.
Winograd_NNP continued_VBD to_TO be_VB a_DT major_JJ influence_NN in_IN the_DT field_NN with_IN the_DT publication_NN of_IN his_PRP$ book_NN Language_NN as_IN a_DT Cognitive_JJ Process_NN ._.
At_IN Stanford_NNP ,_, Winograd_NNP was_VBD later_RB the_DT adviser_NN for_IN Larry_NNP Page_NNP ,_, who_WP co-founded_VBD Google_NNP ._.
In_IN the_DT 1970s_CD and_CC 1980s_CD the_DT natural_JJ language_NN processing_NN group_NN at_IN SRI_NN International_JJ continued_JJ research_NN and_CC development_NN in_IN the_DT field_NN ._.
A_DT number_NN of_IN commercial_JJ efforts_NNS based_VBN on_IN the_DT research_NN were_VBD undertaken_VBN ,_, e.g._FW ,_, in_IN 1982_CD Gary_NNP Hendrix_NNP formed_VBD Symantec_NNP Corporation_NNP originally_RB as_IN a_DT company_NN for_IN developing_VBG a_DT natural_JJ language_NN interface_NN for_IN database_NN queries_NNS on_IN personal_JJ computers_NNS ._.
However_RB ,_, with_IN the_DT advent_NN of_IN mouse_NN driven_JJ ,_, graphic_JJ user_NN interfaces_NNS Symantec_NNP changed_VBD direction_NN ._.
A_DT number_NN of_IN other_JJ commercial_JJ efforts_NNS were_VBD started_VBN around_IN the_DT same_JJ time_NN ,_, e.g._FW ,_, Larry_NNP R._NNP Harris_NNP at_IN the_DT Artificial_NNP Intelligence_NNP Corporation_NNP and_CC Roger_NNP Schank_NNP and_CC his_PRP$ students_NNS at_IN Cognitive_NNP Systems_NNP corp._NNP ._.
In_IN 1983_CD ,_, Michael_NNP Dyer_NNP developed_VBD the_DT BORIS_NNP system_NN at_IN Yale_NNP which_WDT bore_VBD similarities_NNS to_TO the_DT work_NN of_IN Roger_NNP Schank_NNP and_CC W._NNP G._NNP Lehnart_NNP ._.
Scope_NN and_CC context_NN The_DT umbrella_NN term_NN ``_`` natural_JJ language_NN understanding_NN ''_'' can_MD be_VB applied_VBN to_TO a_DT diverse_JJ set_NN of_IN computer_NN applications_NNS ,_, ranging_VBG from_IN small_JJ ,_, relatively_RB simple_JJ tasks_NNS such_JJ as_IN short_JJ commands_NNS issued_VBN to_TO robots_NNS ,_, to_TO highly_RB complex_JJ endeavors_NNS such_JJ as_IN the_DT full_JJ comprehension_NN of_IN newspaper_NN articles_NNS or_CC poetry_NN passages_NNS ._.
Many_JJ real_JJ world_NN applications_NNS fall_VBP between_IN the_DT two_CD extremes_NNS ,_, for_IN instance_NN text_NN classification_NN for_IN the_DT automatic_JJ analysis_NN of_IN emails_NNS and_CC their_PRP$ routing_VBG to_TO a_DT suitable_JJ department_NN in_IN a_DT corporation_NN does_VBZ not_RB require_VB in_IN depth_NN understanding_NN of_IN the_DT text_NN ,_, but_CC is_VBZ far_RB more_RBR complex_JJ than_IN the_DT management_NN of_IN simple_JJ queries_NNS to_TO database_NN tables_NNS with_IN fixed_JJ schemata_NN ._.
Throughout_IN the_DT years_NNS various_JJ attempts_NNS at_IN processing_VBG natural_JJ language_NN or_CC English-like_JJ sentences_NNS presented_VBN to_TO computers_NNS have_VBP taken_VBN place_NN at_IN varying_VBG degrees_NNS of_IN complexity_NN ._.
Some_DT attempts_NNS have_VBP not_RB resulted_VBN in_IN systems_NNS with_IN deep_JJ understanding_NN ,_, but_CC have_VBP helped_VBN overall_JJ system_NN usability_NN ._.
For_IN example_NN ,_, Wayne_NNP Ratliff_NNP originally_RB developed_VBD the_DT Vulcan_NNP program_NN with_IN an_DT English-like_JJ syntax_NN to_TO mimic_VB the_DT English_NNP speaking_NN computer_NN in_IN Star_NNP Trek_NNP ._.
Vulcan_NNP later_RB became_VBD the_DT dBase_NN system_NN whose_WP$ easy-to-use_JJ syntax_NN effectively_RB launched_VBD the_DT personal_JJ computer_NN database_NN industry_NN ._.
Systems_NNP with_IN an_DT easy_JJ to_TO use_VB or_CC English_NNP like_IN syntax_NN are_VBP ,_, however_RB ,_, quite_RB distinct_JJ from_IN systems_NNS that_WDT use_VBP a_DT rich_JJ lexicon_NN and_CC include_VBP an_DT internal_JJ representation_NN -LRB-_-LRB- often_RB as_IN first_JJ order_NN logic_NN -RRB-_-RRB- of_IN the_DT semantics_NNS of_IN natural_JJ language_NN sentences_NNS ._.
Hence_RB the_DT breadth_NN and_CC depth_NN of_IN ``_`` understanding_NN ''_'' aimed_VBN at_IN by_IN a_DT system_NN determine_VBD both_CC the_DT complexity_NN of_IN the_DT system_NN -LRB-_-LRB- and_CC the_DT implied_JJ challenges_NNS -RRB-_-RRB- and_CC the_DT types_NNS of_IN applications_NNS it_PRP can_MD deal_VB with_IN ._.
The_DT ``_`` breadth_NN ''_'' of_IN a_DT system_NN is_VBZ measured_VBN by_IN the_DT sizes_NNS of_IN its_PRP$ vocabulary_NN and_CC grammar_NN ._.
The_DT ``_`` depth_NN ''_'' is_VBZ measured_VBN by_IN the_DT degree_NN to_TO which_WDT its_PRP$ understanding_NN approximates_VBZ that_IN of_IN a_DT fluent_JJ native_JJ speaker_NN ._.
At_IN the_DT narrowest_JJS and_CC shallowest_JJS ,_, English-like_JJ command_NN interpreters_NNS require_VBP minimal_JJ complexity_NN ,_, but_CC have_VBP a_DT small_JJ range_NN of_IN applications_NNS ._.
Narrow_JJ but_CC deep_JJ systems_NNS explore_VB and_CC model_VB mechanisms_NNS of_IN understanding_NN ,_, but_CC they_PRP still_RB have_VBP limited_VBN application_NN ._.
Systems_NNP that_WDT attempt_VBP to_TO understand_VB the_DT contents_NNS of_IN a_DT document_NN such_JJ as_IN a_DT news_NN release_NN beyond_IN simple_JJ keyword_JJ matching_NN and_CC to_TO judge_VB its_PRP$ suitability_NN for_IN a_DT user_NN are_VBP broader_JJR and_CC require_VB significant_JJ complexity_NN ,_, but_CC they_PRP are_VBP still_RB somewhat_RB shallow_JJ ._.
Systems_NNP that_WDT are_VBP both_DT very_RB broad_JJ and_CC very_RB deep_JJ are_VBP beyond_IN the_DT current_JJ state_NN of_IN the_DT art_NN ._.
Components_NNP and_CC architecture_NN Regardless_RB of_IN the_DT approach_NN used_VBN ,_, some_DT common_JJ components_NNS can_MD be_VB identified_VBN in_IN most_JJS natural_JJ language_NN understanding_NN systems_NNS ._.
The_DT system_NN needs_VBZ a_DT lexicon_NN of_IN the_DT language_NN and_CC a_DT parser_NN and_CC grammar_NN rules_NNS to_TO break_VB sentences_NNS into_IN an_DT internal_JJ representation_NN ._.
The_DT construction_NN of_IN a_DT rich_JJ lexicon_NN with_IN a_DT suitable_JJ ontology_NN requires_VBZ significant_JJ effort_NN ,_, e.g._FW ,_, the_DT Wordnet_NNP lexicon_NN required_VBD many_JJ person-years_NNS of_IN effort_NN ._.
The_DT system_NN also_RB needs_VBZ a_DT semantic_JJ theory_NN to_TO guide_VB the_DT comprehension_NN ._.
The_DT interpretation_NN capabilities_NNS of_IN a_DT language_NN understanding_NN system_NN depend_VBP on_IN the_DT semantic_JJ theory_NN it_PRP uses_VBZ ._.
Competing_VBG semantic_JJ theories_NNS of_IN language_NN have_VBP specific_JJ trade_NN offs_NNS in_IN their_PRP$ suitability_NN as_IN the_DT basis_NN of_IN computer_NN automated_JJ semantic_JJ interpretation_NN ._.
These_DT range_NN from_IN naive_JJ semantics_NNS or_CC stochastic_JJ semantic_JJ analysis_NN to_TO the_DT use_NN of_IN pragmatics_NNS to_TO derive_VB meaning_NN from_IN context_NN ._.
Advanced_NNP applications_NNS of_IN natural_JJ language_NN understanding_NN also_RB attempt_VBP to_TO incorporate_VB logical_JJ inference_NN within_IN their_PRP$ framework_NN ._.
This_DT is_VBZ generally_RB achieved_VBN by_IN mapping_VBG the_DT derived_VBN meaning_NN into_IN a_DT set_NN of_IN assertions_NNS in_FW predicate_FW logic_NN ,_, then_RB using_VBG logical_JJ deduction_NN to_TO arrive_VB at_IN conclusions_NNS ._.
Systems_NNP based_VBN on_IN functional_JJ languages_NNS such_JJ as_IN Lisp_NN hence_RB need_VBP to_TO include_VB a_DT subsystem_NN for_IN the_DT representation_NN of_IN logical_JJ assertions_NNS ,_, while_IN logic_NN oriented_JJ systems_NNS such_JJ as_IN those_DT using_VBG the_DT language_NN Prolog_NNP generally_RB rely_VBP on_IN an_DT extension_NN of_IN the_DT built_VBN in_IN logical_JJ representation_NN framework_NN ._.
The_DT management_NN of_IN context_NN in_IN natural_JJ language_NN understanding_NN can_MD present_VB special_JJ challenges_NNS ._.
A_DT large_JJ variety_NN of_IN examples_NNS and_CC counter_JJ examples_NNS have_VBP resulted_VBN in_IN multiple_JJ approaches_NNS to_TO the_DT formal_JJ modeling_NN of_IN context_NN ,_, each_DT with_IN specific_JJ strengths_NNS and_CC weaknesses_NNS ._.
Optical_NNP character_NN recognition_NN ,_, usually_RB abbreviated_VBD to_TO OCR_NNP ,_, is_VBZ the_DT mechanical_JJ or_CC electronic_JJ conversion_NN of_IN scanned_VBN images_NNS of_IN handwritten_JJ ,_, typewritten_JJ or_CC printed_VBN text_NN into_IN machine-encoded_JJ text_NN ._.
It_PRP is_VBZ widely_RB used_VBN as_IN a_DT form_NN of_IN data_NNS entry_NN from_IN some_DT sort_NN of_IN original_JJ paper_NN data_NNS source_NN ,_, whether_IN documents_NNS ,_, sales_NNS receipts_NNS ,_, mail_NN ,_, or_CC any_DT number_NN of_IN printed_VBN records_NNS ._.
It_PRP is_VBZ crucial_JJ to_TO the_DT computerization_NN of_IN printed_VBN texts_NNS so_IN that_IN they_PRP can_MD be_VB electronically_RB searched_VBN ,_, stored_VBN more_RBR compactly_RB ,_, displayed_VBD on-line_JJ ,_, and_CC used_VBN in_IN machine_NN processes_NNS such_JJ as_IN machine_NN translation_NN ,_, text-to-speech_NN and_CC text_NN mining_NN ._.
OCR_NNP is_VBZ a_DT field_NN of_IN research_NN in_IN pattern_NN recognition_NN ,_, artificial_JJ intelligence_NN and_CC computer_NN vision_NN ._.
Early_JJ versions_NNS needed_VBN to_TO be_VB programmed_VBN with_IN images_NNS of_IN each_DT character_NN ,_, and_CC worked_VBD on_IN one_CD font_NN at_IN a_DT time_NN ._.
``_`` Intelligent_JJ ''_'' systems_NNS with_IN a_DT high_JJ degree_NN of_IN recognition_NN accuracy_NN for_IN most_JJS fonts_NNS are_VBP now_RB common_JJ ._.
Some_DT systems_NNS are_VBP capable_JJ of_IN reproducing_VBG formatted_JJ output_NN that_WDT closely_RB approximates_VBZ the_DT original_JJ scanned_JJ page_NN including_VBG images_NNS ,_, columns_NNS and_CC other_JJ non-textual_JJ components_NNS ._.
In_IN 1914_CD ,_, Emanuel_NNP Goldberg_NNP developed_VBD a_DT machine_NN that_IN read_NN characters_NNS and_CC converted_VBD them_PRP into_IN standard_JJ telegraph_NN code_NN ._.
-LRB-_-LRB- citation_NN needed_VBN -RRB-_-RRB- Around_IN the_DT same_JJ time_NN ,_, Edmund_NNP Fournier_NNP d'Albe_NN developed_VBD the_DT Optophone_NNP ,_, a_DT handheld_JJ scanner_NN that_IN when_WRB moved_VBN across_IN a_DT printed_VBN page_NN ,_, produced_VBD tones_NNS that_WDT corresponded_VBD to_TO specific_JJ letters_NNS or_CC characters_NNS ._.
Goldberg_NNP continued_VBD to_TO develop_VB OCR_NNP technology_NN for_IN data_NNS entry_NN ._.
Later_RB ,_, he_PRP proposed_VBD photographing_VBG data_NNS records_NNS and_CC then_RB ,_, using_VBG photocells_NNS ,_, matching_VBG the_DT photos_NNS against_IN a_DT template_NN containing_VBG the_DT desired_VBN identification_NN pattern_NN ._.
In_IN 1929_CD Gustav_NNP Tauschek_NNP had_VBD similar_JJ ideas_NNS ,_, and_CC obtained_VBD a_DT patent_NN on_IN OCR_NNP in_IN Germany_NNP ._.
Paul_NNP W._NNP Handel_NNP also_RB obtained_VBD a_DT US_NNP patent_NN on_IN such_JJ template-matching_JJ OCR_NN technology_NN in_IN USA_NNP in_IN 1933_CD -LRB-_-LRB- U.S._NNP Patent_NNP 1,915,993_CD -RRB-_-RRB- ._.
In_IN 1935_CD Tauschek_NN was_VBD also_RB granted_VBN a_DT US_NNP patent_NN on_IN his_PRP$ method_NN -LRB-_-LRB- U.S._NNP Patent_NNP 2,026,329_CD -RRB-_-RRB- ._.
In_IN 1949_CD RCA_NN engineers_NNS worked_VBD on_IN the_DT first_JJ primitive_JJ computer-type_JJ OCR_NN to_TO help_VB blind_JJ people_NNS for_IN the_DT US_NNP Veterans_NNP Administration_NNP ,_, but_CC instead_RB of_IN converting_VBG the_DT printed_VBN characters_NNS to_TO machine_NN language_NN ,_, their_PRP$ device_NN converted_VBD it_PRP to_TO machine_NN language_NN and_CC then_RB spoke_VBD the_DT letters_NNS :_: an_DT early_JJ text-to-speech_JJ technology_NN ._.
It_PRP proved_VBD far_RB too_RB expensive_JJ and_CC was_VBD not_RB pursued_VBN after_IN testing_NN ._.
In_IN 1950_CD ,_, David_NNP H._NNP Shepard_NNP ,_, a_DT cryptanalyst_NN at_IN the_DT Armed_NNP Forces_NNP Security_NNP Agency_NNP in_IN the_DT United_NNP States_NNPS ,_, addressed_VBD the_DT problem_NN of_IN converting_VBG printed_VBN messages_NNS into_IN machine_NN language_NN for_IN computer_NN processing_NN and_CC built_VBD a_DT machine_NN to_TO do_VB this_DT ,_, called_VBN ``_`` Gismo_NNP ._. ''_'' ._.
He_PRP received_VBD a_DT patent_NN for_IN his_PRP$ ``_`` Apparatus_NNP for_IN Reading_NNP ''_'' in_IN 1953_CD U.S._NNP Patent_NNP 2,663,758_CD ._.
``_`` Gismo_NNP ''_'' could_MD read_VB 23_CD letters_NNS of_IN the_DT English_JJ alphabet_NN ,_, comprehend_VBD Morse_NNP Code_NNP ,_, read_VBD musical_JJ notations_NNS ,_, read_VBD aloud_RB from_IN printed_VBN pages_NNS ,_, and_CC duplicate_VB typewritten_JJ pages_NNS ._.
Shepard_NNP went_VBD on_RP to_TO found_VBN Intelligent_NNP Machines_NNP Research_NNP Corporation_NNP -LRB-_-LRB- IMR_NNP -RRB-_-RRB- ,_, which_WDT soon_RB developed_VBD the_DT world_NN 's_POS first_JJ commercial_JJ OCR_NN systems_NNS ._.
In_IN 1955_CD ,_, the_DT first_JJ commercial_JJ system_NN was_VBD installed_VBN at_IN the_DT Reader_NNP 's_POS Digest_NNP ,_, which_WDT used_VBD OCR_NN to_TO input_NN sales_NNS reports_NNS into_IN a_DT computer_NN ._.
It_PRP converted_VBD the_DT typewritten_JJ reports_NNS into_IN punched_VBN cards_NNS for_IN input_NN into_IN the_DT computer_NN in_IN the_DT magazine_NN 's_POS subscription_NN department_NN ,_, for_IN help_NN in_IN processing_VBG the_DT shipment_NN of_IN 15-20_CD million_CD books_NNS a_DT year_NN ._.
The_DT second_JJ system_NN was_VBD sold_VBN to_TO the_DT Standard_NNP Oil_NNP Company_NNP for_IN reading_VBG credit_NN card_NN imprints_NNS for_IN billing_NN purposes_NNS ._.
Other_JJ systems_NNS sold_VBN by_IN IMR_NNP during_IN the_DT late_JJ 1950s_NNS included_VBD a_DT bill_NN stub_NN reader_NN to_TO the_DT Ohio_NNP Bell_NNP Telephone_NNP Company_NNP and_CC a_DT page_NN scanner_NN to_TO the_DT United_NNP States_NNPS Air_NNP Force_NNP for_IN reading_VBG and_CC transmitting_VBG by_IN teletype_NN typewritten_JJ messages_NNS ._.
IBM_NNP and_CC others_NNS were_VBD later_RB licensed_VBN on_IN Shepard_NNP 's_POS OCR_NNP patents_NNS ._.
In_IN about_IN 1965_CD ,_, Reader_NNP 's_POS Digest_NNP and_CC RCA_NNP collaborated_VBD to_TO build_VB an_DT OCR_NNP Document_NNP reader_NN designed_VBN to_TO digitize_VB the_DT serial_JJ numbers_NNS on_IN Reader_NNP 's_POS Digest_NNP coupons_NNS returned_VBD from_IN advertisements_NNS ._.
The_DT fonts_NNS used_VBN on_IN the_DT documents_NNS were_VBD printed_VBN by_IN an_DT RCA_NN Drum_NN printer_NN using_VBG the_DT OCR-A_NN font_NN ._.
The_DT reader_NN was_VBD connected_VBN directly_RB to_TO an_DT RCA_NN 301_CD computer_NN -LRB-_-LRB- one_CD of_IN the_DT first_JJ solid_JJ state_NN computers_NNS -RRB-_-RRB- ._.
This_DT reader_NN was_VBD followed_VBN by_IN a_DT specialised_VBN document_NN reader_NN installed_VBN at_IN TWA_NNP where_WRB the_DT reader_NN processed_VBN Airline_NNP Ticket_NNP stock_NN ._.
The_DT readers_NNS processed_VBD documents_NNS at_IN a_DT rate_NN of_IN 1,500_CD documents_NNS per_IN minute_NN ,_, and_CC checked_VBD each_DT document_NN ,_, rejecting_VBG those_DT it_PRP was_VBD not_RB able_JJ to_TO process_VB correctly_RB ._.
The_DT product_NN became_VBD part_NN of_IN the_DT RCA_NN product_NN line_NN as_IN a_DT reader_NN designed_VBN to_TO process_VB ``_`` Turn_VB around_IN Documents_NNS ''_'' such_JJ as_IN those_DT utility_NN and_CC insurance_NN bills_NNS returned_VBN with_IN payments_NNS ._.
The_DT United_NNP States_NNPS Postal_NNP Service_NNP has_VBZ been_VBN using_VBG OCR_NNP machines_NNS to_TO sort_VB mail_NN since_IN 1965_CD based_VBN on_IN technology_NN devised_VBN primarily_RB by_IN the_DT prolific_JJ inventor_NN Jacob_NNP Rabinow_NNP ._.
The_DT first_JJ use_NN of_IN OCR_NNP in_IN Europe_NNP was_VBD by_IN the_DT British_JJ General_JJ Post_NN Office_NN -LRB-_-LRB- GPO_NN -RRB-_-RRB- ._.
In_IN 1965_CD it_PRP began_VBD planning_VBG an_DT entire_JJ banking_NN system_NN ,_, the_DT National_NNP Giro_NNP ,_, using_VBG OCR_NNP technology_NN ,_, a_DT process_NN that_WDT revolutionized_VBD bill_NN payment_NN systems_NNS in_IN the_DT UK_NNP ._.
Canada_NNP Post_NNP has_VBZ been_VBN using_VBG OCR_NNP systems_NNS since_IN 1971_CD -LRB-_-LRB- citation_NN needed_VBN -RRB-_-RRB- ._.
OCR_NN systems_NNS read_VBP the_DT name_NN and_CC address_NN of_IN the_DT addressee_NN at_IN the_DT first_JJ mechanized_JJ sorting_NN center_NN ,_, and_CC print_VB a_DT routing_VBG bar_NN code_NN on_IN the_DT envelope_NN based_VBN on_IN the_DT postal_JJ code_NN ._.
To_TO avoid_VB confusion_NN with_IN the_DT human-readable_JJ address_NN field_NN which_WDT can_MD be_VB located_JJ anywhere_RB on_IN the_DT letter_NN ,_, special_JJ ink_NN -LRB-_-LRB- orange_NN in_IN visible_JJ light_NN -RRB-_-RRB- is_VBZ used_VBN that_DT is_VBZ clearly_RB visible_JJ under_IN ultraviolet_JJ light_NN ._.
Envelopes_NNS may_MD then_RB be_VB processed_VBN with_IN equipment_NN based_VBN on_IN simple_JJ bar_NN code_NN readers_NNS ._.
Importance_NN of_IN OCR_NN to_TO the_DT Blind_NNP In_IN 1974_CD Ray_NNP Kurzweil_NNP started_VBD the_DT company_NN Kurzweil_NNP Computer_NNP Products_NNPS ,_, Inc._NNP and_CC continued_JJ development_NN of_IN omni-font_JJ OCR_NN ,_, which_WDT could_MD recognize_VB text_NN printed_VBN in_IN virtually_RB any_DT font_NN ._.
He_PRP decided_VBD that_IN the_DT best_JJS application_NN of_IN this_DT technology_NN would_MD be_VB to_TO create_VB a_DT reading_NN machine_NN for_IN the_DT blind_JJ ,_, which_WDT would_MD allow_VB blind_JJ people_NNS to_TO have_VB a_DT computer_NN read_NN text_NN to_TO them_PRP out_RP loud_RB ._.
This_DT device_NN required_VBD the_DT invention_NN of_IN two_CD enabling_VBG technologies_NNS --_: the_DT CCD_NNP flatbed_JJ scanner_NN and_CC the_DT text-to-speech_JJ synthesizer_NN ._.
On_IN January_NNP 13_CD ,_, 1976_CD the_DT successful_JJ finished_JJ product_NN was_VBD unveiled_VBN during_IN a_DT widely-reported_JJ news_NN conference_NN headed_VBN by_IN Kurzweil_NNP and_CC the_DT leaders_NNS of_IN the_DT National_NNP Federation_NNP of_IN the_DT Blind_NNP -LRB-_-LRB- citation_NN needed_VBN -RRB-_-RRB- ._.
In_IN 1978_CD Kurzweil_NNP Computer_NNP Products_NNPS began_VBD selling_VBG a_DT commercial_JJ version_NN of_IN the_DT optical_JJ character_NN recognition_NN computer_NN program_NN ._.
LexisNexis_NNP was_VBD one_CD of_IN the_DT first_JJ customers_NNS ,_, and_CC bought_VBD the_DT program_NN to_TO upload_VB paper_NN legal_JJ and_CC news_NN documents_NNS onto_IN its_PRP$ nascent_JJ online_NN databases_NNS ._.
Two_CD years_NNS later_RB ,_, Kurzweil_NNP sold_VBD his_PRP$ company_NN to_TO Xerox_NNP ,_, which_WDT had_VBD an_DT interest_NN in_IN further_JJ commercializing_VBG paper-to-computer_NN text_NN conversion_NN ._.
Xerox_NNP eventually_RB spun_VBD it_PRP off_RP as_IN Scansoft_NNP ,_, which_WDT merged_VBD with_IN Nuance_NNP Communications_NNPS -LRB-_-LRB- citation_NN needed_VBN -RRB-_-RRB- ._.
OCR_NN software_NN Desktop_NNP &_CC Server_NN OCR_NNP Software_NNP OCR_NNP software_NN and_CC ICR_NN software_NN technology_NN are_VBP analytical_JJ artificial_JJ intelligence_NN systems_NNS that_WDT consider_VBP sequences_NNS of_IN characters_NNS rather_RB than_IN whole_JJ words_NNS or_CC phrases_NNS ._.
Based_VBN on_IN the_DT analysis_NN of_IN sequential_JJ lines_NNS and_CC curves_NNS ,_, OCR_NN and_CC ICR_NN make_VBP `_`` best_JJS guesses_NNS '_POS at_IN characters_NNS using_VBG database_NN look-up_JJ tables_NNS to_TO closely_RB associate_VB or_CC match_VB the_DT strings_NNS of_IN characters_NNS that_WDT form_VBP words_NNS ._.
WebOCR_NNP &_CC OnlineOCR_NNP With_NNP IT_NNP technology_NN development_NN ,_, the_DT platform_NN for_IN people_NNS to_TO use_VB software_NN has_VBZ been_VBN changed_VBN from_IN single_JJ PC_NN platform_NN to_TO multi-platforms_NNS such_JJ as_IN PC_NN +_CC Web-based_JJ +_CC Cloud_NNP Computing_NNP +_CC Mobile_NNP devices_NNS ._.
After_IN 30_CD years_NNS development_NN ,_, OCR_NN software_NN started_VBD to_TO adapt_VB to_TO new_JJ application_NN requirements_NNS ._.
WebOCR_NN also_RB known_VBN as_IN OnlineOCR_NN or_CC Web-based_JJ OCR_NN service_NN ,_, has_VBZ been_VBN a_DT new_JJ trend_NN to_TO meet_VB larger_JJR volume_NN and_CC larger_JJR group_NN of_IN users_NNS after_IN 30_CD years_NNS development_NN of_IN the_DT desktop_NN OCR_NN ._.
Internet_NN and_CC broadband_NN technologies_NNS have_VBP made_VBN WebOCR_NN &_CC OnlineOCR_NN practically_RB available_JJ to_TO both_DT individual_JJ users_NNS and_CC enterprise_NN customers_NNS ._.
Since_IN 2000_CD ,_, some_DT major_JJ OCR_NN vendors_NNS began_VBD offering_VBG WebOCR_NNP &_CC Online_NNP software_NN ,_, a_DT number_NN of_IN new_JJ entrants_NNS companies_NNS to_TO seize_VB the_DT opportunity_NN to_TO develop_VB innovative_JJ Web-based_JJ OCR_NN service_NN ,_, some_DT of_IN which_WDT are_VBP free_JJ of_IN charge_NN services_NNS ._.
Application-Oriented_JJ OCR_NN Since_IN OCR_NN technology_NN has_VBZ been_VBN more_JJR and_CC more_RBR widely_RB applied_VBN to_TO paper-intensive_JJ industry_NN ,_, it_PRP is_VBZ facing_VBG more_RBR complex_JJ images_NNS environment_NN in_IN the_DT real_JJ world_NN ._.
For_IN example_NN :_: complicated_JJ backgrounds_NNS ,_, degraded-images_NNS ,_, heavy-noise_NN ,_, paper_NN skew_VB ,_, picture_NN distortion_NN ,_, low-resolution_NN ,_, disturbed_VBN by_IN grid_NN &_CC lines_NNS ,_, text_NN image_NN consisting_VBG of_IN special_JJ fonts_NNS ,_, symbols_NNS ,_, glossary_JJ words_NNS and_CC etc._NN ._.
All_PDT the_DT factors_NNS affect_VBP OCR_NN products_NNS '_POS stability_NN in_IN recognition_NN accuracy_NN ._.
In_IN recent_JJ years_NNS ,_, the_DT major_JJ OCR_NNP technology_NN providers_NNS began_VBD to_TO develop_VB dedicated_JJ OCR_NN systems_NNS ,_, each_DT for_IN special_JJ types_NNS of_IN images_NNS ._.
They_PRP combine_VBP various_JJ optimization_NN methods_NNS related_VBN to_TO the_DT special_JJ image_NN ,_, such_JJ as_IN business_NN rules_NNS ,_, standard_JJ expression_NN ,_, glossary_NN or_CC dictionary_NN and_CC rich_JJ information_NN contained_VBN in_IN color_NN images_NNS ,_, to_TO improve_VB the_DT recognition_NN accuracy_NN ._.
Such_JJ strategy_NN to_TO customize_VB OCR_NNP technology_NN is_VBZ called_VBN ``_`` Application-Oriented_JJ OCR_NN ''_'' or_CC ``_`` Customized_JJ OCR_NN ''_'' ,_, widely_RB used_VBN in_IN the_DT fields_NNS of_IN Business-card_JJ OCR_NNP ,_, Invoice_NNP OCR_NNP ,_, Screenshot_NNP OCR_NNP ,_, ID_NNP card_NN OCR_NN ,_, Driver-license_JJ OCR_NN or_CC Auto_NN plant_NN OCR_NN ,_, and_CC so_RB on_RB ._.
See_NNP also_RB :_: List_NN of_IN optical_JJ character_NN recognition_NN software_NN Current_JJ state_NN of_IN OCR_NNP technology_NN This_DT section_NN needs_VBZ additional_JJ citations_NNS for_IN verification_NN ._.
Please_VB help_NN improve_VB this_DT article_NN by_IN adding_VBG citations_NNS to_TO reliable_JJ sources_NNS ._.
Unsourced_JJ material_NN may_MD be_VB challenged_VBN and_CC removed_VBN ._.
-LRB-_-LRB- May_NNP 2009_CD -RRB-_-RRB- Commissioned_VBN by_IN the_DT U.S._NNP Department_NNP of_IN Energy_NNP -LRB-_-LRB- DOE_NNP -RRB-_-RRB- ,_, the_DT Information_NNP Science_NNP Research_NNP Institute_NNP -LRB-_-LRB- ISRI_NNP -RRB-_-RRB- had_VBD the_DT mission_NN to_TO foster_VB the_DT improvement_NN of_IN automated_JJ technologies_NNS for_IN understanding_NN machine_NN printed_VBN documents_NNS -LRB-_-LRB- citation_NN needed_VBN -RRB-_-RRB- ,_, and_CC it_PRP conducted_VBD the_DT most_RBS authoritative_JJ of_IN the_DT Annual_JJ Test_NN of_IN OCR_NNP Accuracy_NNP for_IN 5_CD consecutive_JJ years_NNS in_IN the_DT mid-90s_NNS ._.
Recognition_NN of_IN Latin-script_JJ ,_, typewritten_JJ text_NN is_VBZ still_RB not_RB 100_CD %_NN accurate_JJ even_RB where_WRB clear_JJ imaging_NN is_VBZ available_JJ ._.
One_CD study_NN based_VBN on_IN recognition_NN of_IN 19th_JJ -_: and_CC early_JJ 20th-century_JJ newspaper_NN pages_NNS concluded_VBD that_IN character-by-character_NN OCR_NN accuracy_NN for_IN commercial_JJ OCR_NN software_NN varied_VBD from_IN 71_CD %_NN to_TO 98_CD %_NN ;_: total_JJ accuracy_NN can_MD be_VB achieved_VBN only_RB by_IN human_JJ review_NN ._.
Other_JJ areas_NNS --_: including_VBG recognition_NN of_IN hand_NN printing_NN ,_, cursive_JJ handwriting_NN ,_, and_CC printed_VBD text_NN in_IN other_JJ scripts_NNS -LRB-_-LRB- especially_RB those_DT East_JJ Asian_JJ language_NN characters_NNS which_WDT have_VBP many_JJ strokes_NNS for_IN a_DT single_JJ character_NN -RRB-_-RRB- --_: are_VBP still_RB the_DT subject_NN of_IN active_JJ research_NN ._.
Accuracy_NN rates_NNS can_MD be_VB measured_VBN in_IN several_JJ ways_NNS ,_, and_CC how_WRB they_PRP are_VBP measured_VBN can_MD greatly_RB affect_VB the_DT reported_VBN accuracy_NN rate_NN ._.
For_IN example_NN ,_, if_IN word_NN context_NN -LRB-_-LRB- basically_RB a_DT lexicon_NN of_IN words_NNS -RRB-_-RRB- is_VBZ not_RB used_VBN to_TO correct_VB software_NN finding_VBG non-existent_JJ words_NNS ,_, a_DT character_NN error_NN rate_NN of_IN 1_CD %_NN -LRB-_-LRB- 99_CD %_NN accuracy_NN -RRB-_-RRB- may_MD result_VB in_IN an_DT error_NN rate_NN of_IN 5_CD %_NN -LRB-_-LRB- 95_CD %_NN accuracy_NN -RRB-_-RRB- or_CC worse_JJR if_IN the_DT measurement_NN is_VBZ based_VBN on_IN whether_IN each_DT whole_JJ word_NN was_VBD recognized_VBN with_IN no_DT incorrect_JJ letters_NNS ._.
On-line_JJ character_NN recognition_NN is_VBZ sometimes_RB confused_VBN with_IN Optical_NNP Character_NNP Recognition_NN -LRB-_-LRB- see_VB Handwriting_NN recognition_NN -RRB-_-RRB- ._.
OCR_NNP is_VBZ an_DT instance_NN of_IN off-line_JJ character_NN recognition_NN ,_, where_WRB the_DT system_NN recognizes_VBZ the_DT fixed_JJ static_JJ shape_NN of_IN the_DT character_NN ,_, while_IN on-line_JJ character_NN recognition_NN instead_RB recognizes_VBZ the_DT dynamic_JJ motion_NN during_IN handwriting_NN ._.
For_IN example_NN ,_, on-line_JJ recognition_NN ,_, such_JJ as_IN that_DT used_VBN for_IN gestures_NNS in_IN the_DT Penpoint_JJ OS_NN or_CC the_DT Tablet_NNP PC_NNP can_MD tell_VB whether_IN a_DT horizontal_JJ mark_NN was_VBD drawn_VBN right-to-left_JJ ,_, or_CC left-to-right_JJ ._.
On-line_JJ character_NN recognition_NN is_VBZ also_RB referred_VBN to_TO by_IN other_JJ terms_NNS such_JJ as_IN dynamic_JJ character_NN recognition_NN ,_, real-time_JJ character_NN recognition_NN ,_, and_CC Intelligent_JJ Character_NN Recognition_NN or_CC ICR_NN ._.
On-line_JJ systems_NNS for_IN recognizing_VBG hand-printed_JJ text_NN on_IN the_DT fly_NN have_VBP become_VBN well_RB known_VBN as_IN commercial_JJ products_NNS in_IN recent_JJ years_NNS -LRB-_-LRB- see_VB Tablet_NNP PC_NNP history_NN -RRB-_-RRB- ._.
Among_IN these_DT are_VBP the_DT input_NN devices_NNS for_IN personal_JJ digital_JJ assistants_NNS such_JJ as_IN those_DT running_VBG Palm_NNP OS_NN ._.
The_DT Apple_NNP Newton_NNP pioneered_VBD this_DT product_NN ._.
The_DT algorithms_NNS used_VBN in_IN these_DT devices_NNS take_VBP advantage_NN of_IN the_DT fact_NN that_IN the_DT order_NN ,_, speed_NN ,_, and_CC direction_NN of_IN individual_JJ lines_NNS segments_NNS at_IN input_NN are_VBP known_VBN ._.
Also_RB ,_, the_DT user_NN can_MD be_VB retrained_VBN to_TO use_VB only_RB specific_JJ letter_NN shapes_NNS ._.
These_DT methods_NNS can_MD not_RB be_VB used_VBN in_IN software_NN that_WDT scans_VBZ paper_NN documents_NNS ,_, so_RB accurate_JJ recognition_NN of_IN hand-printed_JJ documents_NNS is_VBZ still_RB largely_RB an_DT open_JJ problem_NN ._.
Accuracy_NN rates_NNS of_IN 80_CD %_NN to_TO 90_CD %_NN on_IN neat_JJ ,_, clean_JJ hand-printed_JJ characters_NNS can_MD be_VB achieved_VBN ,_, but_CC that_IN accuracy_NN rate_NN still_RB translates_VBZ to_TO dozens_NNS of_IN errors_NNS per_IN page_NN ,_, making_VBG the_DT technology_NN useful_JJ only_RB in_IN very_RB limited_JJ applications_NNS ._.
Recognition_NN of_IN cursive_JJ text_NN is_VBZ an_DT active_JJ area_NN of_IN research_NN ,_, with_IN recognition_NN rates_NNS even_RB lower_JJR than_IN that_DT of_IN hand-printed_JJ text_NN ._.
Higher_JJR rates_NNS of_IN recognition_NN of_IN general_JJ cursive_JJ script_NN will_MD likely_RB not_RB be_VB possible_JJ without_IN the_DT use_NN of_IN contextual_JJ or_CC grammatical_JJ information_NN ._.
For_IN example_NN ,_, recognizing_VBG entire_JJ words_NNS from_IN a_DT dictionary_NN is_VBZ easier_JJR than_IN trying_VBG to_TO parse_VB individual_JJ characters_NNS from_IN script_NN ._.
Reading_VBG the_DT Amount_NNP line_NN of_IN a_DT cheque_NN -LRB-_-LRB- which_WDT is_VBZ always_RB a_DT written-out_JJ number_NN -RRB-_-RRB- is_VBZ an_DT example_NN where_WRB using_VBG a_DT smaller_JJR dictionary_NN can_MD increase_VB recognition_NN rates_NNS greatly_RB ._.
Knowledge_NN of_IN the_DT grammar_NN of_IN the_DT language_NN being_VBG scanned_VBN can_MD also_RB help_VB determine_VB if_IN a_DT word_NN is_VBZ likely_JJ to_TO be_VB a_DT verb_NN or_CC a_DT noun_NN ,_, for_IN example_NN ,_, allowing_VBG greater_JJR accuracy_NN ._.
The_DT shapes_NNS of_IN individual_JJ cursive_JJ characters_NNS themselves_PRP simply_RB do_VBP not_RB contain_VB enough_JJ information_NN to_TO accurately_RB -LRB-_-LRB- greater_JJR than_IN 98_CD %_NN -RRB-_-RRB- recognize_VBP all_DT handwritten_JJ cursive_JJ script_NN ._.
It_PRP is_VBZ necessary_JJ to_TO understand_VB that_IN OCR_NNP technology_NN is_VBZ a_DT basic_JJ technology_NN also_RB used_VBN in_IN advanced_JJ scanning_NN applications_NNS ._.
Due_JJ to_TO this_DT ,_, an_DT advanced_JJ scanning_NN solution_NN can_MD be_VB unique_JJ and_CC patented_JJ and_CC not_RB easily_RB copied_VBN despite_IN being_VBG based_VBN on_IN this_DT basic_JJ OCR_NNP technology_NN ._.
For_IN more_RBR complex_JJ recognition_NN problems_NNS ,_, intelligent_JJ character_NN recognition_NN systems_NNS are_VBP generally_RB used_VBN ,_, as_IN artificial_JJ neural_JJ networks_NNS can_MD be_VB made_VBN indifferent_JJ to_TO both_CC affine_NN and_CC non-linear_JJ transformations_NNS ._.
A_DT technique_NN which_WDT is_VBZ having_VBG considerable_JJ success_NN in_IN recognizing_VBG difficult_JJ words_NNS and_CC character_NN groups_NNS within_IN documents_NNS generally_RB amenable_JJ to_TO computer_NN OCR_NN is_VBZ to_TO submit_VB them_PRP automatically_RB to_TO humans_NNS in_IN the_DT reCAPTCHA_NN system_NN ._.
In_IN corpus_NN linguistics_NNS ,_, part-of-speech_JJ tagging_NN -LRB-_-LRB- POS_NN tagging_NN or_CC POST_NN -RRB-_-RRB- ,_, also_RB called_VBD grammatical_JJ tagging_NN or_CC word-category_JJ disambiguation_NN ,_, is_VBZ the_DT process_NN of_IN marking_VBG up_RP a_DT word_NN in_IN a_DT text_NN -LRB-_-LRB- corpus_NN -RRB-_-RRB- as_IN corresponding_VBG to_TO a_DT particular_JJ part_NN of_IN speech_NN ,_, based_VBN on_IN both_DT its_PRP$ definition_NN ,_, as_RB well_RB as_IN its_PRP$ context_NN --_: i.e._FW relationship_NN with_IN adjacent_JJ and_CC related_JJ words_NNS in_IN a_DT phrase_NN ,_, sentence_NN ,_, or_CC paragraph_NN ._.
A_DT simplified_VBN form_NN of_IN this_DT is_VBZ commonly_RB taught_VBN to_TO school-age_JJ children_NNS ,_, in_IN the_DT identification_NN of_IN words_NNS as_IN nouns_NNS ,_, verbs_NNS ,_, adjectives_NNS ,_, adverbs_NNS ,_, etc._FW ._.
Once_RB performed_VBN by_IN hand_NN ,_, POS_NN tagging_NN is_VBZ now_RB done_VBN in_IN the_DT context_NN of_IN computational_JJ linguistics_NNS ,_, using_VBG algorithms_NNS which_WDT associate_VBP discrete_JJ terms_NNS ,_, as_RB well_RB as_IN hidden_JJ parts_NNS of_IN speech_NN ,_, in_IN accordance_NN with_IN a_DT set_NN of_IN descriptive_JJ tags_NNS ._.
POS-tagging_JJ algorithms_NNS fall_VBP into_IN two_CD distinctive_JJ groups_NNS :_: rule-based_JJ and_CC stochastic_JJ ._.
E._NNP Brill_NNP 's_POS tagger_NN ,_, one_CD of_IN the_DT first_JJ and_CC widely_RB used_VBN English_JJ POS-taggers_NNS ,_, employs_VBZ rule-based_JJ algorithms_NNS ._.
This_DT is_VBZ not_RB rare_JJ --_: in_IN natural_JJ languages_NNS -LRB-_-LRB- as_IN opposed_VBN to_TO many_JJ artificial_JJ languages_NNS -RRB-_-RRB- ,_, a_DT large_JJ percentage_NN of_IN word-forms_NNS are_VBP ambiguous_JJ ._.
For_IN example_NN ,_, even_RB ``_`` dogs_NNS ''_'' ,_, which_WDT is_VBZ usually_RB thought_VBN of_IN as_RB just_RB a_DT plural_NN noun_NN ,_, can_MD also_RB be_VB a_DT verb_NN :_: The_DT sailor_NN dogs_VBZ the_DT barmaid_NN ._.
Performing_VBG grammatical_JJ tagging_NN will_MD indicate_VB that_IN ``_`` dogs_NNS ''_'' is_VBZ a_DT verb_NN ,_, and_CC not_RB the_DT more_RBR common_JJ plural_NN noun_NN ,_, since_IN one_CD of_IN the_DT words_NNS must_MD be_VB the_DT main_JJ verb_NN ,_, and_CC the_DT noun_NN reading_NN is_VBZ less_RBR likely_JJ following_VBG ``_`` sailor_NN ''_'' -LRB-_-LRB- sailor_NN !_.
→_CD dogs_NNS -RRB-_-RRB- ._.
Semantic_JJ analysis_NN can_MD then_RB extrapolate_VB that_IN ``_`` sailor_NN ''_'' and_CC ``_`` barmaid_NN ''_'' implicate_VB ``_`` dogs_NNS ''_'' as_IN 1_CD -RRB-_-RRB- in_IN the_DT nautical_JJ context_NN -LRB-_-LRB- sailor_NN →_FW <verb>_FW ←_FW barmaid_NN -RRB-_-RRB- and_CC 2_LS -RRB-_-RRB- an_DT action_NN applied_VBN to_TO the_DT object_NN ``_`` barmaid_NN ''_'' -LRB-_-LRB- -LRB-_-LRB- subject_NN -RRB-_-RRB- dogs_VBZ →_CD barmaid_NN -RRB-_-RRB- ._.
In_IN this_DT context_NN ,_, ``_`` dogs_NNS ''_'' is_VBZ a_DT nautical_JJ term_NN meaning_NN ``_`` fastens_NNS -LRB-_-LRB- a_DT watertight_JJ barmaid_NN -RRB-_-RRB- securely_RB ;_: applies_VBZ a_DT dog_NN to_TO ''_'' ._.
``_`` Dogged_VBN ''_'' ,_, on_IN the_DT other_JJ hand_NN ,_, can_MD be_VB either_CC an_DT adjective_NN or_CC a_DT past-tense_JJ verb_NN ._.
Just_RB which_WDT parts_NNS of_IN speech_NN a_DT word_NN can_MD represent_VB varies_VBZ greatly_RB ._.
Trained_VBN linguists_NNS can_MD identify_VB the_DT grammatical_JJ parts_NNS of_IN speech_NN to_TO various_JJ fine_JJ degrees_NNS depending_VBG on_IN the_DT tagging_NN system_NN ._.
Schools_NNS commonly_RB teach_VBP that_IN there_EX are_VBP 9_CD parts_NNS of_IN speech_NN in_IN English_NNP :_: noun_NN ,_, verb_NN ,_, article_NN ,_, adjective_NN ,_, preposition_NN ,_, pronoun_NN ,_, adverb_NN ,_, conjunction_NN ,_, and_CC interjection_NN ._.
However_RB ,_, there_EX are_VBP clearly_RB many_JJ more_JJR categories_NNS and_CC sub-categories_NNS ._.
For_IN nouns_NNS ,_, plural_NN ,_, possessive_JJ ,_, and_CC singular_JJ forms_NNS can_MD be_VB distinguished_VBN ._.
In_IN many_JJ languages_NNS words_NNS are_VBP also_RB marked_JJ for_IN their_PRP$ ``_`` case_NN ''_'' -LRB-_-LRB- role_NN as_IN subject_NN ,_, object_NN ,_, etc._NN -RRB-_-RRB- ,_, grammatical_JJ gender_NN ,_, and_CC so_RB on_IN ;_: while_IN verbs_NNS are_VBP marked_JJ for_IN tense_JJ ,_, aspect_NN ,_, and_CC other_JJ things_NNS ._.
In_IN part-of-speech_JJ tagging_NN by_IN computer_NN ,_, it_PRP is_VBZ typical_JJ to_TO distinguish_VB from_IN 50_CD to_TO 150_CD separate_JJ parts_NNS of_IN speech_NN for_IN English_NNP ,_, for_IN example_NN ,_, NN_NNP for_IN singular_JJ common_JJ nouns_NNS ,_, NNS_NN for_IN plural_NN common_JJ nouns_NNS ,_, NP_NN for_IN singular_JJ proper_JJ nouns_NNS -LRB-_-LRB- see_VB the_DT POS_NN tags_NNS used_VBN in_IN the_DT Brown_NNP Corpus_NNP -RRB-_-RRB- ._.
Work_NN on_IN stochastic_JJ methods_NNS for_IN tagging_VBG Koine_JJ Greek_JJ -LRB-_-LRB- DeRose_NN 1990_CD -RRB-_-RRB- has_VBZ used_VBN over_IN 1,000_CD parts_NNS of_IN speech_NN ,_, and_CC found_VBD that_IN about_IN as_RB many_JJ words_NNS were_VBD ambiguous_JJ there_RB as_IN in_IN English_NNP ._.
A_DT morphosyntactic_JJ descriptor_NN in_IN the_DT case_NN of_IN morphologically_RB rich_JJ languages_NNS can_MD be_VB expressed_VBN like_IN Ncmsan_NNP ,_, which_WDT means_VBZ Category_NN =_JJ Noun_NN ,_, Type_NN =_JJ common_JJ ,_, Gender_NN =_JJ masculine_JJ ,_, Number_NN =_JJ singular_JJ ,_, Case_NN =_JJ accusative_JJ ,_, Animate_JJ =_JJ no._NN ._.
History_NN The_DT Brown_NNP Corpus_NNP Research_NNP on_IN part-of-speech_JJ tagging_NN has_VBZ been_VBN closely_RB tied_VBN to_TO corpus_NN linguistics_NNS ._.
The_DT first_JJ major_JJ corpus_NN of_IN English_NNP for_IN computer_NN analysis_NN was_VBD the_DT Brown_NNP Corpus_NNP developed_VBD at_IN Brown_NNP University_NNP by_IN Henry_NNP Kucera_NNP and_CC Nelson_NNP Francis_NNP ,_, in_IN the_DT mid-1960s_NNS ._.
It_PRP consists_VBZ of_IN about_IN 1,000,000_CD words_NNS of_IN running_VBG English_JJ prose_NN text_NN ,_, made_VBD up_RP of_IN 500_CD samples_NNS from_IN randomly_RB chosen_VBN publications_NNS ._.
Each_DT sample_NN is_VBZ 2,000_CD or_CC more_JJR words_NNS -LRB-_-LRB- ending_VBG at_IN the_DT first_JJ sentence-end_NN after_IN 2,000_CD words_NNS ,_, so_IN that_IN the_DT corpus_NN contains_VBZ only_RB complete_JJ sentences_NNS -RRB-_-RRB- ._.
The_DT Brown_NNP Corpus_NNP was_VBD painstakingly_RB ``_`` tagged_VBN ''_'' with_IN part-of-speech_JJ markers_NNS over_IN many_JJ years_NNS ._.
A_DT first_JJ approximation_NN was_VBD done_VBN with_IN a_DT program_NN by_IN Greene_NNP and_CC Rubin_NNP ,_, which_WDT consisted_VBD of_IN a_DT huge_JJ handmade_JJ list_NN of_IN what_WP categories_NNS could_MD co-occur_VB at_IN all_DT ._.
For_IN example_NN ,_, article_NN then_RB noun_NN can_MD occur_VB ,_, but_CC article_NN verb_NN -LRB-_-LRB- arguably_RB -RRB-_-RRB- can_MD not_RB ._.
The_DT program_NN got_VBD about_RB 70_CD %_NN correct_JJ ._.
Its_PRP$ results_NNS were_VBD repeatedly_RB reviewed_VBN and_CC corrected_VBN by_IN hand_NN ,_, and_CC later_JJ users_NNS sent_VBN in_IN errata_NNS ,_, so_IN that_IN by_IN the_DT late_JJ 70s_NNS the_DT tagging_NN was_VBD nearly_RB perfect_JJ -LRB-_-LRB- allowing_VBG for_IN some_DT cases_NNS on_IN which_WDT even_RB human_JJ speakers_NNS might_MD not_RB agree_VB -RRB-_-RRB- ._.
This_DT corpus_NN has_VBZ been_VBN used_VBN for_IN innumerable_JJ studies_NNS of_IN word-frequency_NN and_CC of_IN part-of-speech_NN ,_, and_CC inspired_VBD the_DT development_NN of_IN similar_JJ ``_`` tagged_VBN ''_'' corpora_NN in_IN many_JJ other_JJ languages_NNS ._.
Statistics_NNS derived_VBN by_IN analyzing_VBG it_PRP formed_VBD the_DT basis_NN for_IN most_JJS later_JJ part-of-speech_JJ tagging_NN systems_NNS ,_, such_JJ as_IN CLAWS_NNS -LRB-_-LRB- linguistics_NNS -RRB-_-RRB- and_CC VOLSUNGA_NN ._.
However_RB ,_, by_IN this_DT time_NN -LRB-_-LRB- 2005_CD -RRB-_-RRB- it_PRP has_VBZ been_VBN superseded_VBN by_IN larger_JJR corpora_NNS such_JJ as_IN the_DT 100_CD million_CD word_NN British_NNP National_NNP Corpus_NNP ._.
For_IN some_DT time_NN ,_, part-of-speech_JJ tagging_NN was_VBD considered_VBN an_DT inseparable_JJ part_NN of_IN natural_JJ language_NN processing_NN ,_, because_IN there_EX are_VBP certain_JJ cases_NNS where_WRB the_DT correct_JJ part_NN of_IN speech_NN can_MD not_RB be_VB decided_VBN without_IN understanding_VBG the_DT semantics_NNS or_CC even_RB the_DT pragmatics_NNS of_IN the_DT context_NN ._.
This_DT is_VBZ extremely_RB expensive_JJ ,_, especially_RB because_IN analyzing_VBG the_DT higher_JJR levels_NNS is_VBZ much_RB harder_JJR when_WRB multiple_JJ part-of-speech_JJ possibilities_NNS must_MD be_VB considered_VBN for_IN each_DT word_NN ._.
Use_NN of_IN Hidden_NNP Markov_NNP Models_NNS In_IN the_DT mid_JJ 1980s_NNS ,_, researchers_NNS in_IN Europe_NNP began_VBD to_TO use_VB hidden_JJ Markov_NNP models_NNS -LRB-_-LRB- HMMs_NNS -RRB-_-RRB- to_TO disambiguate_VB parts_NNS of_IN speech_NN ,_, when_WRB working_VBG to_TO tag_VB the_DT Lancaster-Oslo-Bergen_NNP Corpus_NNP of_IN British_NNP English_NNP ._.
HMMs_NNS involve_VBP counting_NN cases_NNS -LRB-_-LRB- such_JJ as_IN from_IN the_DT Brown_NNP Corpus_NNP -RRB-_-RRB- ,_, and_CC making_VBG a_DT table_NN of_IN the_DT probabilities_NNS of_IN certain_JJ sequences_NNS ._.
For_IN example_NN ,_, once_RB you_PRP 've_VBP seen_VBN an_DT article_NN such_JJ as_IN `_`` the_DT '_'' ,_, perhaps_RB the_DT next_JJ word_NN is_VBZ a_DT noun_NN 40_CD %_NN of_IN the_DT time_NN ,_, an_DT adjective_JJ 40_CD %_NN ,_, and_CC a_DT number_NN 20_CD %_NN ._.
Knowing_VBG this_DT ,_, a_DT program_NN can_MD decide_VB that_IN ``_`` can_MD ''_'' in_IN ``_`` the_DT can_NN ''_'' is_VBZ far_RB more_RBR likely_JJ to_TO be_VB a_DT noun_NN than_IN a_DT verb_NN or_CC a_DT modal_NN ._.
The_DT same_JJ method_NN can_NN of_IN course_NN be_VB used_VBN to_TO benefit_VB from_IN knowledge_NN about_IN following_VBG words_NNS ._.
More_RBR advanced_JJ -LRB-_-LRB- ``_`` higher_JJR order_NN ''_'' -RRB-_-RRB- HMMs_NNS learn_VBP the_DT probabilities_NNS not_RB only_RB of_IN pairs_NNS ,_, but_CC triples_NNS or_CC even_RB larger_JJR sequences_NNS ._.
So_RB ,_, for_IN example_NN ,_, if_IN you_PRP 've_VBP just_RB seen_VBN an_DT article_NN and_CC a_DT verb_NN ,_, the_DT next_JJ item_NN may_MD be_VB very_RB likely_RB a_DT preposition_NN ,_, article_NN ,_, or_CC noun_NN ,_, but_CC much_RB less_RBR likely_JJ another_DT verb_NN ._.
When_WRB several_JJ ambiguous_JJ words_NNS occur_VBP together_RB ,_, the_DT possibilities_NNS multiply_VBP ._.
However_RB ,_, it_PRP is_VBZ easy_JJ to_TO enumerate_VB every_DT combination_NN and_CC to_TO assign_VB a_DT relative_JJ probability_NN to_TO each_DT one_CD ,_, by_IN multiplying_VBG together_RB the_DT probabilities_NNS of_IN each_DT choice_NN in_IN turn_NN ._.
The_DT combination_NN with_IN highest_JJS probability_NN is_VBZ then_RB chosen_VBN ._.
The_DT European_JJ group_NN developed_VBD CLAWS_NNP ,_, a_DT tagging_NN program_NN that_WDT did_VBD exactly_RB this_DT ,_, and_CC achieved_VBD accuracy_NN in_IN the_DT 93-95_CD %_NN range_NN ._.
It_PRP is_VBZ worth_JJ remembering_VBG ,_, as_IN Eugene_NNP Charniak_NNP points_VBZ out_RP in_IN Statistical_JJ techniques_NNS for_IN natural_JJ language_NN parsing_NN ,_, that_IN merely_RB assigning_VBG the_DT most_RBS common_JJ tag_NN to_TO each_DT known_JJ word_NN and_CC the_DT tag_NN ``_`` proper_JJ noun_NN ''_'' to_TO all_DT unknowns_NNS ,_, will_MD approach_VB 90_CD %_NN accuracy_NN because_IN many_JJ words_NNS are_VBP unambiguous_JJ ._.
CLAWS_NNS pioneered_VBD the_DT field_NN of_IN HMM-based_JJ part_NN of_IN speech_NN tagging_NN ,_, but_CC was_VBD quite_RB expensive_JJ since_IN it_PRP enumerated_VBD all_DT possibilities_NNS ._.
It_PRP sometimes_RB had_VBD to_TO resort_VB to_TO backup_JJ methods_NNS when_WRB there_EX were_VBD simply_RB too_RB many_JJ -LRB-_-LRB- the_DT Brown_NNP Corpus_NNP contains_VBZ a_DT case_NN with_IN 17_CD ambiguous_JJ words_NNS in_IN a_DT row_NN ,_, and_CC there_EX are_VBP words_NNS such_JJ as_IN ``_`` still_RB ''_'' that_WDT can_MD represent_VB as_RB many_JJ as_IN 7_CD distinct_JJ parts_NNS of_IN speech_NN -RRB-_-RRB- ._.
HMMs_NNS underlie_VBP the_DT functioning_NN of_IN stochastic_JJ taggers_NNS and_CC are_VBP used_VBN in_IN various_JJ algorithms_NNS one_CD of_IN the_DT most_RBS widely_RB used_VBN being_VBG the_DT bi-directional_JJ inference_NN algorithm_NN ._.
Dynamic_NNP Programming_NNP methods_NNS In_IN 1987_CD ,_, Steven_NNP DeRose_NNP and_CC Ken_NNP Church_NNP independently_RB developed_VBD dynamic_JJ programming_NN algorithms_NNS to_TO solve_VB the_DT same_JJ problem_NN in_IN vastly_RB less_JJR time_NN ._.
Their_PRP$ methods_NNS were_VBD similar_JJ to_TO the_DT Viterbi_NNP algorithm_NN known_VBN for_IN some_DT time_NN in_IN other_JJ fields_NNS ._.
DeRose_NNP used_VBD a_DT table_NN of_IN pairs_NNS ,_, while_IN Church_NNP used_VBD a_DT table_NN of_IN triples_NNS and_CC a_DT method_NN of_IN estimating_VBG the_DT values_NNS for_IN triples_NNS that_WDT were_VBD rare_JJ or_CC nonexistent_JJ in_IN the_DT Brown_NNP Corpus_NNP -LRB-_-LRB- actual_JJ measurement_NN of_IN triple_JJ probabilities_NNS would_MD require_VB a_DT much_RB larger_JJR corpus_NN -RRB-_-RRB- ._.
Both_DT methods_NNS achieved_VBD accuracy_NN over_IN 95_CD %_NN ._.
DeRose_NNP 's_POS 1990_CD dissertation_NN at_IN Brown_NNP University_NNP included_VBD analyses_NNS of_IN the_DT specific_JJ error_NN types_NNS ,_, probabilities_NNS ,_, and_CC other_JJ related_JJ data_NNS ,_, and_CC replicated_VBD his_PRP$ work_NN for_IN Greek_JJ ,_, where_WRB it_PRP proved_VBD similarly_RB effective_JJ ._.
These_DT findings_NNS were_VBD surprisingly_RB disruptive_JJ to_TO the_DT field_NN of_IN natural_JJ language_NN processing_NN ._.
The_DT accuracy_NN reported_VBD was_VBD higher_JJR than_IN the_DT typical_JJ accuracy_NN of_IN very_RB sophisticated_JJ algorithms_NNS that_WDT integrated_VBD part_NN of_IN speech_NN choice_NN with_IN many_JJ higher_JJR levels_NNS of_IN linguistic_JJ analysis_NN :_: syntax_NN ,_, morphology_NN ,_, semantics_NNS ,_, and_CC so_RB on_RB ._.
CLAWS_NNP ,_, DeRose_NNP 's_POS and_CC Church_NNP 's_POS methods_NNS did_VBD fail_VB for_IN some_DT of_IN the_DT known_JJ cases_NNS where_WRB semantics_NNS is_VBZ required_VBN ,_, but_CC those_DT proved_VBD negligibly_RB rare_JJ ._.
This_DT convinced_VBD many_JJ in_IN the_DT field_NN that_IN part-of-speech_JJ tagging_NN could_MD usefully_RB be_VB separated_VBN out_RP from_IN the_DT other_JJ levels_NNS of_IN processing_NN ;_: this_DT in_IN turn_NN simplified_VBD the_DT theory_NN and_CC practice_NN of_IN computerized_JJ language_NN analysis_NN ,_, and_CC encouraged_JJ researchers_NNS to_TO find_VB ways_NNS to_TO separate_VB out_RP other_JJ pieces_NNS as_RB well_RB ._.
Markov_NNP Models_NNS are_VBP now_RB the_DT standard_JJ method_NN for_IN part-of-speech_JJ assignment_NN ._.
Unsupervised_JJ taggers_NNS The_DT methods_NNS already_RB discussed_VBN involve_VBP working_VBG from_IN a_DT pre-existing_JJ corpus_NN to_TO learn_VB tag_NN probabilities_NNS ._.
It_PRP is_VBZ ,_, however_RB ,_, also_RB possible_JJ to_TO bootstrap_VB using_VBG ``_`` unsupervised_JJ ''_'' tagging_NN ._.
Unsupervised_JJ tagging_NN techniques_NNS use_VBP an_DT untagged_JJ corpus_NN for_IN their_PRP$ training_NN data_NNS and_CC produce_VBP the_DT tagset_NN by_IN induction_NN ._.
That_DT is_VBZ ,_, they_PRP observe_VBP patterns_NNS in_IN word_NN use_NN ,_, and_CC derive_VBP part-of-speech_JJ categories_NNS themselves_PRP ._.
For_IN example_NN ,_, statistics_NNS readily_RB reveal_VBP that_IN ``_`` the_DT ''_'' ,_, ``_`` a_DT ''_'' ,_, and_CC ``_`` an_DT ''_'' occur_VBP in_IN similar_JJ contexts_NNS ,_, while_IN ``_`` eat_VB ''_'' occurs_VBZ in_IN very_RB different_JJ ones_NNS ._.
With_IN sufficient_JJ iteration_NN ,_, similarity_NN classes_NNS of_IN words_NNS emerge_VBP that_WDT are_VBP remarkably_RB similar_JJ to_TO those_DT human_JJ linguists_NNS would_MD expect_VB ;_: and_CC the_DT differences_NNS themselves_PRP sometimes_RB suggest_VBP valuable_JJ new_JJ insights_NNS ._.
These_DT two_CD categories_NNS can_MD be_VB further_RB subdivided_VBN into_IN rule-based_JJ ,_, stochastic_JJ ,_, and_CC neural_JJ approaches_NNS ._.
Other_JJ taggers_NNS and_CC methods_NNS Some_DT current_JJ major_JJ algorithms_NNS for_IN part-of-speech_JJ tagging_NN include_VBP the_DT Viterbi_NNP algorithm_NN ,_, Brill_NNP Tagger_NNP ,_, Constraint_NNP Grammar_NNP ,_, and_CC the_DT Baum-Welch_JJ algorithm_NN -LRB-_-LRB- also_RB known_VBN as_IN the_DT forward-backward_JJ algorithm_NN -RRB-_-RRB- ._.
Hidden_NNP Markov_NNP model_NN and_CC visible_JJ Markov_NNP model_NN taggers_NNS can_MD both_DT be_VB implemented_VBN using_VBG the_DT Viterbi_NNP algorithm_NN ._.
The_DT Brill_NNP tagger_NN is_VBZ unusual_JJ in_IN that_IN it_PRP learns_VBZ a_DT set_NN of_IN patterns_NNS ,_, and_CC then_RB applies_VBZ those_DT patterns_NNS rather_RB than_IN optimizing_VBG a_DT statistical_JJ quantity_NN ._.
Many_JJ machine_NN learning_NN methods_NNS have_VBP also_RB been_VBN applied_VBN to_TO the_DT problem_NN of_IN POS_NN tagging_NN ._.
Methods_NNS such_JJ as_IN SVM_NNP ,_, Maximum_NNP entropy_NN classifier_NN ,_, Perceptron_NNP ,_, and_CC Nearest-neighbor_NN have_VBP all_DT been_VBN tried_VBN ,_, and_CC most_JJS can_MD achieve_VB accuracy_NN above_IN 95_CD %_NN ._.
A_DT direct_JJ comparison_NN of_IN several_JJ methods_NNS is_VBZ reported_VBN -LRB-_-LRB- with_IN references_NNS -RRB-_-RRB- at_IN ._.
This_DT comparison_NN uses_VBZ the_DT Penn_NNP tag_NN set_VBN on_IN some_DT of_IN the_DT Penn_NNP Treebank_NNP data_NNS ,_, so_IN the_DT results_NNS are_VBP directly_RB comparable_JJ ._.
However_RB ,_, many_JJ significant_JJ taggers_NNS are_VBP not_RB included_VBN -LRB-_-LRB- perhaps_RB because_IN of_IN the_DT labor_NN involved_VBN in_IN reconfiguring_VBG them_PRP for_IN this_DT particular_JJ dataset_NN -RRB-_-RRB- ._.
Thus_RB ,_, it_PRP should_MD not_RB be_VB assumed_VBN that_IN the_DT results_NNS reported_VBD there_EX are_VBP the_DT best_JJS that_WDT can_MD be_VB achieved_VBN with_IN a_DT given_VBN approach_NN ;_: nor_CC even_RB the_DT best_JJS that_WDT have_VBP been_VBN achieved_VBN with_IN a_DT given_VBN approach_NN ._.
Issues_NNS While_IN there_EX is_VBZ broad_JJ agreement_NN about_IN basic_JJ categories_NNS ,_, a_DT number_NN of_IN edge_NN cases_NNS make_VBP it_PRP difficult_JJ to_TO settle_VB on_IN a_DT single_JJ ``_`` correct_JJ ''_'' set_NN of_IN tags_NNS ,_, even_RB in_IN a_DT single_JJ language_NN such_JJ as_IN English_NNP ._.
For_IN example_NN ,_, it_PRP is_VBZ hard_JJ to_TO say_VB whether_IN ``_`` fire_NN ''_'' is_VBZ functioning_VBG as_IN an_DT adjective_NN or_CC a_DT noun_NN in_IN the_DT big_JJ green_JJ fire_NN truck_NN A_NN second_JJ important_JJ example_NN is_VBZ the_DT use\/mention_NN distinction_NN ,_, as_IN in_IN the_DT following_VBG example_NN ,_, where_WRB ``_`` blue_JJ ''_'' is_VBZ clearly_RB not_RB functioning_VBG as_IN an_DT adjective_JJ -LRB-_-LRB- the_DT Brown_NNP Corpus_NNP tag_NN set_NN appends_VBZ the_DT suffix_NN ''_'' -_: NC_NN ''_'' in_IN such_JJ cases_NNS -RRB-_-RRB- :_: the_DT word_NN ``_`` blue_JJ ''_'' has_VBZ 4_CD letters_NNS ._.
Words_NNS in_IN a_DT language_NN other_JJ than_IN that_DT of_IN the_DT ``_`` main_JJ ''_'' text_NN ,_, are_VBP commonly_RB tagged_VBN as_IN ``_`` foreign_JJ ''_'' ,_, usually_RB in_IN addition_NN to_TO a_DT tag_NN for_IN the_DT role_NN the_DT foreign_JJ word_NN is_VBZ actually_RB playing_VBG in_IN context_NN ._.
There_EX are_VBP also_RB many_JJ cases_NNS where_WRB POS_NN categories_NNS and_CC ``_`` words_NNS ''_'' do_VBP not_RB map_VB one_CD to_TO one_CD ,_, for_IN example_NN :_: David_NNP 's_POS gonna_NNS do_VBP n't_RB vice_RB versa_RB first-cut_JJ can_MD not_RB pre_JJ -_: and_CC post-secondary_JJ look_NN -LRB-_-LRB- a_DT word_NN -RRB-_-RRB- up_IN In_IN the_DT last_JJ example_NN ,_, ``_`` look_NN ''_'' and_CC ``_`` up_RP ''_'' arguably_RB function_VBP as_IN a_DT single_JJ verbal_JJ unit_NN ,_, despite_IN the_DT possibility_NN of_IN other_JJ words_NNS coming_VBG between_IN them_PRP ._.
Some_DT tag_NN sets_NNS -LRB-_-LRB- such_JJ as_IN Penn_NNP -RRB-_-RRB- break_VBP hyphenated_JJ words_NNS ,_, contractions_NNS ,_, and_CC possessives_NNS into_IN separate_JJ tokens_VBN ,_, thus_RB avoiding_VBG some_DT but_CC far_RB from_IN all_DT such_JJ problems_NNS ._.
It_PRP is_VBZ unclear_JJ whether_IN it_PRP is_VBZ best_JJS to_TO treat_VB words_NNS such_JJ as_IN ``_`` be_VB ''_'' ,_, ``_`` have_VBP ''_'' ,_, and_CC ``_`` do_VBP ''_'' as_IN categories_NNS in_IN their_PRP$ own_JJ right_NN -LRB-_-LRB- as_IN in_IN the_DT Brown_NNP Corpus_NNP -RRB-_-RRB- ,_, or_CC as_IN simply_JJ verbs_NNS -LRB-_-LRB- as_IN in_IN the_DT LOB_NNP Corpus_NNP and_CC the_DT Penn_NNP Treebank_NNP -RRB-_-RRB- ._.
``_`` be_VB ''_'' has_VBZ more_JJR forms_NNS than_IN other_JJ English_JJ verbs_NNS ,_, and_CC occurs_VBZ in_IN quite_RB different_JJ grammatical_JJ contexts_NNS ,_, complicating_VBG the_DT issue_NN ._.
The_DT most_RBS popular_JJ ``_`` tag_NN set_NN ''_'' for_IN POS_NNP tagging_VBG for_IN American_NNP English_NNP is_VBZ probably_RB the_DT Penn_NNP tag_NN set_NN ,_, developed_VBN in_IN the_DT Penn_NNP Treebank_NNP project_NN ._.
It_PRP is_VBZ largely_RB similar_JJ to_TO the_DT earlier_JJR Brown_NNP Corpus_NNP and_CC LOB_NNP Corpus_NNP tag_NN sets_NNS ,_, though_RB much_RB smaller_JJR ._.
In_IN Europe_NNP ,_, tag_NN sets_NNS from_IN the_DT Eagles_NNPS Guidelines_NNS see_VBP wide_JJ use_NN ,_, and_CC include_VBP versions_NNS for_IN multiple_JJ languages_NNS ._.
POS_NN tagging_NN work_NN has_VBZ been_VBN done_VBN in_IN a_DT variety_NN of_IN languages_NNS ,_, and_CC the_DT set_NN of_IN POS_NN tags_NNS used_VBN varies_VBZ greatly_RB with_IN language_NN ._.
Tags_NNS usually_RB are_VBP designed_VBN to_TO include_VB overt_JJ morphological_JJ distinctions_NNS -LRB-_-LRB- this_DT makes_VBZ the_DT tag_NN sets_NNS for_IN heavily_RB inflected_VBN languages_NNS such_JJ as_IN Greek_JJ and_CC Latin_NNP very_RB large_JJ ;_: and_CC makes_VBZ tagging_VBG words_NNS in_IN agglutinative_JJ languages_NNS such_PDT an_DT Inuit_NNP virtually_RB impossible_JJ ._.
However_RB ,_, Petrov_NNP ,_, D._NNP Das_NNP ,_, and_CC R._NNP McDonald_NNP -LRB-_-LRB- ``_`` A_NNP Universal_NNP Part-of-Speech_NNP Tagset_NNP ''_'' http:\/\/arxiv.org\/abs\/1104.2086_NN -RRB-_-RRB- have_VBP proposed_VBN a_DT ``_`` universal_JJ ''_'' tag_NN set_NN ,_, with_IN 12_CD categories_NNS -LRB-_-LRB- for_IN example_NN ,_, no_DT subtypes_NNS of_IN nouns_NNS ,_, verbs_NNS ,_, punctuation_NN ,_, etc._NN ;_: no_DT distinction_NN of_IN ``_`` to_TO ''_'' as_IN an_DT infinitive_JJ marker_NN vs._NN preposition_NN ,_, etc._NN -RRB-_-RRB- ._.
Whether_IN a_DT very_RB small_JJ set_NN of_IN very_RB broad_JJ tags_NNS ,_, or_CC a_DT much_RB larger_JJR set_NN of_IN more_JJR precise_JJ ones_NNS ,_, is_VBZ preferable_JJ ,_, depends_VBZ on_IN the_DT purpose_NN at_IN hand_NN ._.
Automatic_NNP tagging_NN is_VBZ easier_JJR on_IN smaller_JJR tag-sets_NNS ._.
A_DT different_JJ issue_NN is_VBZ that_IN some_DT cases_NNS are_VBP in_IN fact_NN ambiguous_JJ ._.
Beatrice_NNP Santorini_NNP gives_VBZ examples_NNS in_IN ``_`` Part-of-speech_JJ Tagging_NN Guidelines_NNS for_IN the_DT Penn_NNP Treebank_NNP Project_NNP ,_, ''_'' -LRB-_-LRB- 3rd_JJ rev_NN ,_, June_NNP 1990_CD -RRB-_-RRB- ,_, including_VBG the_DT following_JJ -LRB-_-LRB- p._NN 32_CD -RRB-_-RRB- case_NN in_IN which_WDT entertaining_JJ can_NN function_NN either_CC as_IN an_DT adjective_NN or_CC a_DT verb_NN ,_, and_CC there_EX is_VBZ no_DT evident_JJ way_NN to_TO decide_VB :_: The_DT Duchess_NNP was_VBD entertaining_VBG last_JJ night_NN ._.
In_IN computer_NN science_NN and_CC linguistics_NNS ,_, parsing_NN ,_, or_CC ,_, more_RBR formally_RB ,_, syntactic_JJ analysis_NN ,_, is_VBZ the_DT process_NN of_IN analyzing_VBG a_DT text_NN ,_, made_VBN of_IN a_DT sequence_NN of_IN tokens_VBN -LRB-_-LRB- for_IN example_NN ,_, words_NNS -RRB-_-RRB- ,_, to_TO determine_VB its_PRP$ grammatical_JJ structure_NN with_IN respect_NN to_TO a_DT given_JJ -LRB-_-LRB- more_JJR or_CC less_JJR -RRB-_-RRB- formal_JJ grammar_NN ._.
Parsing_NN can_MD also_RB be_VB used_VBN as_IN a_DT linguistic_JJ term_NN ,_, for_IN instance_NN when_WRB discussing_VBG how_WRB phrases_NNS are_VBP divided_VBN up_RP in_IN garden_NN path_NN sentences_NNS ._.
Parsing_NN is_VBZ also_RB an_DT earlier_JJR term_NN for_IN the_DT diagramming_VBG of_IN sentences_NNS of_IN natural_JJ languages_NNS ,_, and_CC is_VBZ still_RB used_VBN for_IN the_DT diagramming_VBG of_IN inflected_JJ languages_NNS ,_, such_JJ as_IN the_DT Romance_NN languages_NNS or_CC Latin_NNP ._.
The_DT term_NN parsing_NN comes_VBZ from_IN Latin_JJ pars_NNS -LRB-_-LRB- ōrātiōnis_NNS -RRB-_-RRB- ,_, meaning_NN part_NN -LRB-_-LRB- of_IN speech_NN -RRB-_-RRB- ._.
Parsing_VBG is_VBZ a_DT common_JJ term_NN used_VBN in_IN psycholinguistics_NNS when_WRB describing_VBG language_NN comprehension_NN ._.
In_IN this_DT context_NN ,_, parsing_NN refers_VBZ to_TO the_DT way_NN that_IN human_JJ beings_NNS ,_, rather_RB than_IN computers_NNS ,_, analyze_VBP a_DT sentence_NN or_CC phrase_NN -LRB-_-LRB- in_IN spoken_VBN language_NN or_CC text_NN -RRB-_-RRB- ``_`` in_IN terms_NNS of_IN grammatical_JJ constituents_NNS ,_, identifying_VBG the_DT parts_NNS of_IN speech_NN ,_, syntactic_JJ relations_NNS ,_, etc._FW ''_'' This_DT term_NN is_VBZ especially_RB common_JJ when_WRB discussing_VBG what_WP linguistic_JJ cues_NNS help_VBP speakers_NNS to_TO parse_VB garden-path_NN sentences_NNS ._.
The_DT parser_NN often_RB uses_VBZ a_DT separate_JJ lexical_JJ analyser_NN to_TO create_VB tokens_VBN from_IN the_DT sequence_NN of_IN input_NN characters_NNS ._.
Parsers_NNS may_MD be_VB programmed_VBN by_IN hand_NN or_CC may_MD be_VB -LRB-_-LRB- semi_JJ -_: -RRB-_-RRB- automatically_RB generated_VBN -LRB-_-LRB- in_IN some_DT programming_NN languages_NNS -RRB-_-RRB- by_IN a_DT tool_NN ._.
Human_JJ languages_NNS See_NNP also_RB :_: Category_NN :_: Natural_JJ language_NN parsing_VBG In_IN some_DT machine_NN translation_NN and_CC natural_JJ language_NN processing_NN systems_NNS ,_, human_JJ languages_NNS are_VBP parsed_VBN by_IN computer_NN programs_NNS ._.
Human_JJ sentences_NNS are_VBP not_RB easily_RB parsed_VBN by_IN programs_NNS ,_, as_IN there_EX is_VBZ substantial_JJ ambiguity_NN in_IN the_DT structure_NN of_IN human_JJ language_NN ,_, whose_WP$ usage_NN is_VBZ to_TO convey_VB meaning_NN -LRB-_-LRB- or_CC semantics_NNS -RRB-_-RRB- amongst_IN a_DT potentially_RB unlimited_JJ range_NN of_IN possibilities_NNS but_CC only_RB some_DT of_IN which_WDT are_VBP germane_JJ to_TO the_DT particular_JJ case_NN ._.
So_IN an_DT utterance_NN ``_`` Man_NN bites_VBZ dog_NN ''_'' versus_CC ``_`` Dog_NN bites_VBZ man_NN ''_'' is_VBZ definite_JJ on_IN one_CD detail_NN but_CC in_IN another_DT language_NN might_MD appear_VB as_IN ``_`` Man_NN dog_NN bites_VBZ ''_'' with_IN a_DT reliance_NN on_IN the_DT larger_JJR context_NN to_TO distinguish_VB between_IN those_DT two_CD possibilities_NNS ,_, if_IN indeed_RB that_DT difference_NN was_VBD of_IN concern_NN ._.
It_PRP is_VBZ difficult_JJ to_TO prepare_VB formal_JJ rules_NNS to_TO describe_VB informal_JJ behavior_NN even_RB though_IN it_PRP is_VBZ clear_JJ that_IN some_DT rules_NNS are_VBP being_VBG followed_VBN ._.
In_IN order_NN to_TO parse_VB natural_JJ language_NN data_NNS ,_, researchers_NNS must_MD first_RB agree_VB on_IN the_DT grammar_NN to_TO be_VB used_VBN ._.
The_DT choice_NN of_IN syntax_NN is_VBZ affected_VBN by_IN both_CC linguistic_JJ and_CC computational_JJ concerns_NNS ;_: for_IN instance_NN some_DT parsing_NN systems_NNS use_VBP lexical_JJ functional_JJ grammar_NN ,_, but_CC in_IN general_JJ ,_, parsing_VBG for_IN grammars_NNS of_IN this_DT type_NN is_VBZ known_VBN to_TO be_VB NP-complete_JJ ._.
Head-driven_JJ phrase_NN structure_NN grammar_NN is_VBZ another_DT linguistic_JJ formalism_NN which_WDT has_VBZ been_VBN popular_JJ in_IN the_DT parsing_VBG community_NN ,_, but_CC other_JJ research_NN efforts_NNS have_VBP focused_VBN on_IN less_JJR complex_JJ formalisms_NNS such_JJ as_IN the_DT one_CD used_VBN in_IN the_DT Penn_NNP Treebank_NNP ._.
Shallow_JJ parsing_NN aims_VBZ to_TO find_VB only_RB the_DT boundaries_NNS of_IN major_JJ constituents_NNS such_JJ as_IN noun_NN phrases_NNS ._.
Another_DT popular_JJ strategy_NN for_IN avoiding_VBG linguistic_JJ controversy_NN is_VBZ dependency_NN grammar_NN parsing_NN ._.
Most_JJS modern_JJ parsers_NNS are_VBP at_IN least_JJS partly_RB statistical_JJ ;_: that_DT is_VBZ ,_, they_PRP rely_VBP on_IN a_DT corpus_NN of_IN training_NN data_NNS which_WDT has_VBZ already_RB been_VBN annotated_JJ -LRB-_-LRB- parsed_VBN by_IN hand_NN -RRB-_-RRB- ._.
This_DT approach_NN allows_VBZ the_DT system_NN to_TO gather_VB information_NN about_IN the_DT frequency_NN with_IN which_WDT various_JJ constructions_NNS occur_VBP in_IN specific_JJ contexts_NNS ._.
-LRB-_-LRB- See_VB machine_NN learning_NN ._. -RRB-_-RRB-
Approaches_NNS which_WDT have_VBP been_VBN used_VBN include_VBP straightforward_JJ PCFGs_NNS -LRB-_-LRB- probabilistic_JJ context-free_JJ grammars_NNS -RRB-_-RRB- ,_, maximum_NN entropy_NN ,_, and_CC neural_JJ nets_NNS ._.
Most_JJS of_IN the_DT more_RBR successful_JJ systems_NNS use_VBP lexical_JJ statistics_NNS -LRB-_-LRB- that_DT is_VBZ ,_, they_PRP consider_VBP the_DT identities_NNS of_IN the_DT words_NNS involved_VBN ,_, as_RB well_RB as_IN their_PRP$ part_NN of_IN speech_NN -RRB-_-RRB- ._.
However_RB such_JJ systems_NNS are_VBP vulnerable_JJ to_TO overfitting_VB and_CC require_VB some_DT kind_NN of_IN smoothing_NN to_TO be_VB effective_JJ ._.
-LRB-_-LRB- citation_NN needed_VBN -RRB-_-RRB- Parsing_VBG algorithms_NNS for_IN natural_JJ language_NN can_MD not_RB rely_VB on_IN the_DT grammar_NN having_VBG `_`` nice_JJ '_'' properties_NNS as_IN with_IN manually_RB designed_VBN grammars_NNS for_IN programming_NN languages_NNS ._.
As_IN mentioned_VBN earlier_RBR some_DT grammar_NN formalisms_NNS are_VBP very_RB difficult_JJ to_TO parse_VB computationally_RB ;_: in_IN general_JJ ,_, even_RB if_IN the_DT desired_VBN structure_NN is_VBZ not_RB context-free_JJ ,_, some_DT kind_NN of_IN context-free_JJ approximation_NN to_TO the_DT grammar_NN is_VBZ used_VBN to_TO perform_VB a_DT first_JJ pass_NN ._.
Algorithms_NNS which_WDT use_VBP context-free_JJ grammars_NNS often_RB rely_VBP on_IN some_DT variant_NN of_IN the_DT CKY_NNP algorithm_NN ,_, usually_RB with_IN some_DT heuristic_NN to_TO prune_VB away_RB unlikely_JJ analyses_NNS to_TO save_VB time_NN ._.
-LRB-_-LRB- See_VB chart_NN parsing_NN ._. -RRB-_-RRB-
However_RB some_DT systems_NNS trade_NN speed_NN for_IN accuracy_NN using_VBG ,_, e.g._FW ,_, linear-time_JJ versions_NNS of_IN the_DT shift-reduce_JJ algorithm_NN ._.
A_DT somewhat_RB recent_JJ development_NN has_VBZ been_VBN parse_JJ reranking_NN in_IN which_WDT the_DT parser_NN proposes_VBZ some_DT large_JJ number_NN of_IN analyses_NNS ,_, and_CC a_DT more_RBR complex_JJ system_NN selects_VBZ the_DT best_JJS option_NN ._.
Programming_NN languages_NNS The_DT most_RBS common_JJ use_NN of_IN a_DT parser_NN is_VBZ as_IN a_DT component_NN of_IN a_DT compiler_NN or_CC interpreter_NN ._.
This_DT parses_VBZ the_DT source_NN code_NN of_IN a_DT computer_NN programming_NN language_NN to_TO create_VB some_DT form_NN of_IN internal_JJ representation_NN ._.
Programming_NN languages_NNS tend_VBP to_TO be_VB specified_VBN in_IN terms_NNS of_IN a_DT context-free_JJ grammar_NN because_IN fast_JJ and_CC efficient_JJ parsers_NNS can_MD be_VB written_VBN for_IN them_PRP ._.
Parsers_NNS are_VBP written_VBN by_IN hand_NN or_CC generated_VBN by_IN parser_NN generators_NNS ._.
Context-free_JJ grammars_NNS are_VBP limited_VBN in_IN the_DT extent_NN to_TO which_WDT they_PRP can_MD express_VB all_DT of_IN the_DT requirements_NNS of_IN a_DT language_NN ._.
Informally_RB ,_, the_DT reason_NN is_VBZ that_IN the_DT memory_NN of_IN such_PDT a_DT language_NN is_VBZ limited_JJ ._.
The_DT grammar_NN can_MD not_RB remember_VB the_DT presence_NN of_IN a_DT construct_NN over_IN an_DT arbitrarily_RB long_JJ input_NN ;_: this_DT is_VBZ necessary_JJ for_IN a_DT language_NN in_IN which_WDT ,_, for_IN example_NN ,_, a_DT name_NN must_MD be_VB declared_VBN before_IN it_PRP may_MD be_VB referenced_VBN ._.
More_RBR powerful_JJ grammars_NNS that_WDT can_MD express_VB this_DT constraint_NN ,_, however_RB ,_, can_MD not_RB be_VB parsed_VBN efficiently_RB ._.
Thus_RB ,_, it_PRP is_VBZ a_DT common_JJ strategy_NN to_TO create_VB a_DT relaxed_VBN parser_NN for_IN a_DT context-free_JJ grammar_NN which_WDT accepts_VBZ a_DT superset_NN of_IN the_DT desired_VBN language_NN constructs_NNS -LRB-_-LRB- that_DT is_VBZ ,_, it_PRP accepts_VBZ some_DT invalid_JJ constructs_NNS -RRB-_-RRB- ;_: later_RB ,_, the_DT unwanted_JJ constructs_NNS can_MD be_VB filtered_VBN out_RP ._.
Overview_NN of_IN process_NN Flow_NN of_IN data_NNS in_IN a_DT typical_JJ parser_NN The_DT following_VBG example_NN demonstrates_VBZ the_DT common_JJ case_NN of_IN parsing_VBG a_DT computer_NN language_NN with_IN two_CD levels_NNS of_IN grammar_NN :_: lexical_JJ and_CC syntactic_JJ ._.
The_DT first_JJ stage_NN is_VBZ the_DT token_JJ generation_NN ,_, or_CC lexical_JJ analysis_NN ,_, by_IN which_WDT the_DT input_NN character_NN stream_NN is_VBZ split_VBN into_IN meaningful_JJ symbols_NNS defined_VBN by_IN a_DT grammar_NN of_IN regular_JJ expressions_NNS ._.
For_IN example_NN ,_, a_DT calculator_NN program_NN would_MD look_VB at_IN an_DT input_NN such_JJ as_IN ``_`` 12_CD \*_NN -LRB-_-LRB- 3_CD +4_NN -RRB-_-RRB- ^_NN 2_CD ''_'' and_CC split_VBD it_PRP into_IN the_DT tokens_VBN 12_CD ,_, \*_NN ,_, -LRB-_-LRB- ,_, 3_CD ,_, +_CC ,_, 4_CD ,_, -RRB-_-RRB- ,_, ^_NN ,_, 2_CD ,_, each_DT of_IN which_WDT is_VBZ a_DT meaningful_JJ symbol_NN in_IN the_DT context_NN of_IN an_DT arithmetic_NN expression_NN ._.
The_DT lexer_NN would_MD contain_VB rules_NNS to_TO tell_VB it_PRP that_IN the_DT characters_NNS \*_NN ,_, +_CC ,_, ^_CD ,_, -LRB-_-LRB- and_CC -RRB-_-RRB- mark_VBP the_DT start_NN of_IN a_DT new_JJ token_JJ ,_, so_RB meaningless_JJ tokens_VBN like_IN ``_`` 12_CD \*_NN ''_'' or_CC ''_'' -LRB-_-LRB- 3_CD ''_'' will_MD not_RB be_VB generated_VBN ._.
The_DT next_JJ stage_NN is_VBZ parsing_VBG or_CC syntactic_JJ analysis_NN ,_, which_WDT is_VBZ checking_VBG that_IN the_DT tokens_VBN form_NN an_DT allowable_JJ expression_NN ._.
This_DT is_VBZ usually_RB done_VBN with_IN reference_NN to_TO a_DT context-free_JJ grammar_NN which_WDT recursively_RB defines_VBZ components_NNS that_WDT can_MD make_VB up_RP an_DT expression_NN and_CC the_DT order_NN in_IN which_WDT they_PRP must_MD appear_VB ._.
However_RB ,_, not_RB all_DT rules_NNS defining_VBG programming_NN languages_NNS can_MD be_VB expressed_VBN by_IN context-free_JJ grammars_NNS alone_RB ,_, for_IN example_NN type_NN validity_NN and_CC proper_JJ declaration_NN of_IN identifiers_NNS ._.
These_DT rules_NNS can_MD be_VB formally_RB expressed_VBN with_IN attribute_NN grammars_NNS ._.
The_DT final_JJ phase_NN is_VBZ semantic_JJ parsing_NN or_CC analysis_NN ,_, which_WDT is_VBZ working_VBG out_RP the_DT implications_NNS of_IN the_DT expression_NN just_RB validated_VBN and_CC taking_VBG the_DT appropriate_JJ action_NN ._.
In_IN the_DT case_NN of_IN a_DT calculator_NN or_CC interpreter_NN ,_, the_DT action_NN is_VBZ to_TO evaluate_VB the_DT expression_NN or_CC program_NN ,_, a_DT compiler_NN ,_, on_IN the_DT other_JJ hand_NN ,_, would_MD generate_VB some_DT kind_NN of_IN code_NN ._.
Attribute_JJ grammars_NNS can_MD also_RB be_VB used_VBN to_TO define_VB these_DT actions_NNS ._.
Types_NNS of_IN parser_NN The_DT task_NN of_IN the_DT parser_NN is_VBZ essentially_RB to_TO determine_VB if_IN and_CC how_WRB the_DT input_NN can_MD be_VB derived_VBN from_IN the_DT start_NN symbol_NN of_IN the_DT grammar_NN ._.
This_DT can_MD be_VB done_VBN in_IN essentially_RB two_CD ways_NNS :_: Top-down_JJ parsing_NN -_: Top-down_JJ parsing_NN can_MD be_VB viewed_VBN as_IN an_DT attempt_NN to_TO find_VB left-most_JJ derivations_NNS of_IN an_DT input-stream_NN by_IN searching_VBG for_IN parse_NN trees_NNS using_VBG a_DT top-down_JJ expansion_NN of_IN the_DT given_VBN formal_JJ grammar_NN rules_NNS ._.
Tokens_NNS are_VBP consumed_VBN from_IN left_NN to_TO right_NN ._.
Inclusive_JJ choice_NN is_VBZ used_VBN to_TO accommodate_VB ambiguity_NN by_IN expanding_VBG all_DT alternative_JJ right-hand-sides_NNS of_IN grammar_NN rules_NNS ._.
Bottom-up_JJ parsing_NN -_: A_DT parser_NN can_MD start_VB with_IN the_DT input_NN and_CC attempt_NN to_TO rewrite_VB it_PRP to_TO the_DT start_NN symbol_NN ._.
Intuitively_RB ,_, the_DT parser_NN attempts_VBZ to_TO locate_VB the_DT most_RBS basic_JJ elements_NNS ,_, then_RB the_DT elements_NNS containing_VBG these_DT ,_, and_CC so_RB on_RB ._.
LR_NN parsers_NNS are_VBP examples_NNS of_IN bottom-up_JJ parsers_NNS ._.
Another_DT term_NN used_VBN for_IN this_DT type_NN of_IN parser_NN is_VBZ Shift-Reduce_NNP parsing_NN ._.
LL_NN parsers_NNS and_CC recursive-descent_JJ parser_NN are_VBP examples_NNS of_IN top-down_JJ parsers_NNS which_WDT can_MD not_RB accommodate_VB left_JJ recursive_JJ productions_NNS ._.
Although_IN it_PRP has_VBZ been_VBN believed_VBN that_IN simple_JJ implementations_NNS of_IN top-down_JJ parsing_NN can_MD not_RB accommodate_VB direct_JJ and_CC indirect_JJ left-recursion_NN and_CC may_MD require_VB exponential_JJ time_NN and_CC space_NN complexity_NN while_IN parsing_VBG ambiguous_JJ context-free_JJ grammars_NNS ,_, more_RBR sophisticated_JJ algorithms_NNS for_IN top-down_JJ parsing_NN have_VBP been_VBN created_VBN by_IN Frost_NNP ,_, Hafiz_NNP ,_, and_CC Callaghan_NNP which_WDT accommodate_VBP ambiguity_NN and_CC left_VBD recursion_NN in_IN polynomial_JJ time_NN and_CC which_WDT generate_VBP polynomial-size_JJ representations_NNS of_IN the_DT potentially_RB exponential_JJ number_NN of_IN parse_NN trees_NNS ._.
Their_PRP$ algorithm_NN is_VBZ able_JJ to_TO produce_VB both_CC left-most_JJ and_CC right-most_JJ derivations_NNS of_IN an_DT input_NN with_IN regard_NN to_TO a_DT given_VBN CFG_NN -LRB-_-LRB- context-free_JJ grammar_NN -RRB-_-RRB- ._.
An_DT important_JJ distinction_NN with_IN regard_NN to_TO parsers_NNS is_VBZ whether_IN a_DT parser_NN generates_VBZ a_DT leftmost_JJ derivation_NN or_CC a_DT rightmost_JJ derivation_NN -LRB-_-LRB- see_VB context-free_JJ grammar_NN -RRB-_-RRB- ._.
LL_NN parsers_NNS will_MD generate_VB a_DT leftmost_JJ derivation_NN and_CC LR_NN parsers_NNS will_MD generate_VB a_DT rightmost_JJ derivation_NN -LRB-_-LRB- although_IN usually_RB in_IN reverse_NN -RRB-_-RRB- ._.
In_IN information_NN retrieval_NN and_CC natural_JJ language_NN processing_NN -LRB-_-LRB- NLP_NN -RRB-_-RRB- ,_, question_NN answering_NN -LRB-_-LRB- QA_NN -RRB-_-RRB- is_VBZ the_DT task_NN of_IN automatically_RB answering_VBG a_DT question_NN posed_VBN in_IN natural_JJ language_NN ._.
To_TO find_VB the_DT answer_NN to_TO a_DT question_NN ,_, a_DT QA_NNP computer_NN program_NN may_MD use_VB either_CC a_DT pre-structured_JJ database_NN or_CC a_DT collection_NN of_IN natural_JJ language_NN documents_NNS -LRB-_-LRB- a_DT text_NN corpus_NN such_JJ as_IN the_DT World_NNP Wide_NN Web_NN or_CC some_DT local_JJ collection_NN -RRB-_-RRB- ._.
QA_NN research_NN attempts_VBZ to_TO deal_VB with_IN a_DT wide_JJ range_NN of_IN question_NN types_NNS including_VBG :_: fact_NN ,_, list_NN ,_, definition_NN ,_, How_WRB ,_, Why_WRB ,_, hypothetical_JJ ,_, semantically_RB constrained_VBN ,_, and_CC cross-lingual_JJ questions_NNS ._.
Search_NN collections_NNS vary_VBP from_IN small_JJ local_JJ document_NN collections_NNS ,_, to_TO internal_JJ organization_NN documents_NNS ,_, to_TO compiled_VBN newswire_NN reports_NNS ,_, to_TO the_DT World_NNP Wide_NN Web_NN ._.
Closed-domain_JJ question_NN answering_VBG deals_NNS with_IN questions_NNS under_IN a_DT specific_JJ domain_NN -LRB-_-LRB- for_IN example_NN ,_, medicine_NN or_CC automotive_JJ maintenance_NN -RRB-_-RRB- ,_, and_CC can_MD be_VB seen_VBN as_IN an_DT easier_JJR task_NN because_IN NLP_NN systems_NNS can_MD exploit_VB domain-specific_JJ knowledge_NN frequently_RB formalized_VBN in_IN ontologies_NNS ._.
Alternatively_RB ,_, closed-domain_NN might_MD refer_VB to_TO a_DT situation_NN where_WRB only_RB a_DT limited_JJ type_NN of_IN questions_NNS are_VBP accepted_VBN ,_, such_JJ as_IN questions_NNS asking_VBG for_IN descriptive_JJ rather_RB than_IN procedural_JJ information_NN ._.
Open-domain_JJ question_NN answering_VBG deals_NNS with_IN questions_NNS about_IN nearly_RB anything_NN ,_, and_CC can_MD only_RB rely_VB on_IN general_JJ ontologies_NNS and_CC world_NN knowledge_NN ._.
On_IN the_DT other_JJ hand_NN ,_, these_DT systems_NNS usually_RB have_VBP much_RB more_JJR data_NNS available_JJ from_IN which_WDT to_TO extract_VB the_DT answer_NN ._.
In_IN contrast_NN ,_, current_JJ QA_NNP systems_NNS use_VBP text_NN documents_NNS as_IN their_PRP$ underlying_JJ knowledge_NN source_NN and_CC combine_VB various_JJ natural_JJ language_NN processing_NN techniques_NNS to_TO search_VB for_IN the_DT answers_NNS ._.
Current_NNP QA_NNP systems_NNS typically_RB include_VBP a_DT question_NN classifier_NN module_NN that_WDT determines_VBZ the_DT type_NN of_IN question_NN and_CC the_DT type_NN of_IN answer_NN ._.
After_IN the_DT question_NN is_VBZ analyzed_VBN ,_, the_DT system_NN typically_RB uses_VBZ several_JJ modules_NNS that_WDT apply_VBP increasingly_RB complex_JJ NLP_NN techniques_NNS on_IN a_DT gradually_RB reduced_VBN amount_NN of_IN text_NN ._.
Thus_RB ,_, a_DT document_NN retrieval_NN module_NN uses_VBZ search_NN engines_NNS to_TO identify_VB the_DT documents_NNS or_CC paragraphs_NNS in_IN the_DT document_NN set_NN that_WDT are_VBP likely_JJ to_TO contain_VB the_DT answer_NN ._.
Subsequently_RB a_DT filter_NN preselects_VBZ small_JJ text_NN fragments_NNS that_WDT contain_VBP strings_NNS of_IN the_DT same_JJ type_NN as_IN the_DT expected_VBN answer_NN ._.
For_IN example_NN ,_, if_IN the_DT question_NN is_VBZ ``_`` Who_WP invented_VBD Penicillin_NNP ''_'' the_DT filter_NN returns_NNS text_NN that_WDT contain_VBP names_NNS of_IN people_NNS ._.
Finally_RB ,_, an_DT answer_NN extraction_NN module_NN looks_VBZ for_IN further_JJ clues_NNS in_IN the_DT text_NN to_TO determine_VB if_IN the_DT answer_NN candidate_NN can_MD indeed_RB answer_VB the_DT question_NN ._.
Question_NN answering_NN methods_NNS QA_NN is_VBZ very_RB dependent_JJ on_IN a_DT good_JJ search_NN corpus_NN -_: for_IN without_IN documents_NNS containing_VBG the_DT answer_NN ,_, there_EX is_VBZ little_RB any_DT QA_NN system_NN can_MD do_VB ._.
It_PRP thus_RB makes_VBZ sense_NN that_IN larger_JJR collection_NN sizes_NNS generally_RB lend_VBP well_RB to_TO better_JJR QA_NN performance_NN ,_, unless_IN the_DT question_NN domain_NN is_VBZ orthogonal_JJ to_TO the_DT collection_NN ._.
The_DT notion_NN of_IN data_NNS redundancy_NN in_IN massive_JJ collections_NNS ,_, such_JJ as_IN the_DT web_NN ,_, means_VBZ that_IN nuggets_NNS of_IN information_NN are_VBP likely_JJ to_TO be_VB phrased_VBN in_IN many_JJ different_JJ ways_NNS in_IN differing_VBG contexts_NNS and_CC documents_NNS ,_, leading_VBG to_TO two_CD benefits_NNS :_: By_IN having_VBG the_DT right_JJ information_NN appear_VBP in_IN many_JJ forms_NNS ,_, the_DT burden_NN on_IN the_DT QA_NN system_NN to_TO perform_VB complex_JJ NLP_NN techniques_NNS to_TO understand_VB the_DT text_NN is_VBZ lessened_VBN ._.
Correct_JJ answers_NNS can_MD be_VB filtered_VBN from_IN false_JJ positives_NNS by_IN relying_VBG on_IN the_DT correct_JJ answer_NN to_TO appear_VB more_JJR times_NNS in_IN the_DT documents_NNS than_IN instances_NNS of_IN incorrect_JJ ones_NNS ._.
Issues_NNS In_IN 2002_CD a_DT group_NN of_IN researchers_NNS wrote_VBD a_DT roadmap_NN of_IN research_NN in_IN question_NN answering_NN ._.
The_DT following_VBG issues_NNS were_VBD identified_VBN ._.
Question_NN classes_NNS Different_JJ types_NNS of_IN questions_NNS -LRB-_-LRB- e.g._FW ,_, ``_`` What_WP is_VBZ the_DT capital_NN of_IN Lichtenstein_NNP ?_. ''_''
vs._CC ``_`` Why_WRB does_VBZ a_DT rainbow_NN form_NN ?_. ''_''
vs._CC ``_`` Did_VBD Marilyn_NNP Monroe_NNP and_CC Cary_NNP Grant_NNP ever_RB appear_VBP in_IN a_DT movie_NN together_RB ?_. ''_'' -RRB-_-RRB-
require_VB the_DT use_NN of_IN different_JJ strategies_NNS to_TO find_VB the_DT answer_NN ._.
Question_NN classes_NNS are_VBP arranged_VBN hierarchically_RB in_IN taxonomies_NNS ._.
Question_NN processing_NN The_DT same_JJ information_NN request_NN can_MD be_VB expressed_VBN in_IN various_JJ ways_NNS ,_, some_DT interrogative_JJ -LRB-_-LRB- ``_`` Who_WP is_VBZ the_DT president_NN of_IN the_DT United_NNP States_NNPS ?_. ''_'' -RRB-_-RRB-
and_CC some_DT assertive_JJ -LRB-_-LRB- ``_`` Tell_VB me_PRP the_DT name_NN of_IN the_DT president_NN of_IN the_DT United_NNP States_NNPS ._. ''_'' -RRB-_-RRB- ._.
A_DT semantic_JJ model_NN of_IN question_NN understanding_NN and_CC processing_NN would_MD recognize_VB equivalent_JJ questions_NNS ,_, regardless_RB of_IN how_WRB they_PRP are_VBP presented_VBN ._.
This_DT model_NN would_MD enable_VB the_DT translation_NN of_IN a_DT complex_JJ question_NN into_IN a_DT series_NN of_IN simpler_JJR questions_NNS ,_, would_MD identify_VB ambiguities_NNS and_CC treat_VB them_PRP in_IN context_NN or_CC by_IN interactive_JJ clarification_NN ._.
Context_NN and_CC QA_NN Questions_NNS are_VBP usually_RB asked_VBN within_IN a_DT context_NN and_CC answers_NNS are_VBP provided_VBN within_IN that_DT specific_JJ context_NN ._.
The_DT context_NN can_MD be_VB used_VBN to_TO clarify_VB a_DT question_NN ,_, resolve_NN ambiguities_NNS or_CC keep_VB track_NN of_IN an_DT investigation_NN performed_VBN through_IN a_DT series_NN of_IN questions_NNS ._.
-LRB-_-LRB- For_IN example_NN ,_, the_DT question_NN ,_, ``_`` Why_WRB did_VBD Joe_NNP Biden_NNP visit_NN Iraq_NNP in_IN January_NNP 2010_CD ?_. ''_''
might_MD be_VB asking_VBG why_WRB Vice_NNP President_NNP Biden_NNP visited_VBD and_CC not_RB President_NNP Obama_NNP ,_, why_WRB he_PRP went_VBD to_TO Iraq_NNP and_CC not_RB Afghanistan_NNP or_CC some_DT other_JJ country_NN ,_, why_WRB he_PRP went_VBD in_IN January_NNP 2010_CD and_CC not_RB before_IN or_CC after_IN ,_, or_CC what_WP Biden_NNP was_VBD hoping_VBG to_TO accomplish_VB with_IN his_PRP$ visit_NN ._.
If_IN the_DT question_NN is_VBZ one_CD of_IN a_DT series_NN of_IN related_JJ questions_NNS ,_, the_DT previous_JJ questions_NNS and_CC their_PRP$ answers_NNS might_MD shed_VB light_NN on_IN the_DT questioner_NN 's_POS intent_NN ._. -RRB-_-RRB-
Data_NN sources_NNS for_IN QA_NN Before_IN a_DT question_NN can_MD be_VB answered_VBN ,_, it_PRP must_MD be_VB known_VBN what_WDT knowledge_NN sources_NNS are_VBP available_JJ and_CC relevant_JJ ._.
If_IN the_DT answer_NN to_TO a_DT question_NN is_VBZ not_RB present_JJ in_IN the_DT data_NNS sources_NNS ,_, no_DT matter_NN how_WRB well_RB the_DT question_NN processing_NN ,_, information_NN retrieval_NN and_CC answer_NN extraction_NN is_VBZ performed_VBN ,_, a_DT correct_JJ result_NN will_MD not_RB be_VB obtained_VBN ._.
Answer_NN extraction_NN Answer_NN extraction_NN depends_VBZ on_IN the_DT complexity_NN of_IN the_DT question_NN ,_, on_IN the_DT answer_NN type_NN provided_VBN by_IN question_NN processing_NN ,_, on_IN the_DT actual_JJ data_NNS where_WRB the_DT answer_NN is_VBZ searched_VBN ,_, on_IN the_DT search_NN method_NN and_CC on_IN the_DT question_NN focus_NN and_CC context_NN ._.
Answer_NN formulation_NN The_DT result_NN of_IN a_DT QA_NN system_NN should_MD be_VB presented_VBN in_IN a_DT way_NN as_RB natural_JJ as_IN possible_JJ ._.
In_IN some_DT cases_NNS ,_, simple_JJ extraction_NN is_VBZ sufficient_JJ ._.
For_IN example_NN ,_, when_WRB the_DT question_NN classification_NN indicates_VBZ that_IN the_DT answer_NN type_NN is_VBZ a_DT name_NN -LRB-_-LRB- of_IN a_DT person_NN ,_, organization_NN ,_, shop_NN or_CC disease_NN ,_, etc._NN -RRB-_-RRB- ,_, a_DT quantity_NN -LRB-_-LRB- monetary_JJ value_NN ,_, length_NN ,_, size_NN ,_, distance_NN ,_, etc._NN -RRB-_-RRB- or_CC a_DT date_NN -LRB-_-LRB- e.g._FW the_DT answer_NN to_TO the_DT question_NN ,_, ``_`` On_IN what_WDT day_NN did_VBD Christmas_NNP fall_VB in_IN 1989_CD ?_. ''_'' -RRB-_-RRB-
the_DT extraction_NN of_IN a_DT single_JJ datum_NN is_VBZ sufficient_JJ ._.
For_IN other_JJ cases_NNS ,_, the_DT presentation_NN of_IN the_DT answer_NN may_MD require_VB the_DT use_NN of_IN fusion_NN techniques_NNS that_WDT combine_VBP the_DT partial_JJ answers_NNS from_IN multiple_JJ documents_NNS ._.
Real_JJ time_NN question_NN answering_VBG There_EX is_VBZ need_NN for_IN developing_VBG Q&A_NNP systems_NNS that_WDT are_VBP capable_JJ of_IN extracting_VBG answers_NNS from_IN large_JJ data_NNS sets_NNS in_IN several_JJ seconds_NNS ,_, regardless_RB of_IN the_DT complexity_NN of_IN the_DT question_NN ,_, the_DT size_NN and_CC multitude_NN of_IN the_DT data_NN sources_NNS or_CC the_DT ambiguity_NN of_IN the_DT question_NN ._.
Multilingual_JJ -LRB-_-LRB- or_CC cross-lingual_JJ -RRB-_-RRB- question_NN answering_VBG The_DT ability_NN to_TO answer_VB a_DT question_NN posed_VBN in_IN one_CD language_NN using_VBG an_DT answer_NN corpus_NN in_IN another_DT language_NN -LRB-_-LRB- or_CC even_RB several_JJ -RRB-_-RRB- ._.
This_DT allows_VBZ users_NNS to_TO consult_VB information_NN that_IN they_PRP can_MD not_RB use_VB directly_RB ._.
-LRB-_-LRB- See_VB also_RB Machine_NNP translation_NN ._. -RRB-_-RRB-
Interactive_JJ QA_NN It_PRP is_VBZ often_RB the_DT case_NN that_IN the_DT information_NN need_NN is_VBZ not_RB well_RB captured_VBN by_IN a_DT QA_NN system_NN ,_, as_IN the_DT question_NN processing_NN part_NN may_MD fail_VB to_TO classify_VB properly_RB the_DT question_NN or_CC the_DT information_NN needed_VBN for_IN extracting_VBG and_CC generating_VBG the_DT answer_NN is_VBZ not_RB easily_RB retrieved_VBN ._.
In_IN such_JJ cases_NNS ,_, the_DT questioner_NN might_MD want_VB not_RB only_RB to_TO reformulate_VB the_DT question_NN ,_, but_CC to_TO have_VB a_DT dialogue_NN with_IN the_DT system_NN ._.
-LRB-_-LRB- For_IN example_NN ,_, the_DT system_NN might_MD ask_VB for_IN a_DT clarification_NN of_IN what_WDT sense_NN a_DT word_NN is_VBZ being_VBG used_VBN ,_, or_CC what_WDT type_NN of_IN information_NN is_VBZ being_VBG asked_VBN for_IN ._. -RRB-_-RRB-
Advanced_NNP reasoning_NN for_IN QA_NNP More_JJR sophisticated_JJ questioners_NNS expect_VBP answers_NNS that_WDT are_VBP outside_IN the_DT scope_NN of_IN written_JJ texts_NNS or_CC structured_JJ databases_NNS ._.
To_TO upgrade_VB a_DT QA_NN system_NN with_IN such_JJ capabilities_NNS ,_, it_PRP would_MD be_VB necessary_JJ to_TO integrate_VB reasoning_NN components_NNS operating_VBG on_IN a_DT variety_NN of_IN knowledge_NN bases_NNS ,_, encoding_VBG world_NN knowledge_NN and_CC common-sense_JJ reasoning_NN mechanisms_NNS ,_, as_RB well_RB as_IN knowledge_NN specific_JJ to_TO a_DT variety_NN of_IN domains_NNS ._.
User_NN profiling_NN for_IN QA_NN The_DT user_NN profile_NN captures_VBZ data_NNS about_IN the_DT questioner_NN ,_, comprising_VBG context_NN data_NNS ,_, domain_NN of_IN interest_NN ,_, reasoning_NN schemes_NNS frequently_RB used_VBN by_IN the_DT questioner_NN ,_, common_JJ ground_NN established_VBN within_IN different_JJ dialogues_NNS between_IN the_DT system_NN and_CC the_DT user_NN ,_, and_CC so_RB forth_RB ._.
The_DT profile_NN may_MD be_VB represented_VBN as_IN a_DT predefined_JJ template_NN ,_, where_WRB each_DT template_NN slot_NN represents_VBZ a_DT different_JJ profile_NN feature_NN ._.
Profile_NN templates_NNS may_MD be_VB nested_JJ one_CD within_IN another_DT ._.
History_NN Some_DT of_IN the_DT early_JJ AI_NN systems_NNS were_VBD question_NN answering_VBG systems_NNS ._.
Two_CD of_IN the_DT most_RBS famous_JJ QA_NN systems_NNS of_IN that_DT time_NN are_VBP BASEBALL_NNP and_CC LUNAR_NNP ,_, both_DT of_IN which_WDT were_VBD developed_VBN in_IN the_DT 1960s_NNS ._.
BASEBALL_NN answered_VBD questions_NNS about_IN the_DT US_NNP baseball_NN league_NN over_IN a_DT period_NN of_IN one_CD year_NN ._.
LUNAR_NN ,_, in_IN turn_NN ,_, answered_VBD questions_NNS about_IN the_DT geological_JJ analysis_NN of_IN rocks_NNS returned_VBN by_IN the_DT Apollo_NNP moon_NN missions_NNS ._.
Both_DT QA_NNP systems_NNS were_VBD very_RB effective_JJ in_IN their_PRP$ chosen_JJ domains_NNS ._.
In_IN fact_NN ,_, LUNAR_NN was_VBD demonstrated_VBN at_IN a_DT lunar_JJ science_NN convention_NN in_IN 1971_CD and_CC it_PRP was_VBD able_JJ to_TO answer_VB 90_CD %_NN of_IN the_DT questions_NNS in_IN its_PRP$ domain_NN posed_VBN by_IN people_NNS untrained_JJ on_IN the_DT system_NN ._.
Further_JJ restricted-domain_JJ QA_NN systems_NNS were_VBD developed_VBN in_IN the_DT following_JJ years_NNS ._.
The_DT common_JJ feature_NN of_IN all_PDT these_DT systems_NNS is_VBZ that_IN they_PRP had_VBD a_DT core_NN database_NN or_CC knowledge_NN system_NN that_WDT was_VBD hand-written_JJ by_IN experts_NNS of_IN the_DT chosen_JJ domain_NN ._.
Some_DT of_IN the_DT early_JJ AI_NNP systems_NNS included_VBD question-answering_JJ abilities_NNS ._.
Two_CD of_IN the_DT most_RBS famous_JJ early_JJ systems_NNS are_VBP SHRDLU_NNP and_CC ELIZA_NNP ._.
SHRDLU_NN simulated_JJ the_DT operation_NN of_IN a_DT robot_NN in_IN a_DT toy_NN world_NN -LRB-_-LRB- the_DT ``_`` blocks_VBZ world_NN ''_'' -RRB-_-RRB- ,_, and_CC it_PRP offered_VBD the_DT possibility_NN to_TO ask_VB the_DT robot_NN questions_NNS about_IN the_DT state_NN of_IN the_DT world_NN ._.
Again_RB ,_, the_DT strength_NN of_IN this_DT system_NN was_VBD the_DT choice_NN of_IN a_DT very_RB specific_JJ domain_NN and_CC a_DT very_RB simple_JJ world_NN with_IN rules_NNS of_IN physics_NNS that_WDT were_VBD easy_JJ to_TO encode_VB in_IN a_DT computer_NN program_NN ._.
ELIZA_NNP ,_, in_IN contrast_NN ,_, simulated_JJ a_DT conversation_NN with_IN a_DT psychologist_NN ._.
ELIZA_NNP was_VBD able_JJ to_TO converse_VB on_IN any_DT topic_NN by_IN resorting_VBG to_TO very_RB simple_JJ rules_NNS that_WDT detected_VBD important_JJ words_NNS in_IN the_DT person_NN 's_POS input_NN ._.
It_PRP had_VBD a_DT very_RB rudimentary_JJ way_NN to_TO answer_VB questions_NNS ,_, and_CC on_IN its_PRP$ own_JJ it_PRP led_VBD to_TO a_DT series_NN of_IN chatterbots_NNS such_JJ as_IN the_DT ones_NNS that_WDT participate_VBP in_IN the_DT annual_JJ Loebner_NNP prize_NN ._.
The_DT 1970s_NNS and_CC 1980s_NNS saw_VBD the_DT development_NN of_IN comprehensive_JJ theories_NNS in_IN computational_JJ linguistics_NNS ,_, which_WDT led_VBD to_TO the_DT development_NN of_IN ambitious_JJ projects_NNS in_IN text_NN comprehension_NN and_CC question_NN answering_NN ._.
One_CD example_NN of_IN such_PDT a_DT system_NN was_VBD the_DT Unix_NNP Consultant_NN -LRB-_-LRB- UC_NN -RRB-_-RRB- ,_, a_DT system_NN that_WDT answered_VBD questions_NNS pertaining_VBG to_TO the_DT Unix_NNP operating_NN system_NN ._.
The_DT system_NN had_VBD a_DT comprehensive_JJ hand-crafted_JJ knowledge_NN base_NN of_IN its_PRP$ domain_NN ,_, and_CC it_PRP aimed_VBD at_IN phrasing_NN the_DT answer_NN to_TO accommodate_VB various_JJ types_NNS of_IN users_NNS ._.
Another_DT project_NN was_VBD LILOG_NNP ,_, a_DT text-understanding_JJ system_NN that_WDT operated_VBD on_IN the_DT domain_NN of_IN tourism_NN information_NN in_IN a_DT German_JJ city_NN ._.
The_DT systems_NNS developed_VBD in_IN the_DT UC_NN and_CC LILOG_NN projects_NNS never_RB went_VBD past_IN the_DT stage_NN of_IN simple_JJ demonstrations_NNS ,_, but_CC they_PRP helped_VBD the_DT development_NN of_IN theories_NNS on_IN computational_JJ linguistics_NNS and_CC reasoning_NN ._.
An_DT increasing_VBG number_NN of_IN systems_NNS include_VBP the_DT World_NNP Wide_NN Web_NN as_IN one_CD more_JJR corpus_NN of_IN text_NN ._. ._.
However_RB ,_, these_DT tools_NNS mostly_RB work_VBP by_IN using_VBG shallow_JJ methods_NNS ,_, as_IN described_VBN above_IN --_: thus_RB returning_VBG a_DT list_NN of_IN documents_NNS ,_, usually_RB with_IN an_DT excerpt_NN containing_VBG the_DT probable_JJ answer_NN highlighted_VBD ,_, plus_CC some_DT context_NN ._.
Furthermore_RB ,_, highly-specialized_JJ natural_JJ language_NN question-answering_NN engines_NNS ,_, such_JJ as_IN EAGLi_NNS for_IN health_NN and_CC life_NN scientists_NNS ,_, have_VBP been_VBN made_VBN available_JJ ._.
The_DT Future_NNP of_IN Question_NNP Answering_NNP QA_NNP systems_NNS have_VBP been_VBN extended_VBN in_IN recent_JJ years_NNS to_TO explore_VB critical_JJ new_JJ scientific_JJ and_CC practical_JJ dimensions_NNS For_IN example_NN ,_, systems_NNS have_VBP been_VBN developed_VBN to_TO automatically_RB answer_VB temporal_JJ and_CC geospatial_JJ questions_NNS ,_, definitional_JJ questions_NNS ,_, biographical_JJ questions_NNS ,_, multilingual_JJ questions_NNS ,_, and_CC questions_NNS from_IN multimedia_NNS -LRB-_-LRB- e.g._FW ,_, audio_NN ,_, imagery_NN ,_, video_NN -RRB-_-RRB- ._.
Additional_JJ aspects_NNS such_JJ as_IN interactivity_NN -LRB-_-LRB- often_RB required_VBN for_IN clarification_NN of_IN questions_NNS or_CC answers_NNS -RRB-_-RRB- ,_, answer_NN reuse_NN ,_, and_CC knowledge_NN representation_NN and_CC reasoning_NN to_TO support_VB question_NN answering_NN have_VBP been_VBN explored_VBN ._.
Future_JJ research_NN may_MD explore_VB what_WP kinds_NNS of_IN questions_NNS can_MD be_VB asked_VBN and_CC answered_VBN about_IN social_JJ media_NNS ,_, including_VBG sentiment_NN analysis_NN ._.
A_DT relationship_NN extraction_NN task_NN requires_VBZ the_DT detection_NN and_CC classification_NN of_IN semantic_JJ relationship_NN mentions_VBZ within_IN a_DT set_NN of_IN artifacts_NNS ,_, typically_RB from_IN text_NN or_CC XML_NN documents_NNS ._.
The_DT task_NN is_VBZ very_RB similar_JJ to_TO that_DT of_IN information_NN extraction_NN -LRB-_-LRB- IE_NN -RRB-_-RRB- ,_, but_CC IE_NN additionally_RB requires_VBZ the_DT removal_NN of_IN repeated_VBN relations_NNS -LRB-_-LRB- disambiguation_NN -RRB-_-RRB- and_CC generally_RB refers_VBZ to_TO the_DT extraction_NN of_IN many_JJ different_JJ relationships_NNS ._.
Approaches_NNS One_CD approach_NN to_TO this_DT problem_NN involves_VBZ the_DT use_NN of_IN domain_NN ontologies_NNS ._.
Another_DT approach_NN involves_VBZ visual_JJ detection_NN of_IN meaningful_JJ relationships_NNS in_IN parametric_JJ values_NNS of_IN objects_NNS listed_VBN on_IN a_DT data_NN table_NN that_IN shift_NN positions_NNS as_IN the_DT table_NN is_VBZ permuted_VBN automatically_RB as_IN controlled_VBN by_IN the_DT software_NN user_NN ._.
The_DT poor_JJ coverage_NN ,_, rarity_NN and_CC development_NN cost_NN related_JJ to_TO structured_JJ resources_NNS such_JJ as_IN semantic_JJ lexicons_NNS -LRB-_-LRB- e.g._FW WordNet_NNP ,_, UMLS_NNP -RRB-_-RRB- and_CC domain_NN ontologies_NNS -LRB-_-LRB- e.g._FW the_DT Gene_NNP Ontology_NNP -RRB-_-RRB- has_VBZ given_VBN rise_NN to_TO new_JJ approaches_NNS based_VBN on_IN broad_JJ ,_, dynamic_JJ background_NN knowledge_NN on_IN the_DT Web_NN ._.
For_IN instance_NN ,_, the_DT ARCHILES_NN technique_NN uses_VBZ only_RB Wikipedia_NNP and_CC search_NN engine_NN page_NN count_NN for_IN acquiring_VBG coarse-grained_JJ relations_NNS to_TO construct_VB lightweight_JJ ontologies_NNS ._.
The_DT relationships_NNS can_MD be_VB represented_VBN using_VBG a_DT variety_NN of_IN formalisms\/languages_NNS ._.
One_CD such_JJ representation_NN language_NN for_IN data_NNS on_IN the_DT Web_NN is_VBZ RDF_NNP ._.
Jump_NN to_TO :_: navigation_NN ,_, search_NN Sentence_NN boundary_NN disambiguation_NN -LRB-_-LRB- SBD_NN -RRB-_-RRB- ,_, also_RB known_VBN as_IN sentence_NN breaking_NN ,_, is_VBZ the_DT problem_NN in_IN natural_JJ language_NN processing_NN of_IN deciding_VBG where_WRB sentences_NNS begin_VB and_CC end_VB ._.
Often_RB natural_JJ language_NN processing_NN tools_NNS require_VBP their_PRP$ input_NN to_TO be_VB divided_VBN into_IN sentences_NNS for_IN a_DT number_NN of_IN reasons_NNS ._.
However_RB sentence_NN boundary_NN identification_NN is_VBZ challenging_JJ because_IN punctuation_NN marks_NNS are_VBP often_RB ambiguous_JJ ._.
For_IN example_NN ,_, a_DT period_NN may_MD denote_VB an_DT abbreviation_NN ,_, decimal_JJ point_NN ,_, an_DT ellipsis_NN ,_, or_CC an_DT email_NN address_NN -_: not_RB the_DT end_NN of_IN a_DT sentence_NN ._.
About_IN 47_CD %_NN of_IN the_DT periods_NNS in_IN the_DT Wall_NNP Street_NNP Journal_NNP corpus_NN denote_VB abbreviations_NNS ._.
As_IN well_RB ,_, question_NN marks_NNS and_CC exclamation_NN marks_NNS may_MD appear_VB in_IN embedded_JJ quotations_NNS ,_, emoticons_NNS ,_, computer_NN code_NN ,_, and_CC slang_NN ._.
Languages_NNS like_IN Japanese_NNP and_CC Chinese_NNP have_VBP unambiguous_JJ sentence-ending_JJ markers_NNS ._.
-LRB-_-LRB- b_NN -RRB-_-RRB- If_IN the_DT preceding_VBG token_JJ is_VBZ on_IN my_PRP$ hand-compiled_JJ list_NN of_IN abbreviations_NNS ,_, then_RB it_PRP does_VBZ n't_RB end_VB a_DT sentence_NN ._.
-LRB-_-LRB- c_NN -RRB-_-RRB- If_IN the_DT next_JJ token_JJ is_VBZ capitalized_VBN ,_, then_RB it_PRP ends_VBZ a_DT sentence_NN ._.
This_DT strategy_NN gets_VBZ about_IN 95_CD %_NN of_IN sentences_NNS correct_JJ ._.
Another_DT approach_NN is_VBZ to_TO automatically_RB learn_VB a_DT set_NN of_IN rules_NNS from_IN a_DT set_NN of_IN documents_NNS where_WRB the_DT sentence_NN breaks_NNS are_VBP pre-marked_JJ ._.
Solutions_NNPS have_VBP been_VBN based_VBN on_IN a_DT maximum_NN entropy_NN model_NN ._.
The_DT SATZ_NNP architecture_NN uses_VBZ a_DT neural_JJ network_NN to_TO disambiguate_VB sentence_NN boundaries_NNS and_CC achieves_VBZ 98.5_CD %_NN accuracy_NN ._.
Sentiment_NN analysis_NN or_CC opinion_NN mining_NN refers_VBZ to_TO the_DT application_NN of_IN natural_JJ language_NN processing_NN ,_, computational_JJ linguistics_NNS ,_, and_CC text_NN analytics_NNS to_TO identify_VB and_CC extract_VB subjective_JJ information_NN in_IN source_NN materials_NNS ._.
Generally_RB speaking_NN ,_, sentiment_NN analysis_NN aims_VBZ to_TO determine_VB the_DT attitude_NN of_IN a_DT speaker_NN or_CC a_DT writer_NN with_IN respect_NN to_TO some_DT topic_NN or_CC the_DT overall_JJ contextual_JJ polarity_NN of_IN a_DT document_NN ._.
The_DT attitude_NN may_MD be_VB his_PRP$ or_CC her_PRP$ judgement_NN or_CC evaluation_NN -LRB-_-LRB- see_VB appraisal_NN theory_NN -RRB-_-RRB- ,_, affective_JJ state_NN -LRB-_-LRB- that_WDT is_VBZ to_TO say_VB ,_, the_DT emotional_JJ state_NN of_IN the_DT author_NN when_WRB writing_VBG -RRB-_-RRB- ,_, or_CC the_DT intended_JJ emotional_JJ communication_NN -LRB-_-LRB- that_WDT is_VBZ to_TO say_VB ,_, the_DT emotional_JJ effect_NN the_DT author_NN wishes_VBZ to_TO have_VB on_IN the_DT reader_NN -RRB-_-RRB- ._.
Advanced_NNP ,_, ``_`` beyond_IN polarity_NN ''_'' sentiment_NN classification_NN looks_VBZ ,_, for_IN instance_NN ,_, at_IN emotional_JJ states_NNS such_JJ as_IN ``_`` angry_JJ ,_, ''_'' ``_`` sad_JJ ,_, ''_'' and_CC ``_`` happy_JJ ._. ''_''
Early_JJ work_NN in_IN that_DT area_NN includes_VBZ Turney_NNP and_CC Pang_NNP who_WP applied_VBD different_JJ methods_NNS for_IN detecting_VBG the_DT polarity_NN of_IN product_NN reviews_NNS and_CC movie_NN reviews_NNS respectively_RB ._.
This_DT work_NN is_VBZ at_IN the_DT document_NN level_NN ._.
One_CD can_MD also_RB classify_VB a_DT document_NN 's_POS polarity_NN on_IN a_DT multi-way_JJ scale_NN ,_, which_WDT was_VBD attempted_VBN by_IN Pang_NNP and_CC Snyder_NNP -LRB-_-LRB- among_IN others_NNS -RRB-_-RRB- :_: expanded_VBN the_DT basic_JJ task_NN of_IN classifying_VBG a_DT movie_NN review_NN as_IN either_CC positive_JJ or_CC negative_JJ to_TO predicting_VBG star_NN ratings_NNS on_IN either_CC a_DT 3_CD or_CC a_DT 4_CD star_NN scale_NN ,_, while_IN Snyder_NNP performed_VBD an_DT in-depth_JJ analysis_NN of_IN restaurant_NN reviews_NNS ,_, predicting_VBG ratings_NNS for_IN various_JJ aspects_NNS of_IN the_DT given_VBN restaurant_NN ,_, such_JJ as_IN the_DT food_NN and_CC atmosphere_NN -LRB-_-LRB- on_IN a_DT five-star_JJ scale_NN -RRB-_-RRB- ._.
A_DT different_JJ method_NN for_IN determining_VBG sentiment_NN is_VBZ the_DT use_NN of_IN a_DT scaling_NN system_NN whereby_WRB words_NNS commonly_RB associated_VBN with_IN having_VBG a_DT negative_JJ ,_, neutral_JJ or_CC positive_JJ sentiment_NN with_IN them_PRP are_VBP given_VBN an_DT associated_VBN number_NN on_IN a_DT -5_CD to_TO +5_CD scale_NN -LRB-_-LRB- most_RBS negative_JJ up_IN to_TO most_RBS positive_JJ -RRB-_-RRB- and_CC when_WRB a_DT piece_NN of_IN unstructured_JJ text_NN is_VBZ analyzed_VBN using_VBG natural_JJ language_NN processing_NN ,_, the_DT subsequent_JJ concepts_NNS are_VBP analyzed_VBN for_IN an_DT understanding_NN of_IN these_DT words_NNS and_CC how_WRB they_PRP relate_VBP to_TO the_DT concept_NN -LRB-_-LRB- citation_NN needed_VBN -RRB-_-RRB- ._.
Each_DT concept_NN is_VBZ then_RB given_VBN a_DT score_NN based_VBN on_IN the_DT way_NN sentiment_NN words_NNS relate_VBP to_TO the_DT concept_NN ,_, and_CC their_PRP$ associated_VBN score_NN ._.
This_DT allows_VBZ movement_NN to_TO a_DT more_RBR sophisticated_JJ understanding_NN of_IN sentiment_NN based_VBN on_IN an_DT 11_CD point_NN scale_NN ._.
Alternatively_RB ,_, texts_NNS can_MD be_VB given_VBN a_DT positive_JJ and_CC negative_JJ sentiment_NN strength_NN score_NN if_IN the_DT goal_NN is_VBZ to_TO determine_VB the_DT sentiment_NN in_IN a_DT text_NN rather_RB than_IN the_DT overall_JJ polarity_NN and_CC strength_NN of_IN the_DT text_NN ._.
Another_DT research_NN direction_NN is_VBZ subjectivity\/objectivity_NN identification_NN ._.
This_DT task_NN is_VBZ commonly_RB defined_VBN as_IN classifying_VBG a_DT given_VBN text_NN -LRB-_-LRB- usually_RB a_DT sentence_NN -RRB-_-RRB- into_IN one_CD of_IN two_CD classes_NNS :_: objective_JJ or_CC subjective_JJ ._.
This_DT problem_NN can_MD sometimes_RB be_VB more_RBR difficult_JJ than_IN polarity_NN classification_NN :_: the_DT subjectivity_NN of_IN words_NNS and_CC phrases_NNS may_MD depend_VB on_IN their_PRP$ context_NN and_CC an_DT objective_JJ document_NN may_MD contain_VB subjective_JJ sentences_NNS -LRB-_-LRB- e.g._FW ,_, a_DT news_NN article_NN quoting_VBG people_NNS 's_POS opinions_NNS -RRB-_-RRB- ._.
Moreover_RB ,_, as_IN mentioned_VBN by_IN Su_NNP ,_, results_NNS are_VBP largely_RB dependent_JJ on_IN the_DT definition_NN of_IN subjectivity_NN used_VBN when_WRB annotating_VBG texts_NNS ._.
However_RB ,_, Pang_NNP showed_VBD that_IN removing_VBG objective_JJ sentences_NNS from_IN a_DT document_NN before_IN classifying_VBG its_PRP$ polarity_NN helped_VBD improve_VB performance_NN ._.
The_DT more_RBR fine-grained_JJ analysis_NN model_NN is_VBZ called_VBN the_DT feature\/aspect-based_JJ sentiment_NN analysis_NN ._.
It_PRP refers_VBZ to_TO determining_VBG the_DT opinions_NNS or_CC sentiments_NNS expressed_VBN on_IN different_JJ features_NNS or_CC aspects_NNS of_IN entities_NNS ,_, e.g._FW ,_, of_IN a_DT cell_NN phone_NN ,_, a_DT digital_JJ camera_NN ,_, or_CC a_DT bank_NN ._.
A_DT feature_NN or_CC aspect_NN is_VBZ an_DT attribute_NN or_CC component_NN of_IN an_DT entity_NN ,_, e.g._FW ,_, the_DT screen_NN of_IN a_DT cell_NN phone_NN ,_, or_CC the_DT picture_NN quality_NN of_IN a_DT camera_NN ._.
This_DT problem_NN involves_VBZ several_JJ sub-problems_NNS ,_, e.g._FW ,_, identifying_VBG relevant_JJ entities_NNS ,_, extracting_VBG their_PRP$ features\/aspects_NNS ,_, and_CC determining_VBG whether_IN an_DT opinion_NN expressed_VBN on_IN each_DT feature\/aspect_NN is_VBZ positive_JJ ,_, negative_JJ or_CC neutral_JJ ._.
More_RBR detailed_JJ discussions_NNS about_IN this_DT level_NN of_IN sentiment_NN analysis_NN can_MD be_VB found_VBN in_IN Liu_NNP 's_POS NLP_NNP Handbook_NNP chapter_NN ,_, ``_`` Sentiment_NN Analysis_NN and_CC Subjectivity_NN ''_'' ._.
Methods_NNS Computers_NNP can_MD perform_VB automated_JJ sentiment_NN analysis_NN of_IN digital_JJ texts_NNS ,_, using_VBG elements_NNS from_IN machine_NN learning_NN such_JJ as_IN latent_JJ semantic_JJ analysis_NN ,_, support_NN vector_NN machines_NNS ,_, ``_`` bag_NN of_IN words_NNS ''_'' and_CC Semantic_NNP Orientation_NNP --_: Pointwise_NNP Mutual_NNP Information_NNP -LRB-_-LRB- See_NNP Peter_NNP Turney_NNP 's_POS work_NN in_IN this_DT area_NN -RRB-_-RRB- ._.
More_RBR sophisticated_JJ methods_NNS try_VBP to_TO detect_VB the_DT holder_NN of_IN a_DT sentiment_NN -LRB-_-LRB- i.e._FW the_DT person_NN who_WP maintains_VBZ that_IN affective_JJ state_NN -RRB-_-RRB- and_CC the_DT target_NN -LRB-_-LRB- i.e._FW the_DT entity_NN about_IN which_WDT the_DT affect_NN is_VBZ felt_VBN -RRB-_-RRB- ._.
To_TO mine_VB the_DT opinion_NN in_IN context_NN and_CC get_VB the_DT feature_NN which_WDT has_VBZ been_VBN opinionated_VBN ,_, the_DT grammatical_JJ relationships_NNS of_IN words_NNS are_VBP used_VBN ._.
Grammatical_JJ dependency_NN relations_NNS are_VBP obtained_VBN by_IN deep_JJ parsing_NN of_IN the_DT text_NN ._.
Open_VB source_NN software_NN tools_NNS deploy_VBP machine_NN learning_NN ,_, statistics_NNS ,_, and_CC natural_JJ language_NN processing_NN techniques_NNS to_TO automate_VB sentiment_NN analysis_NN on_IN large_JJ collections_NNS of_IN texts_NNS ,_, including_VBG web_NN pages_NNS ,_, online_JJ news_NN ,_, internet_NN discussion_NN groups_NNS ,_, online_JJ reviews_NNS ,_, web_NN blogs_NNS ,_, and_CC social_JJ media_NNS ._.
Evaluation_NN The_DT accuracy_NN of_IN a_DT sentiment_NN analysis_NN system_NN is_VBZ ,_, in_IN principle_NN ,_, how_WRB well_RB it_PRP agrees_VBZ with_IN human_JJ judgments_NNS ._.
This_DT is_VBZ usually_RB measured_VBN by_IN precision_NN and_CC recall_NN ._.
However_RB ,_, human_JJ raters_NNS typically_RB agree_VBP about_IN 70_CD %_NN -LRB-_-LRB- citation_NN needed_VBN -RRB-_-RRB- of_IN the_DT time_NN -LRB-_-LRB- see_VB Inter-rater_JJ reliability_NN -RRB-_-RRB- ._.
Thus_RB ,_, a_DT 70_CD %_NN accurate_JJ program_NN is_VBZ doing_VBG as_RB well_RB as_IN humans_NNS ,_, even_RB though_IN such_JJ accuracy_NN may_MD not_RB sound_VB impressive_JJ ._.
If_IN a_DT program_NN were_VBD ``_`` right_JJ ''_'' 100_CD %_NN of_IN the_DT time_NN ,_, humans_NNS would_MD still_RB disagree_VB with_IN it_PRP about_IN 30_CD %_NN of_IN the_DT time_NN ,_, since_IN they_PRP disagree_VBP that_IN much_JJ about_IN any_DT answer_NN ._.
More_RBR sophisticated_JJ measures_NNS can_MD be_VB applied_VBN ,_, but_CC evaluation_NN of_IN sentiment_NN analysis_NN systems_NNS remains_VBZ a_DT complex_JJ matter_NN ._.
For_IN sentiment_NN analysis_NN tasks_NNS returning_VBG a_DT scale_NN rather_RB than_IN a_DT binary_JJ judgement_NN ,_, correlation_NN is_VBZ a_DT better_JJR measure_NN than_IN precision_NN because_IN it_PRP takes_VBZ into_IN account_NN how_WRB close_JJ the_DT predicted_VBN value_NN is_VBZ to_TO the_DT target_NN value_NN ._.
Sentiment_NN analysis_NN was_VBD used_VBN to_TO test_VB the_DT relationship_NN between_IN Internet_NNP financial_JJ message_NN boards_NNS and_CC the_DT behavior_NN of_IN the_DT stock_NN market_NN to_TO find_VB a_DT strong_JJ correlation_NN between_IN posts_NNS and_CC volume_NN of_IN stock_NN ._.
Sentiment_NN analysis_NN and_CC Web_NN 2.0_CD The_DT rise_NN of_IN social_JJ media_NNS such_JJ as_IN blogs_NNS and_CC social_JJ networks_NNS has_VBZ fueled_VBN interest_NN in_IN sentiment_NN analysis_NN ._.
With_IN the_DT proliferation_NN of_IN reviews_NNS ,_, ratings_NNS ,_, recommendations_NNS and_CC other_JJ forms_NNS of_IN online_NN expression_NN ,_, online_JJ opinion_NN has_VBZ turned_VBN into_IN a_DT kind_NN of_IN virtual_JJ currency_NN for_IN businesses_NNS looking_VBG to_TO market_VB their_PRP$ products_NNS ,_, identify_VBP new_JJ opportunities_NNS and_CC manage_VBP their_PRP$ reputations_NNS ._.
As_IN businesses_NNS look_VBP to_TO automate_VB the_DT process_NN of_IN filtering_VBG out_RP the_DT noise_NN ,_, understanding_VBG the_DT conversations_NNS ,_, identifying_VBG the_DT relevant_JJ content_NN and_CC actioning_VBG it_PRP appropriately_RB ,_, many_JJ are_VBP now_RB looking_VBG to_TO the_DT field_NN of_IN sentiment_NN analysis_NN ._.
If_IN web_NN 2.0_CD was_VBD all_DT about_IN democratizing_VBG publishing_NN ,_, then_RB the_DT next_JJ stage_NN of_IN the_DT web_NN may_MD well_RB be_VB based_VBN on_IN democratizing_VBG data_NNS mining_NN of_IN all_PDT the_DT content_NN that_WDT is_VBZ getting_VBG published_VBN ._.
One_CD step_NN towards_IN this_DT aim_NN is_VBZ accomplished_VBN in_IN research_NN ._.
Several_JJ research_NN teams_NNS in_IN universities_NNS around_IN the_DT world_NN currently_RB focus_VB on_IN understanding_VBG the_DT dynamics_NNS of_IN sentiment_NN in_IN e-communities_NNS through_IN sentiment_NN analysis_NN ._.
The_DT CyberEmotions_NNP project_NN ,_, for_IN instance_NN ,_, recently_RB identified_VBD the_DT role_NN of_IN negative_JJ emotions_NNS in_IN driving_VBG social_JJ networks_NNS discussions_NNS ._.
Sentiment_NN analysis_NN could_MD therefore_RB help_VB understand_VB why_WRB certain_JJ e-communities_NNS die_VB or_CC fade_VB away_RB -LRB-_-LRB- e.g._FW ,_, MySpace_NNP -RRB-_-RRB- while_IN others_NNS seem_VBP to_TO grow_VB without_IN limits_NNS -LRB-_-LRB- e.g._FW ,_, Facebook_NNP -RRB-_-RRB- ._.
The_DT problem_NN is_VBZ that_IN most_JJS sentiment_NN analysis_NN algorithms_NNS use_VBP simple_JJ terms_NNS to_TO express_VB sentiment_NN about_IN a_DT product_NN or_CC service_NN ._.
However_RB ,_, cultural_JJ factors_NNS ,_, linguistic_JJ nuances_NNS and_CC differing_VBG contexts_NNS make_VBP it_PRP extremely_RB difficult_JJ to_TO turn_VB a_DT string_NN of_IN written_VBN text_NN into_IN a_DT simple_JJ pro_NN or_CC con_NN sentiment_NN ._.
The_DT fact_NN that_IN humans_NNS often_RB disagree_VBP on_IN the_DT sentiment_NN of_IN text_NN illustrates_VBZ how_WRB big_JJ a_DT task_NN it_PRP is_VBZ for_IN computers_NNS to_TO get_VB this_DT right_NN ._.
The_DT shorter_JJR the_DT string_NN of_IN text_NN ,_, the_DT harder_JJR it_PRP becomes_VBZ ._.
n_NN Computer_NN Science_NN ,_, Speech_NN recognition_NN is_VBZ the_DT translation_NN of_IN spoken_VBN words_NNS into_IN text_NN ._.
It_PRP is_VBZ also_RB known_VBN as_IN ``_`` automatic_JJ speech_NN recognition_NN ''_'' ,_, ``_`` ASR_NNP ''_'' ,_, ``_`` computer_NN speech_NN recognition_NN ''_'' ,_, ``_`` speech_NN to_TO text_NN ''_'' ,_, or_CC just_RB ``_`` STT_NNP ''_'' ._.
Speech_NN Recognition_NN is_VBZ technology_NN that_WDT can_MD translate_VB spoken_VBN words_NNS into_IN text_NN ._.
Some_DT SR_NN systems_NNS use_VBP ``_`` training_NN ''_'' where_WRB an_DT individual_JJ speaker_NN reads_VBZ sections_NNS of_IN text_NN into_IN the_DT SR_NN system_NN ._.
These_DT systems_NNS analyze_VBP the_DT person_NN 's_POS specific_JJ voice_NN and_CC use_VB it_PRP to_TO fine_RB tune_VB the_DT recognition_NN of_IN that_DT person_NN 's_POS speech_NN ,_, resulting_VBG in_IN more_RBR accurate_JJ transcription_NN ._.
Systems_NNP that_WDT do_VBP not_RB use_VB training_NN are_VBP called_VBN ``_`` Speaker_NNP Independent_NNP ''_'' systems_NNS ._.
Systems_NNP that_WDT use_VBP training_NN are_VBP called_VBN ``_`` Speaker_NNP Dependent_NNP ''_'' systems_NNS ._.
Speech_NN recognition_NN applications_NNS include_VBP voice_NN user_NN interfaces_NNS such_JJ as_IN voice_NN dialing_NN -LRB-_-LRB- e.g._FW ,_, ``_`` Call_NN home_NN ''_'' -RRB-_-RRB- ,_, call_VB routing_VBG -LRB-_-LRB- e.g._FW ,_, ``_`` I_PRP would_MD like_VB to_TO make_VB a_DT collect_VB call_NN ''_'' -RRB-_-RRB- ,_, domotic_JJ appliance_NN control_NN ,_, search_NN -LRB-_-LRB- e.g._FW ,_, find_VB a_DT podcast_NN where_WRB particular_JJ words_NNS were_VBD spoken_VBN -RRB-_-RRB- ,_, simple_JJ data_NNS entry_NN -LRB-_-LRB- e.g._FW ,_, entering_VBG a_DT credit_NN card_NN number_NN -RRB-_-RRB- ,_, preparation_NN of_IN structured_JJ documents_NNS -LRB-_-LRB- e.g._FW ,_, a_DT radiology_NN report_NN -RRB-_-RRB- ,_, speech-to-text_JJ processing_NN -LRB-_-LRB- e.g._FW ,_, word_NN processors_NNS or_CC emails_NNS -RRB-_-RRB- ,_, and_CC aircraft_NN -LRB-_-LRB- usually_RB termed_VBN Direct_NNP Voice_NNP Input_NNP -RRB-_-RRB- ._.
The_DT term_NN voice_NN recognition_NN refers_VBZ to_TO finding_VBG the_DT identity_NN of_IN ``_`` who_WP ''_'' is_VBZ speaking_VBG ,_, rather_RB than_IN what_WP they_PRP are_VBP saying_VBG ._.
Recognizing_VBG the_DT speaker_NN can_MD simplify_VB the_DT task_NN of_IN translating_VBG speech_NN in_IN systems_NNS that_WDT have_VBP been_VBN trained_VBN on_IN specific_JJ person_NN 's_POS voices_NNS or_CC it_PRP can_MD be_VB used_VBN to_TO authenticate_VB or_CC verify_VB the_DT identity_NN of_IN a_DT speaker_NN as_IN part_NN of_IN a_DT security_NN process_NN ._.
Front-End_JJ speech_NN recognition_NN is_VBZ where_WRB the_DT provider_NN dictates_VBZ into_IN a_DT speech-recognition_JJ engine_NN ,_, the_DT recognized_VBN words_NNS are_VBP displayed_VBN as_IN they_PRP are_VBP spoken_VBN ,_, and_CC the_DT dictator_NN is_VBZ responsible_JJ for_IN editing_NN and_CC signing_VBG off_RP on_IN the_DT document_NN ._.
Back-End_JJ or_CC deferred_JJ speech_NN recognition_NN is_VBZ where_WRB the_DT provider_NN dictates_VBZ into_IN a_DT digital_JJ dictation_NN system_NN ,_, the_DT voice_NN is_VBZ routed_VBN through_IN a_DT speech-recognition_JJ machine_NN and_CC the_DT recognized_VBN draft_NN document_NN is_VBZ routed_VBN along_IN with_IN the_DT original_JJ voice_NN file_NN to_TO the_DT editor_NN ,_, where_WRB the_DT draft_NN is_VBZ edited_VBN and_CC report_NN finalized_VBD ._.
Deferred_JJ speech_NN recognition_NN is_VBZ widely_RB used_VBN in_IN the_DT industry_NN currently_RB ._.
Many_JJ Electronic_NNP Medical_NNP Records_NNP -LRB-_-LRB- EMR_NNP -RRB-_-RRB- applications_NNS can_MD be_VB more_RBR effective_JJ and_CC may_MD be_VB performed_VBN more_RBR easily_RB when_WRB deployed_VBN in_IN conjunction_NN with_IN a_DT speech-recognition_JJ engine_NN ._.
Searches_NNS ,_, queries_NNS ,_, and_CC form_NN filling_NN may_MD all_DT be_VB faster_JJR to_TO perform_VB by_IN voice_NN than_IN by_IN using_VBG a_DT keyboard_NN ._.
One_CD of_IN the_DT major_JJ issues_NNS relating_VBG to_TO the_DT use_NN of_IN speech_NN recognition_NN in_IN healthcare_NN is_VBZ that_IN the_DT American_NNP Recovery_NNP and_CC Reinvestment_NNP Act_NNP of_IN 2009_CD -LRB-_-LRB- ARRA_NN -RRB-_-RRB- provides_VBZ for_IN substantial_JJ financial_JJ benefits_NNS to_TO physicians_NNS who_WP utilize_VBP an_DT EMR_NN according_VBG to_TO ``_`` Meaningful_JJ Use_NN ''_'' standards_NNS ._.
These_DT standards_NNS require_VBP that_IN a_DT substantial_JJ amount_NN of_IN data_NNS be_VB maintained_VBN by_IN the_DT EMR_NN -LRB-_-LRB- now_RB more_RBR commonly_RB referred_VBN to_TO as_IN an_DT Electronic_NNP Health_NNP Record_NNP or_CC EHR_NNP -RRB-_-RRB- ._.
Unfortunately_RB ,_, in_IN many_JJ instances_NNS ,_, the_DT use_NN of_IN speech_NN recognition_NN within_IN an_DT EHR_NN will_MD not_RB lead_VB to_TO data_NNS maintained_VBN within_IN a_DT database_NN ,_, but_CC rather_RB to_TO narrative_JJ text_NN ._.
For_IN this_DT reason_NN ,_, substantial_JJ resources_NNS are_VBP being_VBG expended_VBN to_TO allow_VB for_IN the_DT use_NN of_IN front-end_JJ SR_NN while_IN capturing_VBG data_NNS within_IN the_DT EHR_NN ._.
Military_JJ High-performance_JJ fighter_NN aircraft_NN Substantial_JJ efforts_NNS have_VBP been_VBN devoted_VBN in_IN the_DT last_JJ decade_NN to_TO the_DT test_NN and_CC evaluation_NN of_IN speech_NN recognition_NN in_IN fighter_NN aircraft_NN ._.
Of_IN particular_JJ note_NN is_VBZ the_DT U.S._NNP program_NN in_IN speech_NN recognition_NN for_IN the_DT Advanced_NNP Fighter_NNP Technology_NNP Integration_NNP -LRB-_-LRB- AFTI_NNP -RRB-_-RRB- \/_: F-16_NN aircraft_NN -LRB-_-LRB- F-16_NN VISTA_NN -RRB-_-RRB- ,_, and_CC a_DT program_NN in_IN France_NNP installing_VBG speech_NN recognition_NN systems_NNS on_IN Mirage_NNP aircraft_NN ,_, and_CC also_RB programs_NNS in_IN the_DT UK_NNP dealing_VBG with_IN a_DT variety_NN of_IN aircraft_NN platforms_NNS ._.
In_IN these_DT programs_NNS ,_, speech_NN recognizers_NNS have_VBP been_VBN operated_VBN successfully_RB in_IN fighter_NN aircraft_NN ,_, with_IN applications_NNS including_VBG :_: setting_VBG radio_NN frequencies_NNS ,_, commanding_VBG an_DT autopilot_NN system_NN ,_, setting_VBG steer-point_NN coordinates_VBZ and_CC weapons_NNS release_NN parameters_NNS ,_, and_CC controlling_VBG flight_NN displays_NNS ._.
Working_VBG with_IN Swedish_JJ pilots_NNS flying_VBG in_IN the_DT JAS-39_NNP Gripen_NNP cockpit_NN ,_, Englund_NNP -LRB-_-LRB- 2004_CD -RRB-_-RRB- found_VBD recognition_NN deteriorated_JJ with_IN increasing_VBG G-loads_NNS ._.
It_PRP was_VBD also_RB concluded_VBN that_IN adaptation_NN greatly_RB improved_VBD the_DT results_NNS in_IN all_DT cases_NNS and_CC introducing_VBG models_NNS for_IN breathing_NN was_VBD shown_VBN to_TO improve_VB recognition_NN scores_NNS significantly_RB ._.
Contrary_JJ to_TO what_WP might_MD be_VB expected_VBN ,_, no_DT effects_NNS of_IN the_DT broken_JJ English_NNP of_IN the_DT speakers_NNS were_VBD found_VBN ._.
It_PRP was_VBD evident_JJ that_IN spontaneous_JJ speech_NN caused_VBD problems_NNS for_IN the_DT recognizer_NN ,_, as_IN could_MD be_VB expected_VBN ._.
A_DT restricted_JJ vocabulary_NN ,_, and_CC above_IN all_DT ,_, a_DT proper_JJ syntax_NN ,_, could_MD thus_RB be_VB expected_VBN to_TO improve_VB recognition_NN accuracy_NN substantially_RB ._.
The_DT Eurofighter_NNP Typhoon_NNP currently_RB in_IN service_NN with_IN the_DT UK_NNP RAF_NN employs_VBZ a_DT speaker-dependent_JJ system_NN ,_, i.e._FW it_PRP requires_VBZ each_DT pilot_NN to_TO create_VB a_DT template_NN ._.
The_DT system_NN is_VBZ not_RB used_VBN for_IN any_DT safety_NN critical_JJ or_CC weapon_NN critical_JJ tasks_NNS ,_, such_JJ as_IN weapon_NN release_NN or_CC lowering_NN of_IN the_DT undercarriage_NN ,_, but_CC is_VBZ used_VBN for_IN a_DT wide_JJ range_NN of_IN other_JJ cockpit_NN functions_NNS ._.
Voice_NNP commands_NNS are_VBP confirmed_VBN by_IN visual_JJ and\/or_CC aural_JJ feedback_NN ._.
The_DT system_NN is_VBZ seen_VBN as_IN a_DT major_JJ design_NN feature_NN in_IN the_DT reduction_NN of_IN pilot_NN workload_NN ,_, and_CC even_RB allows_VBZ the_DT pilot_NN to_TO assign_VB targets_NNS to_TO himself_PRP with_IN two_CD simple_JJ voice_NN commands_NNS or_CC to_TO any_DT of_IN his_PRP$ wingmen_NN with_IN only_RB five_CD commands_NNS ._.
Speaker_NNP independent_JJ systems_NNS are_VBP also_RB being_VBG developed_VBN and_CC are_VBP in_IN testing_NN for_IN the_DT F35_NN Lightning_NN II_CD -LRB-_-LRB- JSF_NN -RRB-_-RRB- and_CC the_DT Alenia_NNP Aermacchi_NNP M-346_NNP Master_NNP lead-in_NN fighter_NN trainer_NN ._.
These_DT systems_NNS have_VBP produced_VBN word_NN accuracies_NNS in_IN excess_NN of_IN 98_CD %_NN ._.
Helicopters_NNPS The_DT problems_NNS of_IN achieving_VBG high_JJ recognition_NN accuracy_NN under_IN stress_NN and_CC noise_NN pertain_NN strongly_RB to_TO the_DT helicopter_NN environment_NN as_RB well_RB as_IN to_TO the_DT jet_NN fighter_NN environment_NN ._.
The_DT acoustic_JJ noise_NN problem_NN is_VBZ actually_RB more_RBR severe_JJ in_IN the_DT helicopter_NN environment_NN ,_, not_RB only_RB because_IN of_IN the_DT high_JJ noise_NN levels_NNS but_CC also_RB because_IN the_DT helicopter_NN pilot_NN ,_, in_IN general_JJ ,_, does_VBZ not_RB wear_VB a_DT facemask_NN ,_, which_WDT would_MD reduce_VB acoustic_JJ noise_NN in_IN the_DT microphone_NN ._.
Substantial_JJ test_NN and_CC evaluation_NN programs_NNS have_VBP been_VBN carried_VBN out_RP in_IN the_DT past_JJ decade_NN in_IN speech_NN recognition_NN systems_NNS applications_NNS in_IN helicopters_NNS ,_, notably_RB by_IN the_DT U.S._NNP Army_NNP Avionics_NNP Research_NNP and_CC Development_NNP Activity_NNP -LRB-_-LRB- AVRADA_NNP -RRB-_-RRB- and_CC by_IN the_DT Royal_NNP Aerospace_NNP Establishment_NN -LRB-_-LRB- RAE_NN -RRB-_-RRB- in_IN the_DT UK_NNP ._.
Work_NN in_IN France_NNP has_VBZ included_VBN speech_NN recognition_NN in_IN the_DT Puma_NNP helicopter_NN ._.
There_EX has_VBZ also_RB been_VBN much_JJ useful_JJ work_NN in_IN Canada_NNP ._.
Results_NNS have_VBP been_VBN encouraging_JJ ,_, and_CC voice_NN applications_NNS have_VBP included_VBN :_: control_NN of_IN communication_NN radios_NNS ,_, setting_VBG of_IN navigation_NN systems_NNS ,_, and_CC control_NN of_IN an_DT automated_JJ target_NN handover_NN system_NN ._.
As_IN in_IN fighter_NN applications_NNS ,_, the_DT overriding_VBG issue_NN for_IN voice_NN in_IN helicopters_NNS is_VBZ the_DT impact_NN on_IN pilot_NN effectiveness_NN ._.
Encouraging_VBG results_NNS are_VBP reported_VBN for_IN the_DT AVRADA_NNP tests_NNS ,_, although_IN these_DT represent_VBP only_RB a_DT feasibility_NN demonstration_NN in_IN a_DT test_NN environment_NN ._.
Much_RB remains_VBZ to_TO be_VB done_VBN both_CC in_IN speech_NN recognition_NN and_CC in_IN overall_JJ speech_NN recognition_NN technology_NN ,_, in_IN order_NN to_TO consistently_RB achieve_VB performance_NN improvements_NNS in_IN operational_JJ settings_NNS ._.
Battle_NN management_NN Question_NN book-new_NN ._.
svg_IN This_DT unreferenced_JJ section_NN requires_VBZ citations_NNS to_TO ensure_VB verifiability_NN ._.
In_IN general_JJ ,_, Battle_NNP Management_NNP command_NN centres_NNS require_VBP rapid_JJ access_NN to_TO and_CC control_NN of_IN large_JJ ,_, rapidly_RB changing_VBG information_NN databases_NNS ._.
Commanders_NNS and_CC system_NN operators_NNS need_VBP to_TO query_VB these_DT databases_NNS as_RB conveniently_RB as_IN possible_JJ ,_, in_IN an_DT eyes-busy_JJ environment_NN where_WRB much_RB of_IN the_DT information_NN is_VBZ presented_VBN in_IN a_DT display_NN format_NN ._.
Human-machine_JJ interaction_NN by_IN voice_NN has_VBZ the_DT potential_NN to_TO be_VB very_RB useful_JJ in_IN these_DT environments_NNS ._.
A_DT number_NN of_IN efforts_NNS have_VBP been_VBN undertaken_VBN to_TO interface_NN commercially_RB available_JJ isolated-word_JJ recognizers_NNS into_IN battle_NN management_NN environments_NNS ._.
In_IN one_CD feasibility_NN study_NN ,_, speech_NN recognition_NN equipment_NN was_VBD tested_VBN in_IN conjunction_NN with_IN an_DT integrated_VBN information_NN display_NN for_IN naval_JJ battle_NN management_NN applications_NNS ._.
Users_NNS were_VBD very_RB optimistic_JJ about_IN the_DT potential_NN of_IN the_DT system_NN ,_, although_IN capabilities_NNS were_VBD limited_VBN ._.
Speech_NN understanding_NN programs_NNS sponsored_VBN by_IN the_DT Defense_NNP Advanced_NNP Research_NNP Projects_NNP Agency_NNP -LRB-_-LRB- DARPA_NNP -RRB-_-RRB- in_IN the_DT U.S._NNP has_VBZ focused_VBN on_IN this_DT problem_NN of_IN natural_JJ speech_NN interface_NN ._.
Speech_NN recognition_NN efforts_NNS have_VBP focused_VBN on_IN a_DT database_NN of_IN continuous_JJ speech_NN recognition_NN -LRB-_-LRB- CSR_NN -RRB-_-RRB- ,_, large-vocabulary_JJ speech_NN designed_VBN to_TO be_VB representative_JJ of_IN the_DT naval_JJ resource_NN management_NN task_NN ._.
Significant_JJ advances_NNS in_IN the_DT state-of-the-art_JJ in_IN CSR_NNP have_VBP been_VBN achieved_VBN ,_, and_CC current_JJ efforts_NNS are_VBP focused_VBN on_IN integrating_VBG speech_NN recognition_NN and_CC natural_JJ language_NN processing_NN to_TO allow_VB spoken_VBN language_NN interaction_NN with_IN a_DT naval_JJ resource_NN management_NN system_NN ._.
Training_NN air_NN traffic_NN controllers_NNS Training_VBG for_IN air_NN traffic_NN controllers_NNS -LRB-_-LRB- ATC_NN -RRB-_-RRB- represents_VBZ an_DT excellent_JJ application_NN for_IN speech_NN recognition_NN systems_NNS ._.
Many_JJ ATC_NN training_NN systems_NNS currently_RB require_VBP a_DT person_NN to_TO act_VB as_IN a_DT ``_`` pseudo-pilot_NN ''_'' ,_, engaging_VBG in_IN a_DT voice_NN dialog_NN with_IN the_DT trainee_NN controller_NN ,_, which_WDT simulates_VBZ the_DT dialog_NN that_IN the_DT controller_NN would_MD have_VB to_TO conduct_VB with_IN pilots_NNS in_IN a_DT real_JJ ATC_NN situation_NN ._.
Speech_NN recognition_NN and_CC synthesis_NN techniques_NNS offer_VBP the_DT potential_NN to_TO eliminate_VB the_DT need_NN for_IN a_DT person_NN to_TO act_VB as_IN pseudo-pilot_NN ,_, thus_RB reducing_VBG training_NN and_CC support_NN personnel_NNS ._.
In_IN theory_NN ,_, Air_NN controller_NN tasks_NNS are_VBP also_RB characterized_VBN by_IN highly_RB structured_VBN speech_NN as_IN the_DT primary_JJ output_NN of_IN the_DT controller_NN ,_, hence_RB reducing_VBG the_DT difficulty_NN of_IN the_DT speech_NN recognition_NN task_NN should_MD be_VB possible_JJ ._.
In_IN practice_NN ,_, this_DT is_VBZ rarely_RB the_DT case_NN ._.
The_DT FAA_NNP document_NN 7110.65_CD details_NNS the_DT phrases_NNS that_WDT should_MD be_VB used_VBN by_IN air_NN traffic_NN controllers_NNS ._.
While_IN this_DT document_NN gives_VBZ less_JJR than_IN 150_CD examples_NNS of_IN such_JJ phrases_NNS ,_, the_DT number_NN of_IN phrases_NNS supported_VBN by_IN one_CD of_IN the_DT simulation_NN vendors_NNS speech_NN recognition_NN systems_NNS is_VBZ in_IN excess_NN of_IN 500,000_CD ._.
The_DT USAF_NNP ,_, USMC_NNP ,_, US_NNP Army_NNP ,_, US_NNP Navy_NNP ,_, and_CC FAA_NNP as_RB well_RB as_IN a_DT number_NN of_IN international_JJ ATC_NN training_NN organizations_NNS such_JJ as_IN the_DT Royal_NNP Australian_NNP Air_NNP Force_NNP and_CC Civil_NNP Aviation_NNP Authorities_NNP in_IN Italy_NNP ,_, Brazil_NNP ,_, and_CC Canada_NNP are_VBP currently_RB using_VBG ATC_NN simulators_NNS with_IN speech_NN recognition_NN from_IN a_DT number_NN of_IN different_JJ vendors_NNS ._.
Telephony_NNP and_CC other_JJ domains_NNS ASR_NNP in_IN the_DT field_NN of_IN telephony_NN is_VBZ now_RB commonplace_JJ and_CC in_IN the_DT field_NN of_IN computer_NN gaming_NN and_CC simulation_NN is_VBZ becoming_VBG more_RBR widespread_JJ ._.
Despite_IN the_DT high_JJ level_NN of_IN integration_NN with_IN word_NN processing_NN in_IN general_JJ personal_JJ computing_NN ._.
However_RB ,_, ASR_NNP in_IN the_DT field_NN of_IN document_NN production_NN has_VBZ not_RB seen_VBN the_DT expected_VBN -LRB-_-LRB- by_IN whom_WP ?_. -RRB-_-RRB-
increases_NNS in_IN use_NN ._.
The_DT improvement_NN of_IN mobile_JJ processor_NN speeds_NNS made_VBD feasible_JJ the_DT speech-enabled_JJ Symbian_NNP and_CC Windows_NNP Mobile_NNP Smartphones_NNPS ._.
Speech_NN is_VBZ used_VBN mostly_RB as_IN a_DT part_NN of_IN User_NN Interface_NNP ,_, for_IN creating_VBG pre-defined_JJ or_CC custom_NN speech_NN commands_NNS ._.
Leading_VBG software_NN vendors_NNS in_IN this_DT field_NN are_VBP :_: Microsoft_NNP Corporation_NNP -LRB-_-LRB- Microsoft_NNP Voice_NNP Command_NN -RRB-_-RRB- ,_, Digital_NNP Syphon_NNP -LRB-_-LRB- Sonic_NNP Extractor_NNP -RRB-_-RRB- ,_, Nuance_NNP Communications_NNP -LRB-_-LRB- Nuance_NNP Voice_NNP Control_NNP -RRB-_-RRB- ,_, Speech_NNP Technology_NNP Center_NNP ,_, Vito_NNP Technology_NNP -LRB-_-LRB- VITO_NNP Voice2Go_NN -RRB-_-RRB- ,_, Speereo_NNP Software_NNP -LRB-_-LRB- Speereo_NNP Voice_NNP Translator_NNP -RRB-_-RRB- ,_, Verbyx_NNP VRX_NNP and_CC SVOX_NNP ._.
Further_JJ applications_NNS Aerospace_NNP -LRB-_-LRB- e.g._FW space_NN exploration_NN ,_, spacecraft_NN ,_, etc._NN -RRB-_-RRB- NASA_NNP 's_POS Mars_NNP Polar_NNP Lander_NNP used_VBD speech_NN recognition_NN from_IN technology_NN Sensory_NNP ,_, Inc._NNP in_IN the_DT Mars_NNP Microphone_NNP on_IN the_DT Lander_NNP Automatic_NNP translation_NN Automotive_NNP speech_NN recognition_NN -LRB-_-LRB- e.g._FW ,_, OnStar_NNP ,_, Ford_NNP Sync_NNP -RRB-_-RRB- Court_NNP reporting_NN -LRB-_-LRB- Realtime_NN Speech_NN Writing_VBG -RRB-_-RRB- Hands-free_JJ computing_NN :_: Speech_NN recognition_NN computer_NN user_NN interface_NN Home_NN automation_NN Interactive_JJ voice_NN response_NN Mobile_NNP telephony_NN ,_, including_VBG mobile_JJ email_JJ Multimodal_JJ interaction_NN Pronunciation_NN evaluation_NN in_IN computer-aided_JJ language_NN learning_VBG applications_NNS Robotics_NNP Speech-to-text_JJ reporter_NN -LRB-_-LRB- transcription_NN of_IN speech_NN into_IN text_NN ,_, video_NN captioning_NN ,_, Court_NNP reporting_NN -RRB-_-RRB- Telematics_NNS -LRB-_-LRB- e.g._FW ,_, vehicle_NN Navigation_NNP Systems_NNP -RRB-_-RRB- Transcription_NN -LRB-_-LRB- digital_JJ speech-to-text_NN -RRB-_-RRB- Video_NNP games_NNS ,_, with_IN Tom_NNP Clancy_NNP 's_POS EndWar_NNP and_CC Lifeline_NNP as_IN working_VBG examples_NNS Performance_NNP The_DT performance_NN of_IN speech_NN recognition_NN systems_NNS is_VBZ usually_RB evaluated_VBN in_IN terms_NNS of_IN accuracy_NN and_CC speed_NN ._.
Accuracy_NN is_VBZ usually_RB rated_VBN with_IN word_NN error_NN rate_NN -LRB-_-LRB- WER_NN -RRB-_-RRB- ,_, whereas_IN speed_NN is_VBZ measured_VBN with_IN the_DT real_JJ time_NN factor_NN ._.
Other_JJ measures_NNS of_IN accuracy_NN include_VBP Single_JJ Word_NN Error_NN Rate_NN -LRB-_-LRB- SWER_NN -RRB-_-RRB- and_CC Command_NN Success_NN Rate_NN -LRB-_-LRB- CSR_NN -RRB-_-RRB- ._.
However_RB ,_, speech_NN recognition_NN -LRB-_-LRB- by_IN a_DT machine_NN -RRB-_-RRB- is_VBZ a_DT very_RB complex_JJ problem_NN ._.
Vocalizations_NNS vary_VBP in_IN terms_NNS of_IN accent_NN ,_, pronunciation_NN ,_, articulation_NN ,_, roughness_NN ,_, nasality_NN ,_, pitch_NN ,_, volume_NN ,_, and_CC speed_NN ._.
Speech_NN is_VBZ distorted_VBN by_IN a_DT background_NN noise_NN and_CC echoes_NNS ,_, electrical_JJ characteristics_NNS ._.
Accuracy_NN of_IN speech_NN recognition_NN vary_VBP with_IN the_DT following_NN :_: Vocabulary_NN size_NN and_CC confusability_NN Speaker_NNP dependence_NN vs._NN independence_NN Isolated_VBN ,_, discontinuous_JJ ,_, or_CC continuous_JJ speech_NN Task_NNP and_CC language_NN constraints_NNS Read_VBP vs._FW spontaneous_JJ speech_NN Adverse_JJ conditions_NNS Accuracy_NN of_IN speech_NN recognition_NN As_IN mentioned_VBN earlier_RBR in_IN this_DT article_NN accuracy_NN of_IN speech_NN recogniton_NN vary_VBP in_IN following_VBG :_: Error_NNP Rates_NNP Increase_NN as_IN the_DT Vocabulary_NN Size_NN Grows_VBZ :_: e.g._FW The_DT 10_CD digits_NNS ``_`` zero_NN ''_'' to_TO ``_`` nine_CD ''_'' can_MD be_VB recognized_VBN essentially_RB perfectly_RB ,_, but_CC vocabulary_NN sizes_NNS of_IN 200_CD ,_, 5000_CD or_CC 100000_CD may_MD have_VB error_NN rates_NNS of_IN 3_CD %_NN ,_, 7_CD %_NN or_CC 45_CD %_NN ._.
Vocabulary_NN is_VBZ Hard_NNP to_TO Recognize_VB if_IN it_PRP Contains_VBZ Confusable_JJ Words_NNS :_: e.g._FW The_DT 26_CD letters_NNS of_IN the_DT English_JJ alphabet_NN are_VBP difficult_JJ to_TO discriminate_VB because_IN they_PRP are_VBP confusable_JJ words_NNS -LRB-_-LRB- most_RBS notoriously_RB ,_, the_DT E-set_NN :_: ``_`` B_NN ,_, C_NN ,_, D_NN ,_, E_NN ,_, G_NN ,_, P_NN ,_, T_NN ,_, V_NN ,_, Z_NN ''_'' -RRB-_-RRB- ;_: An_DT 8_CD %_NN error_NN rate_NN is_VBZ considered_VBN good_JJ for_IN this_DT vocabulary_NN ._.
Speaker_NNP Dependence_NN vs._NN Independence_NN :_: A_DT speaker_NN dependent_JJ system_NN is_VBZ intended_VBN for_IN use_NN by_IN a_DT single_JJ speaker_NN ._.
A_DT speaker_NN independent_JJ system_NN is_VBZ intended_VBN for_IN use_NN by_IN any_DT speaker_NN ,_, more_RBR difficult_JJ ._.
Isolated_JJ ,_, Discontinuous_JJ or_CC Continuous_JJ speech_NN With_IN isolated_VBN speech_NN single_JJ words_NNS are_VBP used_VBN ,_, therefore_RB it_PRP becomes_VBZ easier_JJR to_TO recognize_VB the_DT speech_NN ._.
With_IN discontinuous_JJ speech_NN full_RB sentenced_VBD separated_VBN by_IN silence_NN are_VBP used_VBN ,_, therefore_RB it_PRP becomes_VBZ easier_JJR to_TO recognize_VB the_DT speech_NN as_RB well_RB as_IN with_IN isolated_VBN speech_NN ._.
With_IN continuous_JJ speech_NN naturally_RB spoken_VBN sentences_NNS are_VBP used_VBN ,_, therefore_RB it_PRP becomes_VBZ harder_JJR to_TO recognize_VB the_DT speech_NN ,_, different_JJ from_IN both_CC isloated_JJ and_CC discontinuous_JJ speech_NN ._.
Task_NNP and_CC Language_NNP Constraints_NNPS e.g._NNP Querying_NNP application_NN may_MD dismiss_VB the_DT hypothesis_NN ``_`` The_DT apple_NN is_VBZ red_JJ ._. ''_''
e.g._FW Constraints_NNS may_MD be_VB semantic_JJ ;_: rejecting_VBG ``_`` The_DT apple_NN is_VBZ angry_JJ ._. ''_''
e.g._NNP Syntactic_NNP ;_: rejecting_VBG ``_`` Red_NNP is_VBZ apple_NN the_DT ._. ''_''
Constraints_NNS are_VBP often_RB represented_VBN by_IN a_DT grammar_NN ._.
Read_VB vs._FW Spontaneous_FW Speech_FW When_WRB a_DT person_NN reads_VBZ it_PRP 's_VBZ usually_RB in_IN a_DT context_NN that_WDT has_VBZ been_VBN previously_RB prepared_VBN ,_, but_CC when_WRB a_DT person_NN uses_VBZ spontaneous_JJ speech_NN ,_, it_PRP is_VBZ difficult_JJ to_TO recognize_VB the_DT speech_NN ._.
because_IN of_IN the_DT disfluences_NNS -LRB-_-LRB- like_IN ``_`` uh_UH ''_'' and_CC ``_`` um_FW ''_'' ,_, false_JJ starts_NNS ,_, incomplete_JJ sentences_NNS ,_, stutering_NN ,_, coughing_VBG ,_, and_CC laughter_NN -RRB-_-RRB- and_CC limited_JJ vocabulary_NN ._.
Adverse_JJ conditions_NNS Environmental_NNP noise_NN -LRB-_-LRB- e.g._FW Noise_NNP in_IN a_DT car_NN or_CC a_DT factory_NN -RRB-_-RRB- Acoustical_JJ distortions_NNS -LRB-_-LRB- e.g._FW echoes_NNS ,_, room_NN acoustics_NNS -RRB-_-RRB- Speech_NN recognition_NN is_VBZ a_DT multileveled_JJ pattern_NN recognition_NN task_NN ._.
Acoustical_JJ signals_NNS are_VBP structured_VBN into_IN a_DT hierarchy_NN of_IN units_NNS ;_: e.g._NNP Phonemes_NNP ,_, Words_NNP ,_, Phrases_NNP ,_, and_CC Sentences_NNS ;_: Each_DT level_NN provides_VBZ additional_JJ constraints_NNS ;_: e.g._FW Known_VBN word_NN pronunciations_NNS or_CC legal_JJ word_NN sequences_NNS ,_, which_WDT can_MD compensate_VB for_IN errors_NNS or_CC uncertainties_NNS at_IN lower_JJR level_NN ;_: This_DT hierarchy_NN of_IN constraints_NNS are_VBP exploited_VBN ;_: By_IN combining_VBG decisions_NNS probabilistically_RB at_IN all_DT lower_JJR levels_NNS ,_, and_CC making_VBG more_RBR deterministic_JJ decisions_NNS only_RB at_IN the_DT highest_JJS level_NN ;_: Speech_NN recogniton_NN by_IN a_DT machine_NN is_VBZ a_DT process_NN broken_VBN into_IN several_JJ phases_NNS ._.
Computationally_RB ,_, it_PRP is_VBZ a_DT problem_NN in_IN which_WDT a_DT sound_NN pattern_NN has_VBZ to_TO be_VB recognized_VBN or_CC classified_VBN into_IN a_DT category_NN that_WDT represents_VBZ a_DT meaning_NN to_TO a_DT human_JJ ._.
Every_DT acoustic_JJ signal_NN can_MD be_VB broken_VBN in_IN smaller_JJR more_RBR basic_JJ sub-signals_NNS ._.
As_IN the_DT more_RBR complex_JJ sound_JJ signal_NN is_VBZ broken_VBN into_IN the_DT smaller_JJR sub-sounds_NNS ,_, different_JJ levels_NNS are_VBP created_VBN ,_, where_WRB at_IN the_DT top_JJ level_NN we_PRP have_VBP complex_JJ sounds_NNS ,_, which_WDT are_VBP made_VBN of_IN simpler_JJR sounds_NNS on_IN lower_JJR level_NN ,_, and_CC going_VBG to_TO lower_VB levels_NNS even_RB more_RBR ,_, we_PRP create_VBP more_RBR basic_JJ and_CC shorter_JJR and_CC simpler_JJR sounds_NNS ._.
The_DT lowest_JJS level_NN ,_, where_WRB the_DT sounds_NNS are_VBP the_DT most_RBS fundamental_JJ ,_, a_DT machine_NN would_MD check_VB for_IN simple_JJ and_CC more_RBR probabilistic_JJ rules_NNS of_IN what_WDT sound_NN should_MD represent_VB ._.
Once_RB these_DT sounds_NNS are_VBP put_VBN together_RB into_IN more_RBR complex_JJ sound_NN on_IN upper_JJ level_NN ,_, a_DT new_JJ set_NN of_IN more_JJR deterministic_JJ rules_NNS should_MD predict_VB what_WP new_JJ complex_JJ sound_NN should_MD represent_VB ._.
The_DT most_RBS upper_JJ level_NN of_IN a_DT deterministic_JJ rule_NN should_MD figure_VB out_RP the_DT meaning_NN of_IN complex_JJ expressions_NNS ._.
In_IN order_NN to_TO expand_VB our_PRP$ knowledge_NN about_IN speech_NN recognition_NN we_PRP need_VBP to_TO take_VB into_IN a_DT consideration_NN neural_JJ networks_NNS ._.
There_EX are_VBP four_CD steps_NNS of_IN neural_JJ network_NN approaches_NNS :_: Digitize_VB the_DT speech_NN that_IN we_PRP want_VBP to_TO recognize_VB For_IN telephone_NN speech_NN the_DT sampling_NN rate_NN is_VBZ 8000_CD samples_NNS per_IN second_JJ ;_: Compute_VB features_NNS of_IN spectral-domain_NN of_IN the_DT speech_NN -LRB-_-LRB- with_IN Fourier_NN transform_VBP -RRB-_-RRB- ;_: Computed_VBN every_DT 10msec_NN ,_, with_IN one_CD 10msec_NN section_NN called_VBD a_DT frame_NN ;_: Analysis_NN of_IN four_CD step_NN neural_JJ network_NN approaches_NNS can_MD be_VB explained_VBN by_IN further_JJ information_NN ._.
Sound_NNP is_VBZ produced_VBN by_IN air_NN -LRB-_-LRB- or_CC some_DT other_JJ medium_NN -RRB-_-RRB- vibration_NN ,_, which_WDT we_PRP register_VBP by_IN ears_NNS ,_, but_CC machines_NNS by_IN receivers_NNS ._.
Basic_JJ sound_NN creates_VBZ a_DT wave_NN which_WDT has_VBZ 2_CD descriptions_NNS ;_: Amplitude_NNP -LRB-_-LRB- how_WRB strong_JJ is_VBZ it_PRP -RRB-_-RRB- ,_, and_CC frequency_NN -LRB-_-LRB- how_WRB often_RB it_PRP vibrates_VBZ per_IN second_NN -RRB-_-RRB- ._.
Digitized_NNP Sound_NNP Graph_NNP This_NNP is_VBZ the_DT same_JJ as_IN the_DT wave_NN in_IN the_DT water_NN ._.
Big_JJ wave_NN is_VBZ strong_JJ and_CC smaller_JJR ones_NNS are_VBP usually_RB faster_JJR but_CC weaker_JJR ._.
That_DT is_VBZ how_WRB air_NN is_VBZ distorted_JJ ,_, but_CC we_PRP do_VBP n't_RB see_VB it_PRP easily_RB ,_, in_IN order_NN for_IN sound_NN to_TO travel_VB ._.
These_DT waves_NNS can_MD be_VB digitalized_VBN :_: Sample_VB a_DT strength_NN at_IN short_JJ intervals_NNS like_IN in_IN picture_NN above_IN to_TO get_VB bunch_NN of_IN numbers_NNS that_IN approximate_JJ at_IN each_DT time_NN step_VB the_DT strength_NN of_IN a_DT wave_NN ._.
Collection_NN of_IN these_DT numbers_NNS represent_VBP analog_NN wave_NN ._.
This_DT new_JJ wave_NN is_VBZ digital_JJ ._.
Sound_JJ waves_NNS are_VBP complicated_VBN because_IN they_PRP superimpose_VBP one_CD on_IN top_NN of_IN each_DT other_JJ ._.
Like_IN the_DT waves_NNS would_MD ._.
This_DT way_NN they_PRP create_VBP odd_JJ looking_VBG waves_NNS ._.
For_IN example_NN ,_, if_IN there_EX are_VBP two_CD waves_NNS that_WDT interact_VBP with_IN each_DT other_JJ we_PRP can_MD add_VB them_PRP which_WDT creates_VBZ new_JJ odd_JJ looking_VBG wave_NN as_IN is_VBZ shown_VBN in_IN the_DT picture_NN on_IN the_DT right_NN ._.
Neural_NNP Network_NNP classifies_VBZ features_NNS into_IN phonetic-based_JJ categories_NNS ;_: Given_VBN basic_JJ sound_NN blocks_NNS ,_, that_DT machine_NN digitized_VBD ,_, we_PRP have_VBP a_DT bunch_NN of_IN numbers_NNS which_WDT describe_VBP a_DT wave_NN and_CC waves_NNS describe_VBP words_NNS ._.
Each_DT frame_NN has_VBZ a_DT unit_NN block_NN of_IN sound_NN ,_, which_WDT are_VBP broken_VBN into_IN basic_JJ sound_JJ waves_NNS and_CC represented_VBN by_IN numbers_NNS after_IN Fourier_NN Transform_NN ,_, can_MD be_VB statistically_RB evaluated_VBN to_TO set_VB to_TO which_WDT class_NN of_IN sounds_NNS it_PRP belongs_VBZ to_TO ._.
The_DT nodes_NNS in_IN the_DT figure_NN on_IN a_DT slide_NN represent_VBP a_DT feature_NN of_IN a_DT sound_NN in_IN which_WDT a_DT feature_NN of_IN a_DT wave_NN from_IN first_JJ layer_NN of_IN nodes_NNS to_TO a_DT second_JJ layer_NN of_IN nodes_NNS based_VBN on_IN some_DT statistical_JJ analysis_NN ._.
This_DT analysis_NN depends_VBZ on_IN programer_NN 's_POS instructions_NNS ._.
At_IN this_DT point_NN ,_, a_DT second_JJ layer_NN of_IN nodes_NNS represents_VBZ a_DT higher_JJR level_NN features_NNS of_IN a_DT sound_JJ input_NN which_WDT is_VBZ again_RB statistically_RB evaluated_VBN to_TO see_VB what_WDT class_NN they_PRP belong_VBP to_TO ._.
Last_JJ level_NN of_IN nodes_NNS should_MD be_VB output_NN nodes_NNS that_WDT tell_VBP us_PRP with_IN high_JJ probability_NN what_WP original_JJ sound_NN really_RB was_VBD ._.
Search_NN to_TO match_VB the_DT neural-network_JJ output_NN scores_NNS for_IN the_DT best_JJS word_NN ,_, to_TO determine_VB the_DT word_NN that_WDT was_VBD most_RBS likely_RB uttered_VBN ;_: A_DT machine_NN speech_NN recognition_NN using_VBG neural_JJ network_NN is_VBZ still_RB just_RB a_DT fancy_JJ statistics_NNS ._.
Artificial_JJ neural_JJ network_NN has_VBZ specialized_VBN output_NN nodes_NNS for_IN results_NNS ,_, unlike_IN brain_NN ._.
Our_PRP$ brain_NN recognizes_VBZ the_DT meaning_NN of_IN words_NNS in_IN fundamentally_RB different_JJ way_NN ._.
Our_PRP$ brain_NN is_VBZ entirely_RB committed_VBN into_IN the_DT perception_NN of_IN sound_NN ._.
When_WRB we_PRP hear_VBP sound_JJ ,_, our_PRP$ life_NN experience_NN is_VBZ brought_VBN together_RB to_TO action_NN of_IN listening_VBG to_TO set_VB a_DT sound_NN into_IN a_DT appropriate_JJ perspective_NN so_IN it_PRP is_VBZ meaningful_JJ ._.
Brain_NN has_VBZ a_DT purpose_NN when_WRB it_PRP listens_VBZ for_IN a_DT sound_NN that_WDT is_VBZ steered_VBN toward_IN actions_NNS ._.
In_IN 1982_CD ,_, Kurzweil_NNP Applied_NNP Intelligence_NNP and_CC Dragon_NNP Systems_NNPS released_VBD speech_NN recognition_NN products_NNS ._.
By_IN 1985_CD ,_, Kurzweil_NNP 's_POS software_NN had_VBD a_DT vocabulary_NN of_IN 1,000_CD words_NNS --_: if_IN uttered_VBN one_CD word_NN at_IN a_DT time_NN ._.
Two_CD years_NNS later_RB ,_, in_IN 1987_CD ,_, its_PRP$ lexicon_NN reached_VBD 20,000_CD words_NNS ,_, entering_VBG the_DT realm_NN of_IN human_JJ vocabularies_NNS ,_, which_WDT range_VBP from_IN 10,000_CD to_TO 150,000_CD words_NNS ._.
But_CC recognition_NN accuracy_NN was_VBD only_RB 10_CD %_NN in_IN 1993_CD ._.
Two_CD years_NNS later_RB ,_, the_DT error_NN rate_NN crossed_VBD below_IN 50_CD %_NN ._.
Dragon_NNP Systems_NNP released_VBD ``_`` Naturally_RB Speaking_VBG ''_'' in_IN 1997_CD ,_, which_WDT recognized_VBD normal_JJ human_JJ speech_NN ._.
Progress_NNP mainly_RB came_VBD from_IN improved_VBN computer_NN performance_NN and_CC larger_JJR source_NN text_NN databases_NNS ._.
The_DT Brown_NNP Corpus_NNP was_VBD the_DT first_JJ major_JJ database_NN available_JJ ,_, containing_VBG several_JJ million_CD words_NNS ._.
In_IN 2006_CD ,_, Google_NNP published_VBD a_DT trillion-word_JJ corpus_NN ,_, while_IN Carnegie_NNP Mellon_NNP University_NNP researchers_NNS found_VBD no_DT significant_JJ increase_NN in_IN recognition_NN accuracy_NN ._.
Algorithms_NNS Both_CC acoustic_JJ modeling_NN and_CC language_NN modeling_NN are_VBP important_JJ parts_NNS of_IN modern_JJ statistically-based_JJ speech_NN recognition_NN algorithms_NNS ._.
Hidden_NNP Markov_NNP models_NNS -LRB-_-LRB- HMMs_NNS -RRB-_-RRB- are_VBP widely_RB used_VBN in_IN many_JJ systems_NNS ._.
Language_NN modeling_NN has_VBZ many_JJ other_JJ applications_NNS such_JJ as_IN smart_JJ keyboard_NN and_CC document_NN classification_NN ._.
Hidden_NNP Markov_NNP models_NNS Main_NNP article_NN :_: Hidden_NNP Markov_NNP model_NN Modern_NNP general-purpose_JJ speech_NN recognition_NN systems_NNS are_VBP based_VBN on_IN Hidden_NNP Markov_NNP Models_NNS ._.
These_DT are_VBP statistical_JJ models_NNS that_IN output_NN a_DT sequence_NN of_IN symbols_NNS or_CC quantities_NNS ._.
HMMs_NNS are_VBP used_VBN in_IN speech_NN recognition_NN because_IN a_DT speech_NN signal_NN can_MD be_VB viewed_VBN as_IN a_DT piecewise_JJ stationary_JJ signal_NN or_CC a_DT short-time_JJ stationary_JJ signal_NN ._.
In_IN a_DT short_JJ time-scales_NNS -LRB-_-LRB- e.g._FW ,_, 10_CD milliseconds_NNS -RRB-_-RRB- ,_, speech_NN can_MD be_VB approximated_VBN as_IN a_DT stationary_JJ process_NN ._.
Speech_NN can_MD be_VB thought_VBN of_IN as_IN a_DT Markov_NNP model_NN for_IN many_JJ stochastic_JJ purposes_NNS ._.
Another_DT reason_NN why_WRB HMMs_NNS are_VBP popular_JJ is_VBZ because_IN they_PRP can_MD be_VB trained_VBN automatically_RB and_CC are_VBP simple_JJ and_CC computationally_RB feasible_JJ to_TO use_VB ._.
In_IN speech_NN recognition_NN ,_, the_DT hidden_JJ Markov_NNP model_NN would_MD output_NN a_DT sequence_NN of_IN n-dimensional_JJ real-valued_JJ vectors_NNS -LRB-_-LRB- with_IN n_NN being_VBG a_DT small_JJ integer_NN ,_, such_JJ as_IN 10_CD -RRB-_-RRB- ,_, outputting_VBG one_CD of_IN these_DT every_DT 10_CD milliseconds_NNS ._.
The_DT vectors_NNS would_MD consist_VB of_IN cepstral_JJ coefficients_NNS ,_, which_WDT are_VBP obtained_VBN by_IN taking_VBG a_DT Fourier_NN transform_VB of_IN a_DT short_JJ time_NN window_NN of_IN speech_NN and_CC decorrelating_VBG the_DT spectrum_NN using_VBG a_DT cosine_NN transform_VB ,_, then_RB taking_VBG the_DT first_JJ -LRB-_-LRB- most_RBS significant_JJ -RRB-_-RRB- coefficients_NNS ._.
The_DT hidden_JJ Markov_NNP model_NN will_MD tend_VB to_TO have_VB in_IN each_DT state_NN a_DT statistical_JJ distribution_NN that_WDT is_VBZ a_DT mixture_NN of_IN diagonal_JJ covariance_NN Gaussians_NNP ,_, which_WDT will_MD give_VB a_DT likelihood_NN for_IN each_DT observed_VBN vector_NN ._.
Each_DT word_NN ,_, or_CC -LRB-_-LRB- for_IN more_RBR general_JJ speech_NN recognition_NN systems_NNS -RRB-_-RRB- ,_, each_DT phoneme_NN ,_, will_MD have_VB a_DT different_JJ output_NN distribution_NN ;_: a_DT hidden_JJ Markov_NNP model_NN for_IN a_DT sequence_NN of_IN words_NNS or_CC phonemes_NNS is_VBZ made_VBN by_IN concatenating_VBG the_DT individual_JJ trained_JJ hidden_JJ Markov_NNP models_NNS for_IN the_DT separate_JJ words_NNS and_CC phonemes_NNS ._.
Described_VBN above_RB are_VBP the_DT core_NN elements_NNS of_IN the_DT most_RBS common_JJ ,_, HMM-based_JJ approach_NN to_TO speech_NN recognition_NN ._.
Modern_NNP speech_NN recognition_NN systems_NNS use_VBP various_JJ combinations_NNS of_IN a_DT number_NN of_IN standard_JJ techniques_NNS in_IN order_NN to_TO improve_VB results_NNS over_IN the_DT basic_JJ approach_NN described_VBN above_IN ._.
A_DT typical_JJ large-vocabulary_JJ system_NN would_MD need_VB context_NN dependency_NN for_IN the_DT phonemes_NNS -LRB-_-LRB- so_IN phonemes_NNS with_IN different_JJ left_JJ and_CC right_JJ context_NN have_VBP different_JJ realizations_NNS as_IN HMM_NN states_NNS -RRB-_-RRB- ;_: it_PRP would_MD use_VB cepstral_JJ normalization_NN to_TO normalize_VB for_IN different_JJ speaker_NN and_CC recording_NN conditions_NNS ;_: for_IN further_JJ speaker_NN normalization_NN it_PRP might_MD use_VB vocal_JJ tract_NN length_NN normalization_NN -LRB-_-LRB- VTLN_NN -RRB-_-RRB- for_IN male-female_JJ normalization_NN and_CC maximum_NN likelihood_NN linear_JJ regression_NN -LRB-_-LRB- MLLR_NN -RRB-_-RRB- for_IN more_RBR general_JJ speaker_NN adaptation_NN ._.
The_DT features_NNS would_MD have_VB so-called_JJ delta_NN and_CC delta-delta_NN coefficients_NNS to_TO capture_VB speech_NN dynamics_NNS and_CC in_IN addition_NN might_MD use_VB heteroscedastic_JJ linear_JJ discriminant_JJ analysis_NN -LRB-_-LRB- HLDA_NN -RRB-_-RRB- ;_: or_CC might_MD skip_VB the_DT delta_NN and_CC delta-delta_NN coefficients_NNS and_CC use_NN splicing_NN and_CC an_DT LDA-based_JJ projection_NN followed_VBD perhaps_RB by_IN heteroscedastic_JJ linear_JJ discriminant_JJ analysis_NN or_CC a_DT global_JJ semitied_JJ covariance_NN transform_VB -LRB-_-LRB- also_RB known_VBN as_IN maximum_JJ likelihood_NN linear_NN transform_VBP ,_, or_CC MLLT_NN -RRB-_-RRB- ._.
Many_JJ systems_NNS use_VBP so-called_JJ discriminative_JJ training_NN techniques_NNS that_WDT dispense_VBP with_IN a_DT purely_RB statistical_JJ approach_NN to_TO HMM_NN parameter_NN estimation_NN and_CC instead_RB optimize_VB some_DT classification-related_JJ measure_NN of_IN the_DT training_NN data_NNS ._.
Examples_NNS are_VBP maximum_JJ mutual_JJ information_NN -LRB-_-LRB- MMI_NNP -RRB-_-RRB- ,_, minimum_JJ classification_NN error_NN -LRB-_-LRB- MCE_NN -RRB-_-RRB- and_CC minimum_JJ phone_NN error_NN -LRB-_-LRB- MPE_NN -RRB-_-RRB- ._.
Decoding_NN of_IN the_DT speech_NN -LRB-_-LRB- the_DT term_NN for_IN what_WP happens_VBZ when_WRB the_DT system_NN is_VBZ presented_VBN with_IN a_DT new_JJ utterance_NN and_CC must_MD compute_VB the_DT most_RBS likely_JJ source_NN sentence_NN -RRB-_-RRB- would_MD probably_RB use_VB the_DT Viterbi_NNP algorithm_NN to_TO find_VB the_DT best_JJS path_NN ,_, and_CC here_RB there_EX is_VBZ a_DT choice_NN between_IN dynamically_RB creating_VBG a_DT combination_NN hidden_VBN Markov_NNP model_NN ,_, which_WDT includes_VBZ both_CC the_DT acoustic_JJ and_CC language_NN model_NN information_NN ,_, and_CC combining_VBG it_PRP statically_RB beforehand_RB -LRB-_-LRB- the_DT finite_JJ state_NN transducer_NN ,_, or_CC FST_NN ,_, approach_NN -RRB-_-RRB- ._.
A_DT possible_JJ improvement_NN to_TO decoding_NN is_VBZ to_TO keep_VB a_DT set_NN of_IN good_JJ candidates_NNS instead_RB of_IN just_RB keeping_VBG the_DT best_JJS candidate_NN ,_, and_CC to_TO use_VB a_DT better_JJR scoring_VBG function_NN -LRB-_-LRB- rescoring_NN -RRB-_-RRB- to_TO rate_VB these_DT good_JJ candidates_NNS so_IN that_IN we_PRP may_MD pick_VB the_DT best_JJS one_NN according_VBG to_TO this_DT refined_JJ score_NN ._.
The_DT set_NN of_IN candidates_NNS can_MD be_VB kept_VBN either_CC as_IN a_DT list_NN -LRB-_-LRB- the_DT N-best_NN list_NN approach_NN -RRB-_-RRB- or_CC as_IN a_DT subset_NN of_IN the_DT models_NNS -LRB-_-LRB- a_DT lattice_NN -RRB-_-RRB- ._.
Rescoring_NNP is_VBZ usually_RB done_VBN by_IN trying_VBG to_TO minimize_VB the_DT Bayes_NNP risk_NN -LRB-_-LRB- or_CC an_DT approximation_NN thereof_RB -RRB-_-RRB- :_: Instead_RB of_IN taking_VBG the_DT source_NN sentence_NN with_IN maximal_JJ probability_NN ,_, we_PRP try_VBP to_TO take_VB the_DT sentence_NN that_WDT minimizes_VBZ the_DT expectancy_NN of_IN a_DT given_VBN loss_NN function_NN with_IN regards_VBZ to_TO all_DT possible_JJ transcriptions_NNS -LRB-_-LRB- i.e._FW ,_, we_PRP take_VBP the_DT sentence_NN that_WDT minimizes_VBZ the_DT average_JJ distance_NN to_TO other_JJ possible_JJ sentences_NNS weighted_VBN by_IN their_PRP$ estimated_VBN probability_NN -RRB-_-RRB- ._.
The_DT loss_NN function_NN is_VBZ usually_RB the_DT Levenshtein_NNP distance_NN ,_, though_IN it_PRP can_MD be_VB different_JJ distances_NNS for_IN specific_JJ tasks_NNS ;_: the_DT set_NN of_IN possible_JJ transcriptions_NNS is_VBZ ,_, of_IN course_NN ,_, pruned_VBN to_TO maintain_VB tractability_NN ._.
Efficient_JJ algorithms_NNS have_VBP been_VBN devised_VBN to_TO rescore_VB lattices_NNS represented_VBN as_IN weighted_JJ finite_JJ state_NN transducers_NNS with_IN edit_NN distances_NNS represented_VBD themselves_PRP as_IN a_DT finite_JJ state_NN transducer_NN verifying_VBG certain_JJ assumptions_NNS ._.
Dynamic_NNP time_NN warping_NN -LRB-_-LRB- DTW_NN -RRB-_-RRB- -_: based_VBN speech_NN recognition_NN Main_NNP article_NN :_: Dynamic_NNP time_NN warping_VBG Dynamic_NNP time_NN warping_NN is_VBZ an_DT approach_NN that_WDT was_VBD historically_RB used_VBN for_IN speech_NN recognition_NN but_CC has_VBZ now_RB largely_RB been_VBN displaced_VBN by_IN the_DT more_RBR successful_JJ HMM-based_JJ approach_NN ._.
Dynamic_NNP time_NN warping_NN is_VBZ an_DT algorithm_NN for_IN measuring_VBG similarity_NN between_IN two_CD sequences_NNS that_WDT may_MD vary_VB in_IN time_NN or_CC speed_NN ._.
For_IN instance_NN ,_, similarities_NNS in_IN walking_NN patterns_NNS would_MD be_VB detected_VBN ,_, even_RB if_IN in_IN one_CD video_NN the_DT person_NN was_VBD walking_VBG slowly_RB and_CC if_IN in_IN another_DT he_PRP or_CC she_PRP were_VBD walking_VBG more_RBR quickly_RB ,_, or_CC even_RB if_IN there_EX were_VBD accelerations_NNS and_CC decelerations_NNS during_IN the_DT course_NN of_IN one_CD observation_NN ._.
DTW_NNP has_VBZ been_VBN applied_VBN to_TO video_NN ,_, audio_NN ,_, and_CC graphics_NNS --_: indeed_RB ,_, any_DT data_NNS that_WDT can_MD be_VB turned_VBN into_IN a_DT linear_JJ representation_NN can_MD be_VB analyzed_VBN with_IN DTW_NN ._.
A_DT well-known_JJ application_NN has_VBZ been_VBN automatic_JJ speech_NN recognition_NN ,_, to_TO cope_VB with_IN different_JJ speaking_NN speeds_NNS ._.
In_IN general_JJ ,_, it_PRP is_VBZ a_DT method_NN that_WDT allows_VBZ a_DT computer_NN to_TO find_VB an_DT optimal_JJ match_NN between_IN two_CD given_VBN sequences_NNS -LRB-_-LRB- e.g._FW ,_, time_NN series_NN -RRB-_-RRB- with_IN certain_JJ restrictions_NNS ._.
That_DT is_VBZ ,_, the_DT sequences_NNS are_VBP ``_`` warped_JJ ''_'' non-linearly_JJ to_TO match_VB each_DT other_JJ ._.
This_DT sequence_NN alignment_NN method_NN is_VBZ often_RB used_VBN in_IN the_DT context_NN of_IN hidden_JJ Markov_NNP models_NNS ..._: ._.
Neural_JJ networks_NNS Main_NNP article_NN :_: Neural_JJ networks_NNS Neural_JJ networks_NNS emerged_VBD as_IN an_DT attractive_JJ acoustic_JJ modeling_NN approach_NN in_IN ASR_NNP in_IN the_DT late_JJ 1980s_NNS ._.
Since_IN then_RB ,_, neural_JJ networks_NNS have_VBP been_VBN used_VBN in_IN many_JJ aspects_NNS of_IN speech_NN recognition_NN such_JJ as_IN phoneme_NN classification_NN ,_, isolated_VBN word_NN recognition_NN ,_, and_CC speaker_NN adaptation_NN ._.
In_IN contrast_NN to_TO HMMs_NNS ,_, neural_JJ networks_NNS make_VBP no_DT assumptions_NNS about_IN feature_NN statistical_JJ properties_NNS and_CC have_VBP several_JJ qualities_NNS making_VBG them_PRP attractive_JJ recognition_NN models_NNS for_IN speech_NN recognition_NN ._.
When_WRB used_VBN to_TO estimate_VB the_DT probabilities_NNS of_IN a_DT speech_NN feature_NN segment_NN ,_, neural_JJ networks_NNS allow_VBP discriminative_JJ training_NN in_IN a_DT natural_JJ and_CC efficient_JJ manner_NN ._.
Few_JJ assumptions_NNS on_IN the_DT statistics_NNS of_IN input_NN features_NNS are_VBP made_VBN with_IN neural_JJ networks_NNS ._.
However_RB ,_, in_IN spite_NN of_IN their_PRP$ effectiveness_NN in_IN classifying_VBG short-time_JJ units_NNS such_JJ as_IN individual_JJ phones_NNS and_CC isolated_VBN words_NNS ,_, neural_JJ networks_NNS are_VBP rarely_RB successful_JJ for_IN continuous_JJ recognition_NN tasks_NNS ,_, largely_RB because_IN of_IN their_PRP$ lack_NN of_IN ability_NN to_TO model_VB temporal_JJ dependencies_NNS ._.
Thus_RB ,_, one_CD alternative_JJ approach_NN is_VBZ to_TO use_VB neural_JJ networks_NNS as_IN a_DT pre-processing_FW e.g._FW feature_NN transformation_NN ,_, dimensionality_NN reduction_NN ,_, for_IN the_DT HMM_NNP based_VBN recognition_NN ._.
Further_JJ information_NN Popular_NNP speech_NN recognition_NN conferences_NNS held_VBD each_DT year_NN or_CC two_CD include_VBP SpeechTEK_NN and_CC SpeechTEK_NN Europe_NN ,_, ICASSP_NN ,_, Eurospeech\/ICSLP_NN -LRB-_-LRB- now_RB named_VBN Interspeech_NNP -RRB-_-RRB- and_CC the_DT IEEE_NNP ASRU_NNP ._.
Conferences_NNS in_IN the_DT field_NN of_IN Natural_JJ language_NN processing_NN ,_, such_JJ as_IN ACL_NN ,_, NAACL_NN ,_, EMNLP_NN ,_, and_CC HLT_NNP ,_, are_VBP beginning_VBG to_TO include_VB papers_NNS on_IN speech_NN processing_NN ._.
Important_JJ journals_NNS include_VBP the_DT IEEE_NNP Transactions_NNS on_IN Speech_NNP and_CC Audio_NNP Processing_NNP -LRB-_-LRB- now_RB named_VBN IEEE_NN Transactions_NNS on_IN Audio_NNP ,_, Speech_NNP and_CC Language_NNP Processing_NNP -RRB-_-RRB- ,_, Computer_NNP Speech_NNP and_CC Language_NNP ,_, and_CC Speech_NNP Communication_NNP ._.
Books_NNS like_IN ``_`` Fundamentals_NNS of_IN Speech_NN Recognition_NN ''_'' by_IN Lawrence_NNP Rabiner_NNP can_MD be_VB useful_JJ to_TO acquire_VB basic_JJ knowledge_NN but_CC may_MD not_RB be_VB fully_RB up_IN to_TO date_NN -LRB-_-LRB- 1993_CD -RRB-_-RRB- ._.
A_DT very_RB recent_JJ book_NN -LRB-_-LRB- Dec._NNP 2011_CD -RRB-_-RRB- ,_, ``_`` Fundamentals_NNS of_IN Speaker_NNP Recognition_NN ''_'' by_IN Homayoon_NNP Beigi_NNP covers_VBZ the_DT more_RBR recent_JJ developments_NNS in_IN some_DT detail_NN ._.
Although_IN the_DT title_NN concentrates_VBZ on_IN speaker_NN recognition_NN ,_, but_CC a_DT large_JJ portion_NN of_IN the_DT book_NN applies_VBZ directly_RB to_TO speech_NN recognition_NN ,_, with_IN a_DT lot_NN of_IN valuable_JJ detailed_JJ background_NN material_NN ._.
Another_DT good_JJ source_NN can_MD be_VB ``_`` Statistical_JJ Methods_NNS for_IN Speech_NN Recognition_NN ''_'' by_IN Frederick_NNP Jelinek_NNP and_CC ``_`` Spoken_NNP Language_NNP Processing_NNP -LRB-_-LRB- 2001_CD -RRB-_-RRB- ''_'' by_IN Xuedong_NNP Huang_NNP etc._NN ._.
More_RBR up_RB to_TO date_NN is_VBZ ``_`` Computer_NNP Speech_NNP ''_'' ,_, by_IN Manfred_NNP R._NNP Schroeder_NNP ,_, second_JJ edition_NN published_VBN in_IN 2004_CD ._.
The_DT recently_RB updated_VBN textbook_NN of_IN ``_`` Speech_NN and_CC Language_NN Processing_NN -LRB-_-LRB- 2008_CD -RRB-_-RRB- ''_'' by_IN Jurafsky_NNP and_CC Martin_NNP presents_VBZ the_DT basics_NNS and_CC the_DT state_NN of_IN the_DT art_NN for_IN ASR_NNP ._.
A_DT good_JJ insight_NN into_IN the_DT techniques_NNS used_VBN in_IN the_DT best_JJS modern_JJ systems_NNS can_MD be_VB gained_VBN by_IN paying_VBG attention_NN to_TO government_NN sponsored_VBD evaluations_NNS such_JJ as_IN those_DT organised_VBN by_IN DARPA_NNP -LRB-_-LRB- the_DT largest_JJS speech_NN recognition-related_JJ project_NN ongoing_JJ as_IN of_IN 2007_CD is_VBZ the_DT GALE_NNP project_NN ,_, which_WDT involves_VBZ both_CC speech_NN recognition_NN and_CC translation_NN components_NNS -RRB-_-RRB- ._.
In_IN terms_NNS of_IN freely_RB available_JJ resources_NNS ,_, Carnegie_NNP Mellon_NNP University_NNP 's_POS SPHINX_NNP toolkit_NN is_VBZ one_CD place_NN to_TO start_VB to_TO both_DT learn_VBP about_IN speech_NN recognition_NN and_CC to_TO start_VB experimenting_VBG ._.
Another_DT resource_NN -LRB-_-LRB- free_JJ as_IN in_IN free_JJ beer_NN ,_, not_RB as_IN in_IN free_JJ speech_NN -RRB-_-RRB- is_VBZ the_DT HTK_NN book_NN -LRB-_-LRB- and_CC the_DT accompanying_VBG HTK_NN toolkit_NN -RRB-_-RRB- ._.
The_DT AT&T_NNP libraries_NNS GRM_NNP library_NN ,_, and_CC DCD_NNP library_NN are_VBP also_RB general_JJ software_NN libraries_NNS for_IN large-vocabulary_JJ speech_NN recognition_NN ._.
For_IN more_JJR software_NN resources_NNS ,_, see_VBP List_NN of_IN speech_NN recognition_NN software_NN ._.
A_DT useful_JJ review_NN of_IN the_DT area_NN of_IN robustness_NN in_IN ASR_NNP is_VBZ provided_VBN by_IN Junqua_NNP and_CC Haton_NNP -LRB-_-LRB- 1995_CD -RRB-_-RRB- ._.
People_NNS with_IN disabilities_NNS People_NNS with_IN disabilities_NNS can_MD benefit_VB from_IN speech_NN recognition_NN programs_NNS ._.
For_IN individuals_NNS that_WDT are_VBP Deaf_JJ or_CC Hard_JJ of_IN Hearing_NN ,_, speech_NN recognition_NN software_NN is_VBZ used_VBN to_TO automatically_RB generate_VB a_DT closed-captioning_NN of_IN conversations_NNS such_JJ as_IN discussions_NNS in_IN conference_NN rooms_NNS ,_, classroom_NN lectures_VBZ ,_, and\/or_CC religious_JJ services_NNS ._.
Speech_NN recognition_NN is_VBZ also_RB very_RB useful_JJ for_IN people_NNS who_WP have_VBP difficulty_NN using_VBG their_PRP$ hands_NNS ,_, ranging_VBG from_IN mild_JJ repetitive_JJ stress_NN injuries_NNS to_TO involved_VBN disabilities_NNS that_WDT preclude_VBP using_VBG conventional_JJ computer_NN input_NN devices_NNS ._.
In_IN fact_NN ,_, people_NNS who_WP used_VBD the_DT keyboard_NN a_DT lot_NN and_CC developed_VBD RSI_NNP became_VBD an_DT urgent_JJ early_JJ market_NN for_IN speech_NN recognition_NN ._.
Speech_NN recognition_NN is_VBZ used_VBN in_IN deaf_JJ telephony_NN ,_, such_JJ as_IN voicemail_NN to_TO text_NN ,_, relay_NN services_NNS ,_, and_CC captioned_VBD telephone_NN ._.
Individuals_NNS with_IN learning_VBG disabilities_NNS who_WP have_VBP problems_NNS with_IN thought-to-paper_JJ communication_NN -LRB-_-LRB- essentially_RB they_PRP think_VBP of_IN an_DT idea_NN but_CC it_PRP is_VBZ processed_VBN incorrectly_RB causing_VBG it_PRP to_TO end_VB up_RP differently_RB on_IN paper_NN -RRB-_-RRB- can_MD benefit_VB from_IN the_DT software_NN -LRB-_-LRB- citation_NN needed_VBN -RRB-_-RRB- ._.
-LRB-_-LRB- icon_NN -RRB-_-RRB- This_DT section_NN requires_VBZ expansion_NN ._.
Current_JJ research_NN and_CC funding_NN Measuring_VBG progress_NN in_IN speech_NN recognition_NN performance_NN is_VBZ difficult_JJ and_CC controversial_JJ ._.
Some_DT speech_NN recognition_NN tasks_NNS are_VBP much_RB more_RBR difficult_JJ than_IN others_NNS ._.
Word_NN error_NN rates_NNS on_IN some_DT tasks_NNS are_VBP less_JJR than_IN 1_CD %_NN ._.
On_IN others_NNS they_PRP can_MD be_VB as_RB high_JJ as_IN 50_CD %_NN ._.
Sometimes_RB it_PRP even_RB appears_VBZ that_IN performance_NN is_VBZ going_VBG backward_RB ,_, as_IN researchers_NNS undertake_VBP harder_JJR tasks_NNS that_WDT have_VBP higher_JJR error_NN rates_NNS ._.
Because_IN progress_NN is_VBZ slow_JJ and_CC is_VBZ difficult_JJ to_TO measure_VB ,_, there_EX is_VBZ some_DT perception_NN that_IN performance_NN has_VBZ plateaued_VBN and_CC that_IN funding_NN has_VBZ dried_VBN up_RP or_CC shifted_VBN priorities_NNS ._.
Such_JJ perceptions_NNS are_VBP not_RB new_JJ ._.
In_IN 1969_CD ,_, John_NNP Pierce_NNP wrote_VBD an_DT open_JJ letter_NN that_WDT did_VBD cause_VB much_JJ funding_NN to_TO dry_VB up_RP for_IN several_JJ years_NNS ._.
In_IN 1993_CD there_EX was_VBD a_DT strong_JJ feeling_NN that_IN performance_NN had_VBD plateaued_VBN and_CC there_EX were_VBD workshops_NNS dedicated_VBN to_TO the_DT issue_NN ._.
However_RB ,_, in_IN the_DT 1990s_CD ,_, funding_VBG continued_VBN more_RBR or_CC less_RBR uninterrupted_JJ and_CC performance_NN continued_VBD ,_, slowly_RB but_CC steadily_RB ,_, to_TO improve_VB ._.
For_IN the_DT past_JJ thirty_CD years_NNS ,_, speech_NN recognition_NN research_NN has_VBZ been_VBN characterized_VBN by_IN the_DT steady_JJ accumulation_NN of_IN small_JJ incremental_JJ improvements_NNS ._.
There_EX has_VBZ also_RB been_VBN a_DT trend_NN to_TO change_VB focus_NN to_TO more_RBR difficult_JJ tasks_NNS due_JJ both_DT to_TO progress_VB in_IN speech_NN recognition_NN performance_NN and_CC to_TO the_DT availability_NN of_IN faster_JJR computers_NNS ._.
In_IN particular_JJ ,_, this_DT shifting_VBG to_TO more_RBR difficult_JJ tasks_NNS has_VBZ characterized_VBN DARPA_NNP funding_NN of_IN speech_NN recognition_NN since_IN the_DT 1980s_CD ._.
In_IN the_DT last_JJ decade_NN ,_, it_PRP has_VBZ continued_VBN with_IN the_DT EARS_NNS project_NN ,_, which_WDT undertook_VBD recognition_NN of_IN Mandarin_NNP and_CC Arabic_NNP in_IN addition_NN to_TO English_NNP ,_, and_CC the_DT GALE_NNP project_NN ,_, which_WDT focused_VBD solely_RB on_IN Mandarin_NNP and_CC Arabic_JJ and_CC required_JJ translation_NN simultaneously_RB with_IN speech_NN recognition_NN ._.
Commercial_JJ research_NN and_CC other_JJ academic_JJ research_NN also_RB continue_VBP to_TO focus_VB on_IN increasingly_RB difficult_JJ problems_NNS ._.
One_CD key_JJ area_NN is_VBZ to_TO improve_VB robustness_NN of_IN speech_NN recognition_NN performance_NN ,_, not_RB just_RB robustness_NN against_IN noise_NN but_CC robustness_NN against_IN any_DT condition_NN that_WDT causes_VBZ a_DT major_JJ degradation_NN in_IN performance_NN ._.
Another_DT key_JJ area_NN of_IN research_NN is_VBZ focused_VBN on_IN an_DT opportunity_NN rather_RB than_IN a_DT problem_NN ._.
This_DT research_NN attempts_VBZ to_TO take_VB advantage_NN of_IN the_DT fact_NN that_IN in_IN many_JJ applications_NNS there_EX is_VBZ a_DT large_JJ quantity_NN of_IN speech_NN data_NNS available_JJ ,_, up_RB to_TO millions_NNS of_IN hours_NNS ._.
It_PRP is_VBZ too_RB expensive_JJ to_TO have_VB humans_NNS transcribe_VBP such_JJ large_JJ quantities_NNS of_IN speech_NN ,_, so_IN the_DT research_NN focus_NN is_VBZ on_IN developing_VBG new_JJ methods_NNS of_IN machine_NN learning_NN that_WDT can_MD effectively_RB utilize_VB large_JJ quantities_NNS of_IN unlabeled_JJ data_NNS ._.
Another_DT area_NN of_IN research_NN is_VBZ better_JJR understanding_NN of_IN human_JJ capabilities_NNS and_CC to_TO use_VB this_DT understanding_NN to_TO improve_VB machine_NN recognition_NN performance_NN ._.
Speech_NN segmentation_NN is_VBZ the_DT process_NN of_IN identifying_VBG the_DT boundaries_NNS between_IN words_NNS ,_, syllables_NNS ,_, or_CC phonemes_NNS in_IN spoken_VBN natural_JJ languages_NNS ._.
The_DT term_NN applies_VBZ both_DT to_TO the_DT mental_JJ processes_NNS used_VBN by_IN humans_NNS ,_, and_CC to_TO artificial_JJ processes_NNS of_IN natural_JJ language_NN processing_NN ._.
Speech_NN segmentation_NN is_VBZ an_DT important_JJ subproblem_NN of_IN speech_NN recognition_NN ,_, and_CC can_MD not_RB be_VB adequately_RB solved_VBN in_IN isolation_NN ._.
As_IN in_IN most_JJS natural_JJ language_NN processing_NN problems_NNS ,_, one_PRP must_MD take_VB into_IN account_NN context_NN ,_, grammar_NN ,_, and_CC semantics_NNS ,_, and_CC even_RB so_IN the_DT result_NN is_VBZ often_RB a_DT probabilistic_JJ division_NN rather_RB than_IN a_DT categorical_NN ._.
A_DT comprehensive_JJ survey_NN of_IN speech_NN segmentation_NN problems_NNS and_CC techniques_NNS can_MD be_VB seen_VBN in_IN ._.
Some_DT writing_VBG systems_NNS indicate_VBP speech_NN segmentation_NN between_IN words_NNS by_IN a_DT word_NN divider_NN ,_, such_JJ as_IN the_DT space_NN ._.
The_DT difficulty_NN of_IN this_DT problem_NN is_VBZ compounded_VBN by_IN the_DT phenomenon_NN of_IN co-articulation_NN of_IN speech_NN sounds_NNS ,_, where_WRB one_PRP may_MD be_VB modified_VBN in_IN various_JJ ways_NNS by_IN the_DT adjacent_JJ sounds_NNS :_: it_PRP may_MD blend_VB smoothly_RB with_IN them_PRP ,_, fuse_NN with_IN them_PRP ,_, split_NN ,_, or_CC even_RB disappear_VB ._.
This_DT phenomenon_NN may_MD happen_VB between_IN adjacent_JJ words_NNS just_RB as_RB easily_RB as_IN within_IN a_DT single_JJ word_NN ._.
The_DT notion_NN that_IN speech_NN is_VBZ produced_VBN like_IN writing_NN ,_, as_IN a_DT sequence_NN of_IN distinct_JJ vowels_NNS and_CC consonants_NNS ,_, is_VBZ a_DT relic_NN of_IN our_PRP$ alphabetic_JJ heritage_NN -LRB-_-LRB- citation_NN needed_VBN -RRB-_-RRB- ._.
In_IN fact_NN ,_, the_DT way_NN we_PRP produce_VBP vowels_NNS depends_VBZ on_IN the_DT surrounding_JJ consonants_NNS and_CC the_DT way_NN we_PRP produce_VBP consonants_NNS depends_VBZ on_IN the_DT surrounding_JJ vowels_NNS ._.
For_IN example_NN ,_, when_WRB we_PRP say_VBP `_`` kit_NN '_'' ,_, the_DT -LRB-_-LRB- k_NN -RRB-_-RRB- is_VBZ farther_RBR forward_RB than_IN when_WRB we_PRP say_VBP `_`` caught_VBN '_'' ._.
But_CC also_RB the_DT vowel_NN in_IN `_`` kick_NN '_'' is_VBZ phonetically_RB different_JJ from_IN the_DT vowel_NN in_IN `_`` kit_NN '_'' ,_, though_IN we_PRP normally_RB do_VBP not_RB hear_VB this_DT ._.
In_IN addition_NN ,_, there_EX are_VBP language-specific_JJ changes_NNS which_WDT occur_VBP on_IN casual_JJ speech_NN which_WDT makes_VBZ it_PRP quite_RB different_JJ from_IN spelling_NN ._.
For_IN example_NN ,_, in_IN English_NNP ,_, the_DT phrase_NN `_`` hit_VB you_PRP '_'' could_MD often_RB be_VB more_RBR appropriately_RB spelled_VBN `_`` hitcha_NN '_'' ._.
Therefore_RB ,_, even_RB with_IN the_DT best_JJS algorithms_NNS ,_, the_DT result_NN of_IN phonetic_JJ segmentation_NN will_MD usually_RB be_VB very_RB distant_JJ from_IN the_DT standard_JJ written_JJ language_NN ._.
For_IN this_DT reason_NN ,_, the_DT lexical_JJ and_CC syntactic_JJ parsing_NN of_IN spoken_VBN text_NN normally_RB requires_VBZ specialized_JJ algorithms_NNS ,_, distinct_JJ from_IN those_DT used_VBN for_IN parsing_VBG written_VBN text_NN ._.
Statistical_JJ models_NNS can_MD be_VB used_VBN to_TO segment_NN and_CC align_NN recorded_VBD speech_NN to_TO words_NNS or_CC phones_NNS ._.
Applications_NNS include_VBP automatic_JJ lip-synch_JJ timing_NN for_IN cartoon_NN animation_NN ,_, follow-the-bouncing-ball_JJ video_NN sub-titling_NN ,_, and_CC linguistic_JJ research_NN ._.
Automatic_NNP segmentation_NN and_CC alignment_NN software_NN is_VBZ commercially_RB available_JJ ._.
Lexical_JJ segmentation_NN In_IN all_DT natural_JJ languages_NNS ,_, the_DT meaning_NN of_IN a_DT complex_JJ spoken_VBN sentence_NN -LRB-_-LRB- which_WDT often_RB has_VBZ never_RB been_VBN heard_VBN or_CC uttered_VBN before_RB -RRB-_-RRB- can_MD be_VB understood_VBN only_RB by_IN decomposing_VBG it_PRP into_IN smaller_JJR lexical_JJ segments_NNS -LRB-_-LRB- roughly_RB ,_, the_DT words_NNS of_IN the_DT language_NN -RRB-_-RRB- ,_, associating_VBG a_DT meaning_NN to_TO each_DT segment_NN ,_, and_CC then_RB combining_VBG those_DT meanings_NNS according_VBG to_TO the_DT grammar_NN rules_NNS of_IN the_DT language_NN ._.
The_DT recognition_NN of_IN each_DT lexical_JJ segment_NN in_IN turn_NN requires_VBZ its_PRP$ decomposition_NN into_IN a_DT sequence_NN of_IN discrete_JJ phonetic_JJ segments_NNS and_CC mapping_VBG each_DT segment_NN to_TO one_CD element_NN of_IN a_DT finite_JJ set_NN of_IN elementary_JJ sounds_NNS -LRB-_-LRB- roughly_RB ,_, the_DT phonemes_NNS of_IN the_DT language_NN -RRB-_-RRB- ;_: the_DT meaning_NN then_RB can_MD be_VB found_VBN by_IN standard_JJ table_NN lookup_NN algorithms_NNS ._.
For_IN most_JJS spoken_VBN languages_NNS ,_, the_DT boundaries_NNS between_IN lexical_JJ units_NNS are_VBP surprisingly_RB difficult_JJ to_TO identify_VB ._.
One_CD might_MD expect_VB that_IN the_DT inter-word_JJ spaces_NNS used_VBN by_IN many_JJ written_JJ languages_NNS ,_, like_IN English_NNP or_CC Spanish_NNP ,_, would_MD correspond_VB to_TO pauses_VBZ in_IN their_PRP$ spoken_VBN version_NN ;_: but_CC that_DT is_VBZ true_JJ only_RB in_IN very_RB slow_JJ speech_NN ,_, when_WRB the_DT speaker_NN deliberately_RB inserts_NNS those_DT pauses_VBZ ._.
In_IN normal_JJ speech_NN ,_, one_CD typically_RB finds_VBZ many_JJ consecutive_JJ words_NNS being_VBG said_VBD with_IN no_DT pauses_VBZ between_IN them_PRP ,_, and_CC often_RB the_DT final_JJ sounds_NNS of_IN one_CD word_NN blend_VB smoothly_RB or_CC fuse_NN with_IN the_DT initial_JJ sounds_NNS of_IN the_DT next_JJ word_NN ._.
Moreover_RB ,_, an_DT utterance_NN can_MD have_VB different_JJ meanings_NNS depending_VBG on_IN how_WRB it_PRP is_VBZ split_VBN into_IN words_NNS ._.
A_DT popular_JJ example_NN ,_, often_RB quoted_VBN in_IN the_DT field_NN ,_, is_VBZ the_DT phrase_NN How_WRB to_TO wreck_VB a_DT nice_JJ beach_NN ,_, which_WDT sounds_VBZ very_RB similar_JJ to_TO How_WRB to_TO recognize_VB speech_NN ._.
As_IN this_DT example_NN shows_VBZ ,_, proper_JJ lexical_JJ segmentation_NN depends_VBZ on_IN context_NN and_CC semantics_NNS which_WDT draws_VBZ on_IN the_DT whole_NN of_IN human_JJ knowledge_NN and_CC experience_NN ,_, and_CC would_MD thus_RB require_VB advanced_JJ pattern_NN recognition_NN and_CC artificial_JJ intelligence_NN technologies_NNS to_TO be_VB implemented_VBN on_IN a_DT computer_NN ._.
This_DT problem_NN overlaps_VBZ to_TO some_DT extent_NN with_IN the_DT problem_NN of_IN text_NN segmentation_NN that_WDT occurs_VBZ in_IN some_DT languages_NNS which_WDT are_VBP traditionally_RB written_VBN without_IN inter-word_JJ spaces_NNS ,_, like_IN Chinese_NNP and_CC Japanese_NNP ._.
However_RB ,_, even_RB for_IN those_DT languages_NNS ,_, text_NN segmentation_NN is_VBZ often_RB much_RB easier_JJR than_IN speech_NN segmentation_NN ,_, because_IN the_DT written_VBN language_NN usually_RB has_VBZ little_JJ interference_NN between_IN adjacent_JJ words_NNS ,_, and_CC often_RB contains_VBZ additional_JJ clues_NNS not_RB present_JJ in_IN speech_NN -LRB-_-LRB- such_JJ as_IN the_DT use_NN of_IN Chinese_JJ characters_NNS for_IN word_NN stems_VBZ in_IN Japanese_NNP -RRB-_-RRB- ._.
Text_NN segmentation_NN is_VBZ the_DT process_NN of_IN dividing_VBG written_VBN text_NN into_IN meaningful_JJ units_NNS ,_, such_JJ as_IN words_NNS ,_, sentences_NNS ,_, or_CC topics_NNS ._.
The_DT term_NN applies_VBZ both_DT to_TO mental_JJ processes_NNS used_VBN by_IN humans_NNS when_WRB reading_VBG text_NN ,_, and_CC to_TO artificial_JJ processes_NNS implemented_VBN in_IN computers_NNS ,_, which_WDT are_VBP the_DT subject_NN of_IN natural_JJ language_NN processing_NN ._.
The_DT problem_NN is_VBZ non-trivial_JJ ,_, because_IN while_IN some_DT written_VBN languages_NNS have_VBP explicit_JJ word_NN boundary_NN markers_NNS ,_, such_JJ as_IN the_DT word_NN spaces_NNS of_IN written_VBN English_NNP and_CC the_DT distinctive_JJ initial_JJ ,_, medial_JJ and_CC final_JJ letter_NN shapes_NNS of_IN Arabic_JJ ,_, such_JJ signals_NNS are_VBP sometimes_RB ambiguous_JJ and_CC not_RB present_JJ in_IN all_DT written_VBN languages_NNS ._.
Compare_VB speech_NN segmentation_NN ,_, the_DT process_NN of_IN dividing_VBG speech_NN into_IN linguistically_RB meaningful_JJ portions_NNS ._.
In_IN English_NNP and_CC many_JJ other_JJ languages_NNS using_VBG some_DT form_NN of_IN the_DT Latin_JJ alphabet_NN ,_, the_DT space_NN is_VBZ a_DT good_JJ approximation_NN of_IN a_DT word_NN delimiter_NN ._.
-LRB-_-LRB- Some_DT examples_NNS where_WRB the_DT space_NN character_NN alone_RB may_MD not_RB be_VB sufficient_JJ include_VBP contractions_NNS like_IN ca_MD n't_RB for_IN can_MD not_RB ._. -RRB-_-RRB-
However_RB the_DT equivalent_NN to_TO this_DT character_NN is_VBZ not_RB found_VBN in_IN all_DT written_VBN scripts_NNS ,_, and_CC without_IN it_PRP word_NN segmentation_NN is_VBZ a_DT difficult_JJ problem_NN ._.
Languages_NNS which_WDT do_VBP not_RB have_VB a_DT trivial_JJ word_NN segmentation_NN process_NN include_VBP Chinese_NNP ,_, Japanese_NNP ,_, where_WRB sentences_NNS but_CC not_RB words_NNS are_VBP delimited_VBN ,_, Thai_NNP and_CC Lao_NNP ,_, where_WRB phrases_NNS and_CC sentences_NNS but_CC not_RB words_NNS are_VBP delimited_VBN ,_, and_CC Vietnamese_NNP ,_, where_WRB syllables_NNS but_CC not_RB words_NNS are_VBP delimited_VBN ._.
In_IN some_DT writing_NN systems_NNS however_RB ,_, such_JJ as_IN the_DT Ge'ez_NN script_NN used_VBN for_IN Amharic_NNP and_CC Tigrinya_NNP among_IN other_JJ languages_NNS ,_, words_NNS are_VBP explicitly_RB delimited_VBN -LRB-_-LRB- at_IN least_JJS historically_RB -RRB-_-RRB- with_IN a_DT non-whitespace_JJ character_NN ._.
The_DT Unicode_NNP Consortium_NNP has_VBZ published_VBN a_DT Standard_NNP Annex_NNP on_IN Text_NNP Segmentation_NNP ,_, exploring_VBG the_DT issues_NNS of_IN segmentation_NN in_IN multiscript_JJ texts_NNS ._.
Word_NN splitting_NN is_VBZ the_DT process_NN of_IN parsing_VBG concatenated_VBN text_NN -LRB-_-LRB- i.e._FW text_NN that_WDT contains_VBZ no_DT spaces_NNS or_CC other_JJ word_NN separators_NNS -RRB-_-RRB- to_TO infer_VB where_WRB word_NN breaks_NNS exist_VBP ._.
Word_NN splitting_NN may_MD also_RB refer_VB to_TO the_DT process_NN of_IN hyphenation_NN ._.
Sentence_NN segmentation_NN Sentence_NN segmentation_NN is_VBZ the_DT problem_NN of_IN dividing_VBG a_DT string_NN of_IN written_VBN language_NN into_IN its_PRP$ component_NN sentences_NNS ._.
In_IN English_NNP and_CC some_DT other_JJ languages_NNS ,_, using_VBG punctuation_NN ,_, particularly_RB the_DT full_JJ stop_NN character_NN is_VBZ a_DT reasonable_JJ approximation_NN ._.
However_RB even_RB in_IN English_NNP this_DT problem_NN is_VBZ not_RB trivial_JJ due_JJ to_TO the_DT use_NN of_IN the_DT full_JJ stop_NN character_NN for_IN abbreviations_NNS ,_, which_WDT may_MD or_CC may_MD not_RB also_RB terminate_VB a_DT sentence_NN ._.
For_IN example_NN Mr._NNP is_VBZ not_RB its_PRP$ own_JJ sentence_NN in_IN ``_`` Mr._NNP Smith_NNP went_VBD to_TO the_DT shops_NNS in_IN Jones_NNP Street_NNP ._. ''_''
When_WRB processing_VBG plain_JJ text_NN ,_, tables_NNS of_IN abbreviations_NNS that_WDT contain_VBP periods_NNS can_MD help_VB prevent_VB incorrect_JJ assignment_NN of_IN sentence_NN boundaries_NNS ._.
As_IN with_IN word_NN segmentation_NN ,_, not_RB all_DT written_VBN languages_NNS contain_VBP punctuation_NN characters_NNS which_WDT are_VBP useful_JJ for_IN approximating_VBG sentence_NN boundaries_NNS ._.
Other_JJ segmentation_NN problems_NNS Processes_NNS may_MD be_VB required_VBN to_TO segment_NN text_NN into_IN segments_NNS besides_IN words_NNS ,_, including_VBG morphemes_NNS -LRB-_-LRB- a_DT task_NN usually_RB called_VBD morphological_JJ analysis_NN -RRB-_-RRB- ,_, paragraphs_NNS ,_, topics_NNS or_CC discourse_NN turns_NNS ._.
A_DT document_NN may_MD contain_VB multiple_JJ topics_NNS ,_, and_CC the_DT task_NN of_IN computerized_JJ text_NN segmentation_NN may_MD be_VB to_TO discover_VB these_DT topics_NNS automatically_RB and_CC segment_NN the_DT text_NN accordingly_RB ._.
The_DT topic_NN boundaries_NNS may_MD be_VB apparent_JJ from_IN section_NN titles_NNS and_CC paragraphs_NNS ._.
In_IN other_JJ cases_NNS one_CD needs_VBZ to_TO use_VB techniques_NNS similar_JJ to_TO those_DT used_VBN in_IN document_NN classification_NN ._.
Many_JJ different_JJ approaches_NNS have_VBP been_VBN tried_VBN ._.
Automatic_NNP segmentation_NN approaches_VBZ Automatic_NNP segmentation_NN is_VBZ the_DT problem_NN in_IN natural_JJ language_NN processing_NN of_IN implementing_VBG a_DT computer_NN process_NN to_TO segment_NN text_NN ._.
When_WRB punctuation_NN and_CC similar_JJ clues_NNS are_VBP not_RB consistently_RB available_JJ ,_, the_DT segmentation_NN task_NN often_RB requires_VBZ fairly_RB non-trivial_JJ techniques_NNS ,_, such_JJ as_IN statistical_JJ decision-making_NN ,_, large_JJ dictionaries_NNS ,_, as_RB well_RB as_IN consideration_NN of_IN syntactic_JJ and_CC semantic_JJ constraints_NNS ._.
Effective_JJ natural_JJ language_NN processing_NN systems_NNS and_CC text_NN segmentation_NN tools_NNS usually_RB operate_VBP on_IN text_NN in_IN specific_JJ domains_NNS and_CC sources_NNS ._.
As_IN an_DT example_NN ,_, processing_NN text_NN used_VBN in_IN medical_JJ records_NNS is_VBZ a_DT very_RB different_JJ problem_NN than_IN processing_VBG news_NN articles_NNS or_CC real_JJ estate_NN advertisements_NNS ._.
The_DT process_NN of_IN developing_VBG text_NN segmentation_NN tools_NNS starts_VBZ with_IN collecting_VBG a_DT large_JJ corpus_NN of_IN text_NN in_IN an_DT application_NN domain_NN ._.
There_EX are_VBP two_CD general_JJ approaches_NNS :_: Manual_JJ analysis_NN of_IN text_NN and_CC writing_VBG custom_NN software_NN Annotate_VBP the_DT sample_NN corpus_NN with_IN boundary_NN information_NN and_CC use_VB Machine_NNP Learning_NNP Some_DT text_NN segmentation_NN systems_NNS take_VBP advantage_NN of_IN any_DT markup_NN like_IN HTML_NNP and_CC know_VB document_NN formats_NNS like_IN PDF_NN to_TO provide_VB additional_JJ evidence_NN for_IN sentence_NN and_CC paragraph_NN boundaries_NNS ._.
